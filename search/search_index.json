{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"/]+|(?!\\b)(?=[A-Z][a-z])|\\.(?!\\d)|&[lg]t;","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Getting started","text":"<p>JPC is a JAX library for training neural  networks with Predictive Coding (PC). It is built on top of three main  libraries:</p> <ul> <li>Equinox, to define neural  networks with PyTorch-like syntax,</li> <li>Diffrax, to solve the gradient  flow PC inference dynamics, and</li> <li>Optax, for parameter optimisation.</li> </ul> <p>JPC provides a simple, fast and flexible API for  training of a variety of PCNs including discriminative, generative and hybrid  models.</p> <ul> <li>Like JAX, JPC is completely functional in design, and the core library is &lt;1000 lines of code. </li> <li>Unlike existing implementations, JPC leverages ordinary differential  equation (ODE) solvers to integrate the gradient flow inference dynamics of PC  networks (PCNs). </li> <li>JPC also provides some analytical tools that can be used to study and potentially diagnose issues with PCNs.</li> </ul> <p>If you're new to JPC, we recommend starting from the  example notebooks.</p>"},{"location":"#installation","title":"\ud83d\udcbb Installation","text":"<p>Clone the repo and in the project's directory run <pre><code>pip install jpc\n</code></pre></p> <p>Requires Python 3.10+ and JAX 0.4.38\u20130.5.2 (inclusive). For GPU usage, upgrade  jax to the appropriate cuda version (12 as an example here).</p> <pre><code>pip install --upgrade \"jax[cuda12]\"\n</code></pre>"},{"location":"#quick-example","title":"\u26a1\ufe0f Quick example","text":"<p>Use <code>jpc.make_pc_step()</code> to update the parameters of any neural network  compatible with PC updates (see examples) <pre><code>import jax.random as jr\nimport jax.numpy as jnp\nimport equinox as eqx\nimport optax\nimport jpc\n\n# toy data\nx = jnp.array([1., 1., 1.])\ny = -x\n\n# define model and optimiser\nkey = jr.PRNGKey(0)\nmodel = jpc.make_mlp(\n    key, \n    input_dim=3,\n    width=50,\n    depth=5,\n    output_dim=3\n    act_fn=\"relu\"\n)\noptim = optax.adam(1e-3)\nopt_state = optim.init(\n    (eqx.filter(model, eqx.is_array), None)\n)\n\n# perform one training step with PC\nresult = jpc.make_pc_step(\n    model=model,\n    optim=optim,\n    opt_state=opt_state,\n    output=y,\n    input=x\n)\n\n# updated model and optimiser\nmodel, opt_state = result[\"model\"], result[\"opt_state\"]\n</code></pre> Under the hood, <code>jpc.make_pc_step()</code></p> <ol> <li>integrates the inference (activity) dynamics using a diffrax ODE solver, and</li> <li>updates model parameters at the numerical solution of the activities with a given optax optimiser.</li> </ol> <p>NOTE: All convenience training and test functions such as <code>make_pc_step()</code>  are already \"jitted\" (for optimised performance) for the user's convenience.</p>"},{"location":"#advanced-usage","title":"\ud83d\ude80 Advanced usage","text":"<p>Advanced users can access all the underlying functions of <code>jpc.make_pc_step()</code>  as well as additional features. A custom PC training step looks like the  following: <pre><code>import jpc\n\n# 1. initialise activities with a feedforward pass\nactivities = jpc.init_activities_with_ffwd(model=model, input=x)\n\n# 2. run inference to equilibrium\nequilibrated_activities = jpc.solve_inference(\n    params=(model, None), \n    activities=activities, \n    output=y, \n    input=x\n)\n\n# 3. update parameters at the activities' solution with PC\nresult = jpc.update_params(\n    params=(model, None), \n    activities=equilibrated_activities,\n    optim=optim,\n    opt_state=opt_state,\n    output=y, \n    input=x\n)\n</code></pre> which can be embedded in a jitted function with any other additional  computations.</p>"},{"location":"#citation","title":"\ud83d\udcc4 Citation","text":"<p>If you found this library useful in your work, please cite (paper link):</p> <p><pre><code>@article{innocenti2024jpc,\n  title={JPC: Flexible Inference for Predictive Coding Networks in JAX},\n  author={Innocenti, Francesco and Kinghorn, Paul and Yun-Farmbrough, Will and Varona, Miguel De Llanza and Singh, Ryan and Buckley, Christopher L},\n  journal={arXiv preprint arXiv:2412.03676},\n  year={2024}\n}\n</code></pre> Also consider starring the project on GitHub! \u2b50\ufe0f </p>"},{"location":"#acknowledgements","title":"\ud83d\ude4f Acknowledgements","text":"<p>We are grateful to Patrick Kidger for early advice on how to use Diffrax.</p>"},{"location":"#see-also-other-pc-libraries","title":"See also: other PC libraries","text":"<ul> <li>ngc-learn (jax &amp; pytorch)</li> <li>pcx (jax)</li> <li>pyhgf (jax)</li> <li>Torch2PC (pytorch)</li> <li>pypc (pytorch)</li> <li>pybrid (pytorch)</li> </ul>"},{"location":"advanced_usage/","title":"Advanced usage","text":"<p>Advanced users can access all the underlying functions of <code>jpc.make_pc_step()</code>  as well as additional features. A custom PC training step looks like the  following: <pre><code>import jpc\n\n# 1. initialise activities with a feedforward pass\nactivities = jpc.init_activities_with_ffwd(model=model, input=x)\n\n# 2. run inference to equilibrium\nequilibrated_activities = jpc.solve_inference(\n    params=(model, None), \n    activities=activities, \n    output=y, \n    input=x\n)\n\n# 3. update parameters at the activities' solution with PC\nparam_update_result = jpc.update_params(\n    params=(model, None), \n    activities=equilibrated_activities,\n    optim=param_optim,\n    opt_state=param_opt_state,\n    output=y, \n    input=x\n)\n\n# updated model and optimiser\nmodel = param_update_result[\"model\"]\nparam_opt_state = param_update_result[\"opt_state\"]\n</code></pre> which can be embedded in a jitted function with any other additional  computations. One can also use any optax  optimiser to  equilibrate the inference dynamics by replacing the function in step 2, as  shown below. <pre><code>activity_optim = optax.adam(1e-3)\n\n# 1. initialise activities\n...\n\n# 2. infer with adam\nactivity_opt_state = activity_optim.init(activities)\n\nfor t in range(T):\n    activity_update_result = jpc.update_activities(\n        params=(model, None),\n        activities=activities,\n        optim=activity_optim,\n        opt_state=activity_opt_state,\n        output=y,\n        input=x\n    )\n    # updated activities and optimiser\n    activities = activity_update_result[\"activities\"]\n    activity_opt_state = activity_update_result[\"opt_state\"]\n\n# 3. update parameters at the activities' solution with PC\n...\n</code></pre> See the updates docs  for more details. JPC also  comes with some analytical tools that can be used to study and potentially  diagnose issues with PCNs  (see docs   and example notebook ).</p>"},{"location":"basic_usage/","title":"Basic usage","text":"<p>JPC provides two types of API depending on the use case:</p> <ul> <li>a simple, high-level API that allows to train and test models with predictive  coding in a few lines of code, and</li> <li>a more advanced API offering greater flexibility as well as additional features.</li> </ul>"},{"location":"basic_usage/#basic-usage","title":"Basic usage","text":"<p>At a high level, JPC provides a single convenience function <code>jpc.make_pc_step()</code>  to update the parameters of a neural network with PC. <pre><code>import jax.random as jr\nimport jax.numpy as jnp\nimport equinox as eqx\nimport optax\nimport jpc\n\n# toy data\nx = jnp.array([1., 1., 1.])\ny = -x\n\n# define model and optimiser\nkey = jr.PRNGKey(0)\nmodel = jpc.make_mlp(\n    key, \n    input_dim=3,\n    width=50,\n    depth=5,\n    output_dim=3\n    act_fn=\"relu\"\n)\noptim = optax.adam(1e-3)\nopt_state = optim.init(\n    (eqx.filter(model, eqx.is_array), None)\n)\n\n# perform one training step with PC\nupdate_result = jpc.make_pc_step(\n    model=model,\n    optim=optim,\n    opt_state=opt_state,\n    output=y,\n    input=x\n)\n\n# updated model and optimiser\nmodel, opt_state = update_result[\"model\"], update_result[\"opt_state\"]\n</code></pre> As shown above, at a minimum <code>jpc.make_pc_step()</code> takes a model, an optax  optimiser and its  state, and some data. The model needs to be compatible with PC updates in the  sense that it's split into callable layers (see the  example notebooks ). Also note  that the <code>input</code> is actually not needed for unsupervised training. In fact,  <code>jpc.make_pc_step()</code> can be used for classification and generation tasks, for  supervised as well as unsupervised training (again see the example notebooks ). </p> <p>Under the hood, <code>jpc.make_pc_step()</code> uses diffrax  to solve the activity (inference)  dynamics of PC. Many default arguments, for example related to the ODE solver, can be changed, including the ODE solver, and there is an option to record a  variety of metrics such as loss, accuracy, and energies. See the docs  for more  details.</p> <p>A similar convenience function <code>jpc.make_hpc_step()</code> is provided for updating the parameters of a hybrid PCN (Tschantz et al., 2023 ). <pre><code>import jax.random as jr\nimport equinox as eqx\nimport optax\nimport jpc\n\n# models\nkey = jr.PRNGKey(0)\nsubkeys = jr.split(key, 2)\n\ninput_dim, output_dim = 10, 3\nwidth, depth = 100, 5\ngenerator = jpc.make_mlp(\n    subkeys[0], \n    input_dim=input_dim,\n    width=width,\n    depth=depth,\n    output_dim=output_dim\n    act_fn=\"tanh\"\n)\n# NOTE that the input and output of the amortiser are reversed\namortiser = jpc.make_mlp(\n    subkeys[0], \n    input_dim=output_dim,\n    width=width,\n    depth=depth,\n    output_dim=input_dim\n    act_fn=\"tanh\"\n)\n\n# optimisers\ngen_optim = optax.adam(1e-3)\namort_optim = optax.adam(1e-3)\ngen_pt_state = gen_optim.init(\n    (eqx.filter(generator, eqx.is_array), None)\n)\namort_opt_state = amort_optim.init(\n    eqx.filter(amortiser, eqx.is_array)\n)\n\nupdate_result = jpc.make_hpc_step(\n    generator=generator,\n    amortiser=amortiser,\n    optims=[gen_optim, amort_optim],\n    opt_states=[gen_opt_state, amort_opt_state],\n    output=y,\n    input=x\n)\ngenerator, amortiser = update_result[\"generator\"], update_result[\"amortiser\"]\nopt_states = update_result[\"opt_states\"]\ngen_loss, amort_loss = update_result[\"losses\"]\n</code></pre> See the docs  and the example notebook  for more details.</p>"},{"location":"api/Continuous-time%20Inference/","title":"Continuous-time inference","text":"<p>The inference or activity dynamics of PC networks can be solved in either  discrete or continuous time. jpc.solve_inference()  leverages ODE solvers to integrate the continuous-time dynamics.</p>"},{"location":"api/Continuous-time%20Inference/#jpc.solve_inference","title":"<code>jpc.solve_inference(params: typing.Tuple[jaxtyping.PyTree[typing.Callable], typing.Optional[jaxtyping.PyTree[typing.Callable]]], activities: PyTree[jax.Array | numpy.ndarray | numpy.bool | numpy.number | bool | int | float | complex], output: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex], *, input: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex, NoneType] = None, loss_id: str = 'mse', param_type: str = 'sp', solver: AbstractSolver = Heun(), max_t1: int = 20, dt: float | int = None, stepsize_controller: AbstractStepSizeController = PIDController(rtol=0.001, atol=0.001), weight_decay: Shaped[Array, ''] = 0.0, spectral_penalty: Shaped[Array, ''] = 0.0, activity_decay: Shaped[Array, ''] = 0.0, record_iters: bool = False, record_every: int = None) -&gt; PyTree[jax.Array]</code>","text":"<p>Solves the inference (activity) dynamics of a predictive coding network.</p> <p>This is a wrapper around <code>diffrax.diffeqsolve()</code>  to integrate the gradient ODE system <code>jpc.neg_activity_grad()</code>  defining the PC inference dynamics.</p> \\[ d\\mathbf{z} / dt = - \u2207_{\\mathbf{z}} \\mathcal{F} \\] <p>where \\(\\mathcal{F}\\) is the free energy, \\(\\mathbf{z}\\) are the activities, with \\(\\mathbf{z}_L\\) clamped to some target and \\(\\mathbf{z}_0\\) optionally set to some prior.</p> <p>Main arguments:</p> <ul> <li><code>params</code>: Tuple with callable model layers and optional skip connections.</li> <li><code>activities</code>: List of activities for each layer free to vary.</li> <li><code>output</code>: Observation or target of the generative model.</li> </ul> <p>Other arguments:</p> <ul> <li><code>input</code>: Optional prior of the generative model.</li> <li><code>loss_id</code>: Loss function to use at the output layer. Options are mean squared      error <code>\"mse\"</code> (default) or cross-entropy <code>\"ce\"</code>.</li> <li><code>param_type</code>: Determines the parameterisation. Options are <code>\"sp\"</code>      (standard parameterisation), <code>\"mupc\"</code> (\u03bcPC),      or <code>\"ntp\"</code> (neural tangent parameterisation).      See <code>_get_param_scalings()</code>      for the specific scalings of these different parameterisations. Defaults     to <code>\"sp\"</code>.</li> <li><code>solver</code>: diffrax ODE solver      to be used. Default is <code>Heun</code>,      a 2nd order explicit Runge--Kutta method.</li> <li><code>max_t1</code>: Maximum end of integration region (20 by default).</li> <li><code>dt</code>: Integration step size. Defaults to <code>None</code> since the default     <code>stepsize_controller</code> will automatically determine it.</li> <li><code>stepsize_controller</code>: diffrax controller      for step size integration. Defaults to <code>PIDController</code>.      Note that the relative and absolute tolerances of the controller will      also determine the steady state to terminate the solver.</li> <li><code>weight_decay</code>: \\(\\ell^2\\) regulariser for the weights (0 by default).</li> <li><code>spectral_penalty</code>: Weight spectral penalty of the form      \\(||\\mathbf{I} - \\mathbf{W}_\\ell^T \\mathbf{W}_\\ell||^2\\) (0 by default).</li> <li><code>activity_decay</code>: \\(\\ell^2\\) regulariser for the activities (0 by default).</li> <li><code>record_iters</code>: If <code>True</code>, returns all integration steps.</li> <li><code>record_every</code>: int determining the sampling frequency of the integration     steps.</li> </ul> <p>Returns:</p> <p>List with solution of the activity dynamics for each layer.</p>"},{"location":"api/Discrete%20updates/","title":"Discrete updates","text":"<p>JPC provides access to standard discrete optimisers to update the parameters of  PC networks (jpc.update_params), and to both discrete (jpc.update_activities)  and continuous optimisers (jpc.solve_inference)  to solve the PC inference or activity dynamics.</p>"},{"location":"api/Discrete%20updates/#jpc.update_activities","title":"<code>jpc.update_activities(params: typing.Tuple[jaxtyping.PyTree[typing.Callable], typing.Optional[jaxtyping.PyTree[typing.Callable]]], activities: PyTree[jax.Array | numpy.ndarray | numpy.bool | numpy.number | bool | int | float | complex], optim: optax._src.base.GradientTransformation | optax._src.base.GradientTransformationExtraArgs, opt_state: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, typing.Iterable[ForwardRef(ArrayTree)], typing.Mapping[typing.Any, ForwardRef(ArrayTree)]], output: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex], *, input: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex, NoneType] = None, loss_id: str = 'mse', param_type: str = 'sp', weight_decay: Shaped[Array, ''] = 0.0, spectral_penalty: Shaped[Array, ''] = 0.0, activity_decay: Shaped[Array, ''] = 0.0) -&gt; typing.Dict</code>","text":"<p>Updates activities of a predictive coding network with a given  optax optimiser.</p> <p>Warning</p> <p><code>param_type = \"mupc\"</code> (\u03bcPC) assumes  that one is using <code>jpc.make_mlp()</code>  to create the model.</p> <p>Main arguments:</p> <ul> <li><code>params</code>: Tuple with callable model layers and optional skip connections.</li> <li><code>activities</code>: List of activities for each layer free to vary.</li> <li><code>optim</code>: optax optimiser, e.g. <code>optax.sgd()</code>.</li> <li><code>opt_state</code>: State of optax optimiser.</li> <li><code>output</code>: Observation or target of the generative model.</li> </ul> <p>Other arguments:</p> <ul> <li><code>input</code>: Optional prior of the generative model.</li> <li><code>loss_id</code>: Loss function to use at the output layer. Options are mean squared      error <code>\"mse\"</code> (default) or cross-entropy <code>\"ce\"</code>.</li> <li><code>param_type</code>: Determines the parameterisation. Options are <code>\"sp\"</code>      (standard parameterisation), <code>\"mupc\"</code> (\u03bcPC),      or <code>\"ntp\"</code> (neural tangent parameterisation).      See <code>_get_param_scalings()</code>      for the specific scalings of these different parameterisations. Defaults     to <code>\"sp\"</code>.</li> <li><code>weight_decay</code>: Weight decay for the weights (0 by default).</li> <li><code>spectral_penalty</code>: Weight spectral penalty of the form      \\(||\\mathbf{I} - \\mathbf{W}_\\ell^T \\mathbf{W}_\\ell||^2\\) (0 by default).</li> <li><code>activity_decay</code>: Activity decay for the activities (0 by default).</li> </ul> <p>Returns:</p> <p>Dictionary with energy, updated activities, activity gradients, and  optimiser state.</p>"},{"location":"api/Discrete%20updates/#jpc.update_params","title":"<code>jpc.update_params(params: typing.Tuple[jaxtyping.PyTree[typing.Callable], typing.Optional[jaxtyping.PyTree[typing.Callable]]], activities: PyTree[jax.Array | numpy.ndarray | numpy.bool | numpy.number | bool | int | float | complex], optim: optax._src.base.GradientTransformation | optax._src.base.GradientTransformationExtraArgs, opt_state: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, typing.Iterable[ForwardRef(ArrayTree)], typing.Mapping[typing.Any, ForwardRef(ArrayTree)]], output: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex], *, input: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex, NoneType] = None, loss_id: str = 'mse', param_type: str = 'sp', weight_decay: Shaped[Array, ''] = 0.0, spectral_penalty: Shaped[Array, ''] = 0.0, activity_decay: Shaped[Array, ''] = 0.0) -&gt; typing.Dict</code>","text":"<p>Updates parameters of a predictive coding network with a given  optax optimiser.</p> <p>Warning</p> <p><code>param_type = \"mupc\"</code> (\u03bcPC) assumes  that one is using <code>jpc.make_mlp()</code>  to create the model.</p> <p>Main arguments:</p> <ul> <li><code>params</code>: Tuple with callable model layers and optional skip connections.</li> <li><code>activities</code>: List of activities for each layer free to vary.</li> <li><code>optim</code>: optax optimiser, e.g. <code>optax.sgd()</code>.</li> <li><code>opt_state</code>: State of optax optimiser.</li> <li><code>output</code>: Observation or target of the generative model.</li> </ul> <p>Other arguments:</p> <ul> <li><code>input</code>: Optional prior of the generative model.</li> <li><code>loss_id</code>: Loss function to use at the output layer. Options are mean squared      error <code>\"mse\"</code> (default) or cross-entropy <code>\"ce\"</code>.</li> <li><code>param_type</code>: Determines the parameterisation. Options are <code>\"sp\"</code>      (standard parameterisation), <code>\"mupc\"</code> (\u03bcPC),      or <code>\"ntp\"</code> (neural tangent parameterisation).      See <code>_get_param_scalings()</code>      for the specific scalings of these different parameterisations. Defaults     to <code>\"sp\"</code>.</li> <li><code>weight_decay</code>: Weight decay for the weights (0 by default).</li> <li><code>spectral_penalty</code>: Weight spectral penalty of the form      \\(||\\mathbf{I} - \\mathbf{W}_\\ell^T \\mathbf{W}_\\ell||^2\\) (0 by default).</li> <li><code>activity_decay</code>: Activity decay for the activities (0 by default).</li> </ul> <p>Returns:</p> <p>Dictionary with model and optional skip model with updated parameters, parameter gradients, and optimiser state.</p>"},{"location":"api/Energy%20functions/","title":"Energy functions","text":"<p>JPC provides two main PC energy functions:</p> <ul> <li>jpc.pc_energy_fn()  for standard PC networks, and</li> <li>jpc.hpc_energy_fn()  for hybrid PC models (Tscshantz et al., 2023).</li> </ul>"},{"location":"api/Energy%20functions/#jpc.pc_energy_fn","title":"<code>jpc.pc_energy_fn(params: typing.Tuple[jaxtyping.PyTree[typing.Callable], typing.Optional[jaxtyping.PyTree[typing.Callable]]], activities: PyTree[jax.Array | numpy.ndarray | numpy.bool | numpy.number | bool | int | float | complex], y: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex], *, x: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex, NoneType] = None, loss: str = 'mse', param_type: str = 'sp', weight_decay: Shaped[Array, ''] = 0.0, spectral_penalty: Shaped[Array, ''] = 0.0, activity_decay: Shaped[Array, ''] = 0.0, record_layers: bool = False) -&gt; jaxtyping.Shaped[Array, ''] | jax.Array</code>","text":"<p>Computes the free energy for a neural network with optional skip  connections of the form</p> \\[ \\mathcal{F}(\\mathbf{z}; \u03b8) = 1/N \\sum_i^N \\sum_{\\ell=1}^L || \\mathbf{z}_{i, \\ell} - f_\\ell(\\mathbf{z}_{i, \\ell-1}; \u03b8) ||^2 \\] <p>given parameters \\(\u03b8\\), activities \\(\\mathbf{z}\\), output  \\(\\mathbf{z}_L = \\mathbf{y}\\), and optional input \\(\\mathbf{z}_0 = \\mathbf{x}\\) for supervised training. The activity of each layer \\(\\mathbf{z}_\\ell\\) is some function of the previous layer, e.g. ReLU\\((\\mathbf{W}_\\ell \\mathbf{z}_{\\ell-1} + \\mathbf{b}_\\ell)\\) for a fully  connected layer with biases and ReLU as activation.</p> <p>Note</p> <p>The input \\(x\\) and output \\(y\\) correspond to the prior and observation of the generative model, respectively.</p> <p>Main arguments:</p> <ul> <li><code>params</code>: Tuple with callable model (e.g. neural network) layers and     optional skip connections.</li> <li><code>activities</code>: List of activities for each layer free to vary.</li> <li><code>y</code>: Observation or target of the generative model.</li> </ul> <p>Other arguments:</p> <ul> <li><code>x</code>: Optional prior of the generative model (for supervised training).</li> <li><code>loss</code>: Loss function to use at the output layer. Options are mean squared      error <code>\"mse\"</code> (default) or cross-entropy <code>\"ce\"</code>.</li> <li><code>param_type</code>: Determines the parameterisation. Options are <code>\"sp\"</code>      (standard parameterisation), <code>\"mupc\"</code> (\u03bcPC),      or <code>\"ntp\"</code> (neural tangent parameterisation).      See <code>_get_param_scalings()</code>      for the specific scalings of these different parameterisations. Defaults     to <code>\"sp\"</code>.</li> <li><code>weight_decay</code>: \\(\\ell^2\\) regulariser for the weights (0 by default).</li> <li><code>spectral_penalty</code>: Weight spectral penalty of the form      \\(||\\mathbf{I} - \\mathbf{W}_\\ell^T \\mathbf{W}_\\ell||^2\\) (0 by default).</li> <li><code>activity_decay</code>: \\(\\ell^2\\) regulariser for the activities (0 by default).</li> <li><code>record_layers</code>: If <code>True</code>, returns the energy of each layer.</li> </ul> <p>Returns:</p> <p>The total or layer-wise energy normalised by the batch size.</p>"},{"location":"api/Energy%20functions/#jpc.hpc_energy_fn","title":"<code>jpc.hpc_energy_fn(model: PyTree[typing.Callable], equilib_activities: PyTree[jax.Array | numpy.ndarray | numpy.bool | numpy.number | bool | int | float | complex], amort_activities: PyTree[jax.Array | numpy.ndarray | numpy.bool | numpy.number | bool | int | float | complex], x: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex], y: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex, NoneType] = None, record_layers: bool = False) -&gt; jaxtyping.Shaped[Array, ''] | jax.Array</code>","text":"<p>Computes the free energy of an amortised PC network (Tscshantz et al., 2023)</p> \\[ \\mathcal{F}(\\mathbf{z}^*, \\hat{\\mathbf{z}}; \u03b8) = 1/N \\sum_i^N \\sum_{\\ell=1}^L || \\mathbf{z}^*_{i, \\ell} - f_\\ell(\\hat{\\mathbf{z}}_{i, \\ell-1}; \u03b8) ||^2 \\] <p>given the equilibrated activities of the generator \\(\\mathbf{z}^*\\) (target for the amortiser), the feedforward guesses of the amortiser \\(\\hat{\\mathbf{z}}\\), the amortiser's parameters \\(\u03b8\\), input \\(\\mathbf{z}_0 = \\mathbf{x}\\), and optional output \\(\\mathbf{z}_L = \\mathbf{y}\\) for supervised training.</p> <p>Note</p> <p>The input \\(x\\) and output \\(y\\) are reversed compared to <code>pc_energy_fn()</code> (\\(x\\) is the generator's target and \\(y\\) is its optional input or prior). Just think of \\(x\\) and \\(y\\) as the actual input and output of the amortiser, respectively.</p> Reference <pre><code>@article{tscshantz2023hybrid,\n    title={Hybrid predictive coding: Inferring, fast and slow},\n    author={Tscshantz, Alexander and Millidge, Beren and Seth, Anil K and Buckley, Christopher L},\n    journal={PLoS computational biology},\n    volume={19},\n    number={8},\n    pages={e1011280},\n    year={2023},\n    publisher={Public Library of Science San Francisco, CA USA}\n}\n</code></pre> <p>Main arguments:</p> <ul> <li><code>model</code>: List of callable model (e.g. neural network) layers.</li> <li><code>equilib_activities</code>: List of equilibrated activities reached by the     generator and target for the amortiser.</li> <li><code>amort_activities</code>: List of amortiser's feedforward guesses     (initialisation) for the network activities.</li> <li><code>x</code>: Input to the amortiser.</li> <li><code>y</code>: Optional target of the amortiser (for supervised training).</li> </ul> <p>Other arguments:</p> <ul> <li><code>record_layers</code>: If <code>True</code>, returns energies for each layer.</li> </ul> <p>Returns:</p> <p>The total or layer-wise energy normalised by batch size.</p>"},{"location":"api/Energy%20functions/#jpc._get_param_scalings","title":"<code>jpc._get_param_scalings(model: PyTree[typing.Callable], input: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex], *, skip_model: typing.Optional[jaxtyping.PyTree[typing.Callable]] = None, param_type: str = 'sp') -&gt; list[float]</code>","text":"<p>Gets layer scalings for a given parameterisation.</p> <p>Warning</p> <p><code>param_type = \"mupc\"</code> (\u03bcPC) assumes  that one is using <code>jpc.make_mlp()</code>  to create the model.</p> <p>Main arguments:</p> <ul> <li><code>model</code>: List of callable model (e.g. neural network) layers.</li> <li><code>input</code>: input to the model.</li> </ul> <p>Other arguments:</p> <ul> <li><code>skip_model</code>: Optional skip connection model.</li> <li><code>param_type</code>: Determines the parameterisation. Options are <code>\"sp\"</code>      (standard parameterisation), <code>\"mupc\"</code> (\u03bcPC),      or <code>\"ntp\"</code> (neural tangent parameterisation). Defaults to <code>\"sp\"</code>.</li> </ul> <p>Returns:</p> <p>List with scalings for each layer.</p>"},{"location":"api/Gradients/","title":"Gradients","text":"<p>Info</p> <p>There are two similar functions to compute the gradient of the energy with respect to the activities: <code>jpc.neg_activity_grad()</code>  and <code>jpc.compute_activity_grad()</code>.  The first is used by <code>jpc.solve_inference()</code>  as gradient flow, while the second is for compatibility with discrete  optax optimisers such as  gradient descent.</p>"},{"location":"api/Gradients/#jpc.neg_activity_grad","title":"<code>jpc.neg_activity_grad(t: float | int, activities: PyTree[jax.Array | numpy.ndarray | numpy.bool | numpy.number | bool | int | float | complex], args: typing.Tuple[typing.Tuple[jaxtyping.PyTree[typing.Callable], typing.Optional[jaxtyping.PyTree[typing.Callable]]], typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex], typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex, NoneType], int, str, str, diffrax._step_size_controller.base.AbstractStepSizeController]) -&gt; PyTree[jax.Array]</code>","text":"<p>Computes the negative gradient of the PC energy  with respect to the activities \\(- \u2207_{\\mathbf{z}} \\mathcal{F}\\).</p> <p>This defines an ODE system to be integrated by <code>jpc.solve_pc_inference()</code>.</p> <p>Main arguments:</p> <ul> <li><code>t</code>: Time step of the ODE system, used for downstream integration by     <code>diffrax.diffeqsolve()</code>.</li> <li><code>activities</code>: List of activities for each layer free to vary.</li> <li> <p><code>args</code>: 5-Tuple with:</p> <p>(i) Tuple with callable model layers and optional skip connections,</p> <p>(ii) model output (observation),</p> <p>(iii) model input (prior),</p> <p>(iv) loss specified at the output layer (<code>\"mse\"</code> as default or <code>\"ce\"</code>),</p> <p>(v) parameterisation type (<code>\"sp\"</code> as default, <code>\"mupc\"</code>, or <code>\"ntp\"</code>),</p> <p>(vi) \\(\\ell^2\\) regulariser for the weights (0 by default),</p> <p>(vii) spectral penalty for the weights (0 by default),</p> <p>(viii) \\(\\ell^2\\) regulariser for the activities (0 by default), and</p> <p>(ix) diffrax controller for step size integration.</p> </li> </ul> <p>Returns:</p> <p>List of negative gradients of the energy with respect to the activities.</p>"},{"location":"api/Gradients/#jpc.compute_activity_grad","title":"<code>jpc.compute_activity_grad(params: typing.Tuple[jaxtyping.PyTree[typing.Callable], typing.Optional[jaxtyping.PyTree[typing.Callable]]], activities: PyTree[jax.Array | numpy.ndarray | numpy.bool | numpy.number | bool | int | float | complex], y: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex], *, x: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex, NoneType], loss_id: str = 'mse', param_type: str = 'sp', weight_decay: Shaped[Array, ''] = 0.0, spectral_penalty: Shaped[Array, ''] = 0.0, activity_decay: Shaped[Array, ''] = 0.0) -&gt; PyTree[jax.Array]</code>","text":"<p>Computes the gradient of the PC energy with respect to the activities \\(\u2207_{\\mathbf{z}} \\mathcal{F}\\).</p> <p>Note</p> <p>This function differs from <code>jpc.neg_activity_grad()</code>  only in the sign of the gradient (positive as opposed to negative) and  is called in <code>jpc.update_activities()</code>  for use with any optax  optimiser.</p> <p>Main arguments:</p> <ul> <li><code>params</code>: Tuple with callable model layers and optional skip connections.</li> <li><code>activities</code>: List of activities for each layer free to vary.</li> <li><code>y</code>: Observation or target of the generative model.</li> </ul> <p>Other arguments:</p> <ul> <li><code>x</code>: Optional prior of the generative model.</li> <li><code>loss_id</code>: Loss function to use at the output layer. Options are mean squared      error <code>\"mse\"</code> (default) or cross-entropy <code>\"ce\"</code>.</li> <li><code>param_type</code>: Determines the parameterisation. Options are <code>\"sp\"</code>      (standard parameterisation), <code>\"mupc\"</code> (\u03bcPC),      or <code>\"ntp\"</code> (neural tangent parameterisation).      See <code>_get_param_scalings()</code>      for the specific scalings of these different parameterisations. Defaults     to <code>\"sp\"</code>.</li> <li><code>weight_decay</code>: \\(\\ell^2\\) regulariser for the weights (0 by default).</li> <li><code>spectral_penalty</code>: Weight spectral penalty of the form      \\(||\\mathbf{I} - \\mathbf{W}_\\ell^T \\mathbf{W}_\\ell||^2\\) (0 by default).</li> <li><code>activity_decay</code>: \\(\\ell^2\\) regulariser for the activities (0 by default).</li> </ul> <p>Returns:</p> <p>List of negative gradients of the energy with respect to the activities.</p>"},{"location":"api/Gradients/#jpc.compute_pc_param_grads","title":"<code>jpc.compute_pc_param_grads(params: typing.Tuple[jaxtyping.PyTree[typing.Callable], typing.Optional[jaxtyping.PyTree[typing.Callable]]], activities: PyTree[jax.Array | numpy.ndarray | numpy.bool | numpy.number | bool | int | float | complex], y: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex], *, x: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex, NoneType] = None, loss_id: str = 'mse', param_type: str = 'sp', weight_decay: Shaped[Array, ''] = 0.0, spectral_penalty: Shaped[Array, ''] = 0.0, activity_decay: Shaped[Array, ''] = 0.0) -&gt; typing.Tuple[jaxtyping.PyTree[jax.Array], jaxtyping.PyTree[jax.Array]]</code>","text":"<p>Computes the gradient of the PC energy with respect to model parameters \\(\u2207_\u03b8 \\mathcal{F}\\).</p> <p>Main arguments:</p> <ul> <li><code>params</code>: Tuple with callable model layers and optional skip connections.</li> <li><code>activities</code>: List of activities for each layer free to vary.</li> <li><code>y</code>: Observation or target of the generative model.</li> </ul> <p>Other arguments:</p> <ul> <li><code>x</code>: Optional prior of the generative model.</li> <li><code>loss_id</code>: Loss function to use at the output layer. Options are mean squared      error <code>\"mse\"</code> (default) or cross-entropy <code>\"ce\"</code>.</li> <li><code>param_type</code>: Determines the parameterisation. Options are <code>\"sp\"</code>      (standard parameterisation), <code>\"mupc\"</code> (\u03bcPC),      or <code>\"ntp\"</code> (neural tangent parameterisation).      See <code>_get_param_scalings()</code>      for the specific scalings of these different parameterisations. Defaults     to <code>\"sp\"</code>.</li> <li><code>weight_decay</code>: \\(\\ell^2\\) regulariser for the weights (0 by default).</li> <li><code>spectral_penalty</code>: Weight spectral penalty of the form      \\(||\\mathbf{I} - \\mathbf{W}_\\ell^T \\mathbf{W}_\\ell||^2\\) (0 by default).</li> <li><code>activity_decay</code>: \\(\\ell^2\\) regulariser for the activities (0 by default).</li> </ul> <p>Returns:</p> <p>List of parameter gradients for each model layer.</p>"},{"location":"api/Gradients/#jpc.compute_hpc_param_grads","title":"<code>jpc.compute_hpc_param_grads(model: PyTree[typing.Callable], equilib_activities: PyTree[jax.Array | numpy.ndarray | numpy.bool | numpy.number | bool | int | float | complex], amort_activities: PyTree[jax.Array | numpy.ndarray | numpy.bool | numpy.number | bool | int | float | complex], x: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex], y: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex, NoneType] = None) -&gt; PyTree[jax.Array]</code>","text":"<p>Computes the gradient of the hybrid PC energy  with respect to the amortiser's parameters \\(\u2207_\u03b8 \\mathcal{F}\\).</p> <p>Warning</p> <p>The input \\(x\\) and output \\(y\\) are reversed compared to  <code>jpc.compute_pc_param_grads()</code>  (\\(x\\) is the generator's target and \\(y\\) is its optional input or prior).  Just think of \\(x\\) and \\(y\\) as the actual input and output of the  amortiser, respectively.</p> <p>Main arguments:</p> <ul> <li><code>model</code>: List of callable model (e.g. neural network) layers.</li> <li><code>equilib_activities</code>: List of equilibrated activities reached by the     generator and target for the amortiser.</li> <li><code>amort_activities</code>: List of amortiser's feedforward guesses     (initialisation) for the model activities.</li> <li><code>x</code>: Input to the amortiser.</li> <li><code>y</code>: Optional target of the amortiser (for supervised training).</li> </ul> <p>Returns:</p> <p>List of parameter gradients for each model layer.</p>"},{"location":"api/Initialisation/","title":"Initialisation","text":"<p>JPC provides 3 ways of initialising the activities of a PC network: </p> <ul> <li>jpc.init_activities_with_ffwd() for a feedforward pass (standard), </li> <li>jpc.init_activities_from_normal() for random initialisation, and </li> <li>jpc.init_activities_with_amort() for use of an amortised network.</li> </ul>"},{"location":"api/Initialisation/#jpc.init_activities_with_ffwd","title":"<code>jpc.init_activities_with_ffwd(model: PyTree[typing.Callable], input: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex], *, skip_model: typing.Optional[jaxtyping.PyTree[typing.Callable]] = None, param_type: str = 'sp') -&gt; PyTree[jax.Array]</code>","text":"<p>Initialises the layers' activity with a feedforward pass \\(\\{ f_\\ell(\\mathbf{z}_{\\ell-1}) \\}_{\\ell=1}^L\\) where \\(f_\\ell(\\cdot)\\) is some callable layer transformation and \\(\\mathbf{z}_0 = \\mathbf{x}\\) is the input.</p> <p>Warning</p> <p><code>param_type = \"mupc\"</code> (\u03bcPC) assumes  that one is using <code>jpc.make_mlp()</code>  to create the model.</p> <p>Main arguments:</p> <ul> <li><code>model</code>: List of callable model (e.g. neural network) layers.</li> <li><code>input</code>: input to the model.</li> </ul> <p>Other arguments:</p> <ul> <li><code>skip_model</code>: Optional skip connection model.</li> <li><code>param_type</code>: Determines the parameterisation. Options are <code>\"sp\"</code>      (standard parameterisation), <code>\"mupc\"</code> (\u03bcPC),      or <code>\"ntp\"</code> (neural tangent parameterisation).      See <code>_get_param_scalings()</code>      for the specific scalings of these different parameterisations. Defaults     to <code>\"sp\"</code>.</li> </ul> <p>Returns:</p> <p>List with activity values of each layer.</p>"},{"location":"api/Initialisation/#jpc.init_activities_from_normal","title":"<code>jpc.init_activities_from_normal(key: typing.Union[jaxtyping.Key[Array, ''], jaxtyping.UInt32[Array, 2]], layer_sizes: PyTree[int], mode: str, batch_size: int, sigma: Shaped[Array, ''] = 0.05) -&gt; PyTree[jax.Array]</code>","text":"<p>Initialises network activities from a zero-mean Gaussian  \\(z_i \\sim \\mathcal{N}(0, \\sigma^2)\\).</p> <p>Main arguments:</p> <ul> <li><code>key</code>: <code>jax.random.PRNGKey</code> for sampling.</li> <li><code>layer_sizes</code>: List with dimension of all layers (input, hidden and     output).</li> <li><code>mode</code>: If <code>\"supervised\"</code>, all hidden layers are initialised. If     <code>\"unsupervised\"</code> the input layer \\(\\mathbf{z}_0\\) is also initialised.</li> <li><code>batch_size</code>: Dimension of data batch.</li> <li><code>sigma</code>: Standard deviation for Gaussian to sample activities from.     Defaults to 5e-2.</li> </ul> <p>Returns:</p> <p>List of randomly initialised activities for each layer.</p>"},{"location":"api/Initialisation/#jpc.init_activities_with_amort","title":"<code>jpc.init_activities_with_amort(amortiser: PyTree[typing.Callable], generator: PyTree[typing.Callable], input: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex]) -&gt; PyTree[jax.Array]</code>","text":"<p>Initialises layers' activity with an amortised network \\(\\{ f_{L-\\ell+1}(\\mathbf{z}_{L-\\ell}) \\}_{\\ell=1}^L\\) where \\(\\mathbf{z}_0 = \\mathbf{y}\\) is the input or generator's target.</p> <p>Note</p> <p>The output order is reversed for downstream use by the generator.</p> <p>Main arguments:</p> <ul> <li><code>amortiser</code>: List of callable layers for model amortising the inference     of the <code>generator</code>.</li> <li><code>generator</code>: List of callable layers for the generative model.</li> <li><code>input</code>: Input to the amortiser.</li> </ul> <p>Returns:</p> <p>List with amortised initialisation of each layer.</p>"},{"location":"api/Testing/","title":"Testing","text":"<p>JPC provides a few convenience functions to test different types of PC network (PCN):</p> <ul> <li>jpc.test_discriminative_pc()  for test loss and accuracy of discriminative PCNs;</li> <li>jpc.test_generative_pc()  for accuracy and output predictions of generative PCNs; and</li> <li>jpc.test_hpc() for accuracy of all models (amortiser, generator, &amp; hybrid) as well as output predictions.</li> </ul>"},{"location":"api/Testing/#jpc.test_discriminative_pc","title":"<code>jpc.test_discriminative_pc(model: PyTree[typing.Callable], output: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex], input: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex], *, skip_model: typing.Optional[jaxtyping.PyTree[typing.Callable]] = None, loss: str = 'mse', param_type: str = 'sp') -&gt; typing.Tuple[jaxtyping.Shaped[Array, ''], jaxtyping.Shaped[Array, '']]</code>","text":"<p>Computes test metrics for a discriminative predictive coding network.</p> <p>Main arguments:</p> <ul> <li><code>model</code>: List of callable model (e.g. neural network) layers.</li> <li><code>output</code>: Observation or target of the generative model.</li> <li><code>input</code>: Optional prior of the generative model.</li> </ul> <p>Other arguments:</p> <ul> <li><code>skip_model</code>: Optional skip connection model.</li> <li><code>loss</code>: Loss function to use at the output layer. Options are mean squared      error <code>\"mse\"</code> (default) or cross-entropy <code>\"ce\"</code>.</li> <li><code>param_type</code>: Determines the parameterisation. Options are <code>\"sp\"</code>      (standard parameterisation), <code>\"mupc\"</code> (\u03bcPC),      or <code>\"ntp\"</code> (neural tangent parameterisation). See <code>_get_param_scalings()</code>      for the specific scalings of these different parameterisations. Defaults     to <code>\"sp\"</code>.</li> </ul> <p>Returns:</p> <p>Test loss and accuracy of output predictions.</p>"},{"location":"api/Testing/#jpc.test_generative_pc","title":"<code>jpc.test_generative_pc(model: PyTree[typing.Callable], output: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex], input: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex], key: typing.Union[jaxtyping.Key[Array, ''], jaxtyping.UInt32[Array, 2]], layer_sizes: PyTree[int], batch_size: int, *, skip_model: typing.Optional[jaxtyping.PyTree[typing.Callable]] = None, loss_id: str = 'mse', param_type: str = 'sp', sigma: Shaped[Array, ''] = 0.05, ode_solver: AbstractSolver = Heun(), max_t1: int = 500, dt: jaxtyping.Shaped[Array, ''] | int = None, stepsize_controller: AbstractStepSizeController = PIDController(rtol=0.001, atol=0.001), weight_decay: Shaped[Array, ''] = 0.0, spectral_penalty: Shaped[Array, ''] = 0.0, activity_decay: Shaped[Array, ''] = 0.0) -&gt; typing.Tuple[jaxtyping.Shaped[Array, ''], jax.Array]</code>","text":"<p>Computes test metrics for a generative predictive coding network.</p> <p>Gets output predictions (e.g. of an image given a label) with a feedforward pass and calculates accuracy of inferred input (e.g. of a label given an image).</p> <p>Main arguments:</p> <ul> <li><code>model</code>: List of callable model (e.g. neural network) layers.</li> <li><code>output</code>: Observation or target of the generative model.</li> <li><code>input</code>: Prior of the generative model.</li> <li><code>key</code>: <code>jax.random.PRNGKey</code> for random initialisation of activities.</li> <li><code>layer_sizes</code>: Dimension of all layers (input, hidden and output).</li> <li><code>batch_size</code>: Dimension of data batch for activity initialisation.</li> </ul> <p>Other arguments:</p> <ul> <li><code>skip_model</code>: Optional skip connection model.</li> <li><code>loss_id</code>: Loss function to use at the output layer. Options are mean squared      error <code>\"mse\"</code> (default) or cross-entropy <code>\"ce\"</code>.</li> <li><code>param_type</code>: Determines the parameterisation. Options are <code>\"sp\"</code>      (standard parameterisation), <code>\"mupc\"</code> (\u03bcPC),      or <code>\"ntp\"</code> (neural tangent parameterisation). See <code>_get_param_scalings()</code>      for the specific scalings of these different parameterisations. Defaults     to <code>\"sp\"</code>.</li> <li><code>sigma</code>: Standard deviation for Gaussian to sample activities from.     Defaults to 5e-2.</li> <li><code>ode_solver</code>: diffrax ODE solver      to be used. Default is <code>Heun</code>,      a 2nd order explicit Runge--Kutta method.</li> <li><code>max_t1</code>: Maximum end of integration region (500 by default).</li> <li><code>dt</code>: Integration step size. Defaults to None since the default     <code>stepsize_controller</code> will automatically determine it.</li> <li><code>stepsize_controller</code>: diffrax controller      for step size integration. Defaults to <code>PIDController</code>.      Note that the relative and absolute tolerances of the controller will      also determine the steady state to terminate the solver.</li> <li><code>weight_decay</code>: Weight decay for the weights (0 by default).</li> <li><code>spectral_penalty</code>: Weight spectral penalty of the form      \\(||\\mathbf{I} - \\mathbf{W}_\\ell^T \\mathbf{W}_\\ell||^2\\) (0 by default).</li> <li><code>activity_decay</code>: Activity decay for the activities (0 by default).</li> </ul> <p>Returns:</p> <p>Accuracy and output predictions.</p>"},{"location":"api/Testing/#jpc.test_hpc","title":"<code>jpc.test_hpc(generator: PyTree[typing.Callable], amortiser: PyTree[typing.Callable], output: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex], input: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex], key: typing.Union[jaxtyping.Key[Array, ''], jaxtyping.UInt32[Array, 2]], layer_sizes: PyTree[int], batch_size: int, sigma: Shaped[Array, ''] = 0.05, ode_solver: AbstractSolver = Heun(), max_t1: int = 500, dt: jaxtyping.Shaped[Array, ''] | int = None, stepsize_controller: AbstractStepSizeController = PIDController(rtol=0.001, atol=0.001)) -&gt; typing.Tuple[jaxtyping.Shaped[Array, ''], jaxtyping.Shaped[Array, ''], jaxtyping.Shaped[Array, ''], jax.Array]</code>","text":"<p>Computes test metrics for hybrid predictive coding trained in a supervised manner.</p> <p>Calculates input accuracy of (i) amortiser, (ii) generator, and (iii) hybrid (amortiser + generator). Also returns output predictions (e.g. of an image given a label) with a feedforward pass of the generator.</p> <p>Note</p> <p>The input and output of the generator are the output and input of the amortiser, respectively.</p> <p>Main arguments:</p> <ul> <li><code>generator</code>: List of callable layers for the generative model.</li> <li><code>amortiser</code>: List of callable layers for model amortising the inference     of the <code>generator</code>.</li> <li><code>output</code>: Observation or target of the generative model.</li> <li><code>input</code>: Optional prior of the generator, target for the amortiser.</li> <li><code>key</code>: <code>jax.random.PRNGKey</code> for random initialisation of activities.</li> <li><code>layer_sizes</code>: Dimension of all layers (input, hidden and output).</li> <li><code>batch_size</code>: Dimension of data batch for initialisation of activities.</li> </ul> <p>Other arguments:</p> <ul> <li><code>sigma</code>: Standard deviation for Gaussian to sample activities from.     Defaults to 5e-2.</li> <li><code>ode_solver</code>: diffrax ODE solver      to be used. Default is <code>Heun</code>,      a 2nd order explicit Runge--Kutta method.</li> <li><code>max_t1</code>: Maximum end of integration region (500 by default).</li> <li><code>dt</code>: Integration step size. Defaults to None since the default     <code>stepsize_controller</code> will automatically determine it.</li> <li><code>stepsize_controller</code>: diffrax controller      for step size integration. Defaults to <code>PIDController</code>.      Note that the relative and absolute tolerances of the controller will      also determine the steady state to terminate the solver.</li> </ul> <p>Returns:</p> <p>Accuracies of all models and output predictions.</p>"},{"location":"api/Theoretical%20tools/","title":"Theoretical tools","text":"<p>JPC provides the following theoretical tools that can be used to study  deep linear networks (DLNs) trained with PC: </p> <ul> <li>jpc.compute_linear_equilib_energy()  to compute the theoretical PC energy at the solution of the activities for DLNs;</li> <li>jpc.compute_linear_activity_hessian()  to compute the theoretical Hessian of the energy with respect to the activities of DLNs; </li> <li>jpc.compute_linear_activity_solution()  to compute the analytical PC inference solution for DLNs.</li> </ul>"},{"location":"api/Theoretical%20tools/#jpc.compute_linear_equilib_energy","title":"<code>jpc.compute_linear_equilib_energy(network: PyTree[equinox.nn._linear.Linear], x: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex], y: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex]) -&gt; Array</code>","text":"<p>Computes the theoretical PC energy  at the solution of the activities for a deep linear network (Innocenti et al. 2024):</p> \\[ \\mathcal{F}^* = 1/2N \\sum_i^N (\\mathbf{y}_i - \\mathbf{W}_{L:1}\\mathbf{x}_i)^T \\mathbf{S}^{-1}(\\mathbf{y}_i - \\mathbf{W}_{L:1}\\mathbf{x}_i) \\] <p>where \\(\\mathbf{S} = \\mathbf{I}_{d_y} + \\sum_{\\ell=2}^L (\\mathbf{W}_{L:\\ell})(\\mathbf{W}_{L:\\ell})^T\\) and \\(\\mathbf{W}_{k:\\ell} = \\mathbf{W}_k \\dots \\mathbf{W}_\\ell\\) for \\(\\ell, k \\in 1,\\dots, L\\).</p> <p>Note</p> <p>This expression assumes no biases. It could also be generalised to  other network architectures (e.g. ResNets) and parameterisations (see Innocenti et al. 2025).  However, note that the equilibrated energy for ResNets and other parameterisations can still be computed by getting the activity solution with <code>jpc.compute_linear_activity_solution()</code>  and then plugging this into the standard PC energy  jpc.pc_energy_fn().</p> <p>Example</p> <p>In practice, this means that if you run, at any point in training, the  inference dynamics of any PC linear network to equilibrium, then  <code>jpc.pc_energy_fn()</code>  will return the same energy value as this function. For a demonstration, see this example notebook.</p> <p></p> Reference <pre><code>@article{innocenti2025only,\n    title={Only Strict Saddles in the Energy Landscape of Predictive Coding Networks?},\n    author={Innocenti, Francesco and Achour, El Mehdi and Singh, Ryan and Buckley, Christopher L},\n    journal={Advances in Neural Information Processing Systems},\n    volume={37},\n    pages={53649--53683},\n    year={2025}\n}\n</code></pre> <p>Main arguments:</p> <ul> <li><code>network</code>: Linear network defined as a list of Equinox Linear layers.</li> <li><code>x</code>: Network input.</li> <li><code>y</code>: Network output.</li> </ul> <p>Returns:</p> <p>Mean total theoretical energy over a data batch.</p>"},{"location":"api/Theoretical%20tools/#jpc.compute_linear_activity_hessian","title":"<code>jpc.compute_linear_activity_hessian(Ws: PyTree[jax.Array], *, use_skips: bool = False, param_type: str = 'sp', activity_decay: bool = False, diag: bool = True, off_diag: bool = True) -&gt; Array</code>","text":"<p>Computes the theoretical Hessian matrix of the PC energy  with respect to the activities for a linear network,  \\((\\mathbf{H}_{\\mathbf{z}})_{\\ell k} := \\partial^2 \\mathcal{F} / \\partial \\mathbf{z}_\\ell \\partial \\mathbf{z}_k \\in \\mathbb{R}^{(NH)\u00d7(NH)}\\)  where \\(N\\) and \\(H\\) are the width and number of hidden layers, respectively (Innocenti et al., 2025).</p> <p>Info</p> <p>This function can be used (i) to study the inference landscape of linear PC networks and (ii) to compute the analytical solution with  <code>jpc.compute_linear_activity_solution()</code>.</p> <p>Warning</p> <p>This was highly hard-coded for quick experimental iteration with  different models and parameterisations. The computation of the blocks could be implemented much more elegantly by fetching the  transformation for each layer.</p> Reference <pre><code>@article{innocenti2025mu,\n    title={$$\backslash$mu $ PC: Scaling Predictive Coding to 100+ Layer Networks},\n    author={Innocenti, Francesco and Achour, El Mehdi and Buckley, Christopher L},\n    journal={arXiv preprint arXiv:2505.13124},\n    year={2025}\n}\n</code></pre> <p>Main arguments:</p> <ul> <li><code>Ws</code>: List of all the network weight matrices.</li> </ul> <p>Other arguments:</p> <ul> <li><code>use_skips</code>: Whether to assume one-layer skip connections at every layer      except from the input and to the output. <code>False</code> by default.</li> <li><code>param_type</code>: Determines the parameterisation. Options are <code>\"sp\"</code>      (standard parameterisation), <code>\"mupc\"</code> (\u03bcPC),      or <code>\"ntp\"</code> (neural tangent parameterisation).      See <code>_get_param_scalings()</code>      for the specific scalings of these different parameterisations. Defaults     to <code>\"sp\"</code>.</li> <li><code>activity_decay</code>: \\(\\ell^2\\) regulariser for the activities.</li> <li><code>diag</code>: Whether to compute the diagonal blocks of the Hessian.</li> <li><code>off-diag</code>: Whether to compute the off-diagonal blocks of the Hessian.</li> </ul> <p>Returns:</p> <p>The activity Hessian matrix of size \\(NH\u00d7NH\\) where \\(N\\) is the width and \\(H\\)  is the number of hidden layers.</p>"},{"location":"api/Theoretical%20tools/#jpc.compute_linear_activity_solution","title":"<code>jpc.compute_linear_activity_solution(network: PyTree[equinox.nn._linear.Linear], x: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex], y: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex], *, use_skips: bool = False, param_type: str = 'sp', activity_decay: bool = False) -&gt; PyTree[jax.Array]</code>","text":"<p>Computes the theoretical solution for the PC activities of a linear network (Innocenti et al., 2025).</p> \\[ \\mathbf{z}^* = \\mathbf{H}_{\\mathbf{z}}^{-1}\\mathbf{b} \\] <p>where \\((\\mathbf{H}_{\\mathbf{z}})_{\\ell k} := \\partial^2 \\mathcal{F} / \\partial \\mathbf{z}_\\ell \\partial \\mathbf{z}_k \\in \\mathbb{R}^{(NH)\u00d7(NH)}\\)  is the Hessian of the energy with respect to the activities, and  \\(\\mathbf{b} \\in \\mathbb{R}^{NH}\\) is a sparse vector depending only on the  data and associated weights. The activity Hessian is computed analytically  using <code>jpc.compute_linear_activity_hessian()</code>.    </p> <p>Info</p> <p>This can be used to study how linear PC networks learn when they perform  perfect inference. An example notebook demonstration is in the works!</p> Reference <pre><code>@article{innocenti2025mu,\n    title={$$\backslash$mu $ PC: Scaling Predictive Coding to 100+ Layer Networks},\n    author={Innocenti, Francesco and Achour, El Mehdi and Buckley, Christopher L},\n    journal={arXiv preprint arXiv:2505.13124},\n    year={2025}\n}\n</code></pre> <p>Main arguments:</p> <ul> <li><code>network</code>: Linear network defined as a list of Equinox Linear layers.</li> <li><code>x</code>: Network input.</li> <li><code>y</code>: Network output.</li> </ul> <p>Other arguments:</p> <ul> <li><code>use_skips</code>: Whether to assume one-layer skip connections at every layer      except from the input and to the output. <code>False</code> by default.</li> <li><code>param_type</code>: Determines the parameterisation. Options are <code>\"sp\"</code>      (standard parameterisation), <code>\"mupc\"</code> (\u03bcPC),      or <code>\"ntp\"</code> (neural tangent parameterisation).      See <code>_get_param_scalings()</code>      for the specific scalings of these different parameterisations. Defaults     to <code>\"sp\"</code>.</li> <li><code>activity_decay</code>: \\(\\ell^2\\) regulariser for the activities.</li> </ul> <p>Returns:</p> <p>List of theoretical activities for each layer.</p>"},{"location":"api/Training/","title":"Training","text":"<p>JPC provides 2 single convenience functions to update the parameters of any  PC-compatible model with PC:</p> <ul> <li>jpc.make_pc_step() to  perform an update using standard PC, and</li> <li>jpc.make_hpc_step()  to use hybrid PC (Tscshantz et al., 2023).</li> </ul>"},{"location":"api/Training/#jpc.make_pc_step","title":"<code>jpc.make_pc_step(model: PyTree[typing.Callable], optim: optax._src.base.GradientTransformation | optax._src.base.GradientTransformationExtraArgs, opt_state: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, typing.Iterable[ForwardRef(ArrayTree)], typing.Mapping[typing.Any, ForwardRef(ArrayTree)]], output: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex], *, input: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex, NoneType] = None, loss_id: str = 'mse', param_type: str = 'sp', ode_solver: AbstractSolver = Heun(), max_t1: int = 20, dt: jaxtyping.Shaped[Array, ''] | int = None, stepsize_controller: AbstractStepSizeController = PIDController(rtol=0.001, atol=0.001), skip_model: typing.Optional[jaxtyping.PyTree[typing.Callable]] = None, weight_decay: Shaped[Array, ''] = 0.0, spectral_penalty: Shaped[Array, ''] = 0.0, activity_decay: Shaped[Array, ''] = 0.0, key: typing.Union[jaxtyping.Key[Array, ''], jaxtyping.UInt32[Array, 2], NoneType] = None, layer_sizes: typing.Optional[jaxtyping.PyTree[int]] = None, batch_size: typing.Optional[int] = None, sigma: Shaped[Array, ''] = 0.05, record_activities: bool = False, record_energies: bool = False, record_every: int = None, activity_norms: bool = False, param_norms: bool = False, grad_norms: bool = False, calculate_accuracy: bool = False) -&gt; typing.Dict</code>","text":"<p>Performs one model parameter update with predictive coding.</p> <p>Main arguments:</p> <ul> <li><code>model</code>: List of callable model (e.g. neural network) layers.</li> <li><code>optim</code>: Optax optimiser, e.g. <code>optax.sgd()</code>.</li> <li><code>opt_state</code>: State of Optax optimiser.</li> <li><code>output</code>: Observation or target of the generative model.</li> </ul> <p>Note</p> <p><code>key</code>, <code>layer_sizes</code> and <code>batch_size</code> must be passed if <code>input = None</code>,  since unsupervised training will be assumed and activities need to be  initialised randomly.</p> <p>Other arguments:</p> <ul> <li><code>input</code>: Optional prior of the generative model.</li> <li><code>loss_id</code>: Loss function to use at the output layer. Options are mean squared      error <code>\"mse\"</code> (default) or cross-entropy <code>\"ce\"</code>.</li> <li><code>param_type</code>: Determines the parameterisation. Options are <code>\"sp\"</code>      (standard parameterisation), <code>\"mupc\"</code> (\u03bcPC),      or <code>\"ntp\"</code> (neural tangent parameterisation).      See <code>_get_param_scalings()</code>      for the specific scalings of these different parameterisations. Defaults     to <code>\"sp\"</code>.</li> <li><code>ode_solver</code>: diffrax ODE solver      to be used. Default is <code>Heun</code>,      a 2nd order explicit Runge--Kutta method.</li> <li><code>max_t1</code>: Maximum end of integration region (20 by default).</li> <li><code>dt</code>: Integration step size. Defaults to <code>None</code> since the default     <code>stepsize_controller</code> will automatically determine it.</li> <li><code>stepsize_controller</code>: diffrax controller      for step size integration. Defaults to <code>PIDController</code>.      Note that the relative and absolute tolerances of the controller will      also determine the steady state to terminate the solver.</li> <li><code>skip_model</code>: Optional list of callable skip connection functions.</li> <li><code>weight_decay</code>: Weight decay for the weights (0 by default).</li> <li><code>spectral_penalty</code>: Weight spectral penalty of the form      \\(||\\mathbf{I} - \\mathbf{W}_\\ell^T \\mathbf{W}_\\ell||^2\\) (0 by default).</li> <li><code>activity_decay</code>: Activity decay for the activities (0 by default).</li> <li><code>key</code>: <code>jax.random.PRNGKey</code> for random initialisation of activities.</li> <li><code>layer_sizes</code>: Dimension of all layers (input, hidden and output).</li> <li><code>batch_size</code>: Dimension of data batch for activity initialisation.</li> <li><code>sigma</code>: Standard deviation for Gaussian to sample activities from for     random initialisation. Defaults to 5e-2.</li> <li><code>record_activities</code>: If <code>True</code>, returns activities at every inference     iteration.</li> <li><code>record_energies</code>: If <code>True</code>, returns layer-wise energies at every     inference iteration.</li> <li><code>record_every</code>: int determining the sampling frequency the integration     steps.</li> <li><code>activity_norms</code>: If <code>True</code>, computes \\(\\ell^2\\) norm of the activities.</li> <li><code>param_norms</code>: If <code>True</code>, computes \\(\\ell^2\\) norm of the parameters.</li> <li><code>grad_norms</code>: If <code>True</code>, computes \\(\\ell^2\\) norm of parameter gradients.</li> <li><code>calculate_accuracy</code>: If <code>True</code>, computes the training accuracy.</li> </ul> <p>Returns:</p> <p>Dict including model (and optional skip model) with updated parameters,  updated optimiser state, loss, energies, activities, and optionally other  metrics (see other args above).</p> <p>Raises:</p> <ul> <li><code>ValueError</code> for inconsistent inputs and invalid losses.</li> </ul>"},{"location":"api/Training/#jpc.make_hpc_step","title":"<code>jpc.make_hpc_step(generator: PyTree[typing.Callable], amortiser: PyTree[typing.Callable], optims: typing.Tuple[optax._src.base.GradientTransformationExtraArgs], opt_states: typing.Tuple[typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, typing.Iterable[ForwardRef(ArrayTree)], typing.Mapping[typing.Any, ForwardRef(ArrayTree)]]], output: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex], *, input: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex, NoneType] = None, ode_solver: AbstractSolver = Heun(), max_t1: int = 300, dt: jaxtyping.Shaped[Array, ''] | int = None, stepsize_controller: AbstractStepSizeController = PIDController(rtol=0.001, atol=0.001), record_activities: bool = False, record_energies: bool = False) -&gt; typing.Dict</code>","text":"<p>Performs one update of the parameters of a hybrid predictive coding  network (Tscshantz et al., 2023).</p> Reference <pre><code>@article{tscshantz2023hybrid,\n  title={Hybrid predictive coding: Inferring, fast and slow},\n  author={Tscshantz, Alexander and Millidge, Beren and Seth, Anil K and Buckley, Christopher L},\n  journal={PLoS Computational Biology},\n  volume={19},\n  number={8},\n  pages={e1011280},\n  year={2023},\n  publisher={Public Library of Science San Francisco, CA USA}\n}\n</code></pre> <p>Note</p> <p>The input and output of the generator are the output and input of the amortiser, respectively.</p> <p>Main arguments:</p> <ul> <li><code>generator</code>: List of callable layers for the generative model.</li> <li><code>amortiser</code>: List of callable layers for model amortising the inference     of the <code>generator</code>.</li> <li><code>optims</code>: Optax optimisers (e.g. <code>optax.sgd()</code>), one for each model.</li> <li><code>opt_states</code>: State of Optax optimisers, one for each model.</li> <li><code>output</code>: Observation of the generator, input to the amortiser.</li> <li><code>input</code>: Optional prior of the generator, target for the amortiser.</li> </ul> <p>Other arguments:</p> <ul> <li><code>ode_solver</code>: diffrax ODE solver      to be used. Default is <code>Heun</code>,      a 2nd order explicit Runge--Kutta method.</li> <li><code>max_t1</code>: Maximum end of integration region (300 by default).</li> <li><code>dt</code>: Integration step size. Defaults to <code>None</code> since the default     <code>stepsize_controller</code> will automatically determine it.</li> <li><code>stepsize_controller</code>: diffrax controller      for step size integration. Defaults to <code>PIDController</code>.      Note that the relative and absolute tolerances of the controller will      also determine the steady state to terminate the solver.</li> <li><code>record_activities</code>: If <code>True</code>, returns activities at every inference     iteration.</li> <li><code>record_energies</code>: If <code>True</code>, returns layer-wise energies at every     inference iteration.</li> </ul> <p>Returns:</p> <p>Dict including models with updated parameters, optimiser state for each  model, model activities, last inference step for the generator, MSE losses, and energies.</p>"},{"location":"api/Utils/","title":"Utils","text":"<p>JPC provides several standard utilities for neural network training, including  creation of simple models, losses, and metrics.</p>"},{"location":"api/Utils/#jpc.make_mlp","title":"<code>jpc.make_mlp(key: typing.Union[jaxtyping.Key[Array, ''], jaxtyping.UInt32[Array, 2]], input_dim: int, width: int, depth: int, output_dim: int, act_fn: str, use_bias: bool = False, param_type: str = 'sp') -&gt; PyTree[typing.Callable]</code>","text":"<p>Creates a multi-layer perceptron compatible with predictive coding updates.</p> <p>Note</p> <p>This implementation places the activation function before the linear  transformation, \\(\\mathbf{W}_\\ell \\phi(\\mathbf{z}_{\\ell-1})\\), for  compatibility with the \u03bcPC  scalings when <code>param_type = \"mupc\"</code> in functions including  <code>jpc.init_activities_with_ffwd()</code>,  <code>jpc.update_activities()</code>,  and <code>jpc.update_params()</code>.</p> <p>Main arguments:</p> <ul> <li><code>key</code>: <code>jax.random.PRNGKey</code> for parameter initialisation.</li> <li><code>input_dim</code>: Input dimension.</li> <li><code>width</code>: Network width.</li> <li><code>depth</code>: Network depth.</li> <li><code>output_dim</code>: Output dimension.</li> <li><code>act_fn</code>: Activation function (for all layers except the output).</li> <li><code>use_bias</code>: <code>False</code> by default.</li> <li><code>param_type</code>: Determines the parameterisation. Options are <code>\"sp\"</code>      (standard parameterisation), <code>\"mupc\"</code> (\u03bcPC),      or <code>\"ntp\"</code> (neural tangent parameterisation). See <code>jpc._get_param_scalings()</code>      for the specific scalings of these different parameterisations. Defaults     to <code>\"sp\"</code>.</li> </ul> <p>Returns:</p> <p>List of callable fully connected layers.</p>"},{"location":"api/Utils/#jpc.make_skip_model","title":"<code>jpc.make_skip_model(depth: int) -&gt; PyTree[typing.Callable]</code>","text":"<p>Creates a residual network with one-layer skip connections at every layer  except from the input to the next layer and from the penultimate layer to  the output.</p> <p>This is used for compatibility with the \u03bcPC  parameterisation when <code>param_type = \"mupc\"</code> in functions including  <code>jpc.init_activities_with_ffwd()</code>,  <code>jpc.update_activities()</code>,  and <code>jpc.update_params()</code>.</p>"},{"location":"api/Utils/#jpc.get_act_fn","title":"<code>jpc.get_act_fn(name: str) -&gt; typing.Callable</code>","text":""},{"location":"api/Utils/#jpc.mse_loss","title":"<code>jpc.mse_loss(preds: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex], labels: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex]) -&gt; Shaped[Array, '']</code>","text":""},{"location":"api/Utils/#jpc.cross_entropy_loss","title":"<code>jpc.cross_entropy_loss(logits: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex], labels: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex]) -&gt; Shaped[Array, '']</code>","text":""},{"location":"api/Utils/#jpc.compute_accuracy","title":"<code>jpc.compute_accuracy(truths: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex], preds: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex]) -&gt; Shaped[Array, '']</code>","text":""},{"location":"api/Utils/#jpc.get_t_max","title":"<code>jpc.get_t_max(activities_iters: PyTree[jax.Array]) -&gt; Array</code>","text":""},{"location":"api/Utils/#jpc.compute_infer_energies","title":"<code>jpc.compute_infer_energies(params: typing.Tuple[jaxtyping.PyTree[typing.Callable], typing.Optional[jaxtyping.PyTree[typing.Callable]]], activities_iters: PyTree[jax.Array], t_max: Array, y: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex], *, x: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex, NoneType] = None, loss: str = 'mse', param_type: str = 'sp', weight_decay: Shaped[Array, ''] = 0.0, spectral_penalty: Shaped[Array, ''] = 0.0, activity_decay: Shaped[Array, ''] = 0.0) -&gt; PyTree[jaxtyping.Shaped[Array, '']]</code>","text":"<p>Calculates layer energies during predictive coding inference.</p> <p>Main arguments:</p> <ul> <li><code>params</code>: Tuple with callable model layers and optional skip connections.</li> <li><code>activities_iters</code>: Layer-wise activities at every inference iteration.     Note that each set of activities will have 4096 steps as first     dimension by diffrax default.</li> <li><code>t_max</code>: Maximum number of inference iterations to compute energies for.</li> <li><code>y</code>: Observation or target of the generative model.</li> </ul> <p>Other arguments:</p> <ul> <li><code>x</code>: Optional prior of the generative model.</li> <li><code>loss</code>: Loss function to use at the output layer (mean squared error     <code>\"mse\"</code> vs cross-entropy <code>\"ce\"</code>).</li> <li><code>param_type</code>: Determines the parameterisation. Options are <code>\"sp\"</code>,      <code>\"mupc\"</code>, or <code>\"ntp\"</code>.</li> <li><code>weight_decay</code>: Weight decay for the weights.</li> <li><code>spectral_penalty</code>: Spectral penalty for the weights.</li> <li><code>activity_decay</code>: Activity decay for the activities.</li> </ul> <p>Returns:</p> <p>List of layer-wise energies at every inference iteration.</p>"},{"location":"api/Utils/#jpc.compute_activity_norms","title":"<code>jpc.compute_activity_norms(activities: PyTree[jax.Array]) -&gt; Array</code>","text":"<p>Calculates \\(\\ell^2\\) norm of activities at each layer.</p>"},{"location":"api/Utils/#jpc.compute_param_norms","title":"<code>jpc.compute_param_norms(params)</code>","text":"<p>Calculates \\(\\ell^2\\) norm of all model parameters.</p>"},{"location":"examples/bidirectional_pc/","title":"Bidirectional PC on MNIST","text":"<p>This notebook demonstrates how to train a bidirectional predictive coding network (BPC; Olivers et al., 2025) that can both generate and classify MNIST digits. </p> <p>Note: BPC has the same architecture as a hybrid PC network (HPC; Tschantz et al., 2023), but the neural (inference) dynamics are driven by both top-down and bottom-up prediction errors. By contrast, in HPC the amortiser (or bottom-up network) only serves to initialise the inference dynamics.</p> <pre><code>%%capture\n!pip install torch==2.3.1\n!pip install torchvision==0.18.1`\n!pip install matplotlib==3.0.0\n</code></pre> <pre><code>import jpc\n\nimport jax\nimport jax.numpy as jnp\nimport equinox as eqx\nimport equinox.nn as nn\nimport optax\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\n\nimport io\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.simplefilter('ignore')  # ignore warnings\n</code></pre>"},{"location":"examples/bidirectional_pc/#hyperparameters","title":"Hyperparameters","text":"<p>We define some global parameters, including the network architecture, learning rate, batch size, etc.</p> <pre><code>SEED = 0\n\nINPUT_DIM = 10\nWIDTH = 300\nDEPTH = 3\nOUTPUT_DIM = 784\nACT_FN = \"relu\"\n\nACTIVITY_LR = 5e-1\nPARAM_LR = 1e-3\nBATCH_SIZE = 64\nTEST_EVERY = 100\nN_TRAIN_ITERS = 300\n</code></pre>"},{"location":"examples/bidirectional_pc/#dataset","title":"Dataset","text":"<p>Some utils to fetch MNIST.</p> <pre><code>def get_mnist_loaders(batch_size):\n    train_data = MNIST(train=True, normalise=True)\n    test_data = MNIST(train=False, normalise=True)\n    train_loader = DataLoader(\n        dataset=train_data,\n        batch_size=batch_size,\n        shuffle=True,\n        drop_last=True\n    )\n    test_loader = DataLoader(\n        dataset=test_data,\n        batch_size=batch_size,\n        shuffle=True,\n        drop_last=True\n    )\n    return train_loader, test_loader\n\n\nclass MNIST(datasets.MNIST):\n    def __init__(self, train, normalise=True, save_dir=\"data\"):\n        if normalise:\n            transform = transforms.Compose(\n                [\n                    transforms.ToTensor(),\n                    transforms.Normalize(\n                        mean=(0.1307), std=(0.3081)\n                    )\n                ]\n            )\n        else:\n            transform = transforms.Compose([transforms.ToTensor()])\n        super().__init__(save_dir, download=True, train=train, transform=transform)\n\n    def __getitem__(self, index):\n        img, label = super().__getitem__(index)\n        img = torch.flatten(img)\n        label = one_hot(label)\n        return img, label\n\n\ndef one_hot(labels, n_classes=10):\n    arr = torch.eye(n_classes)\n    return arr[labels]\n\n\ndef plot_mnist_imgs(imgs, labels, n_imgs=10):\n    plt.figure(figsize=(20, 2))\n    for i in range(n_imgs):\n        plt.subplot(1, n_imgs, i + 1)\n        plt.xticks([])\n        plt.yticks([])\n        plt.grid(False)\n        plt.imshow(imgs[i].reshape(28, 28), cmap=plt.cm.binary_r)\n        plt.xlabel(jnp.argmax(labels, axis=1)[i])\n    plt.show()\n</code></pre>"},{"location":"examples/bidirectional_pc/#train-and-test","title":"Train and test","text":"<pre><code>def evaluate(generator, amortiser, test_loader):\n    amort_accs = 0.\n    for _, (img_batch, label_batch) in enumerate(test_loader):\n        img_batch, label_batch = img_batch.numpy(), label_batch.numpy()\n\n        preds = jpc.init_activities_with_ffwd(\n            model=amortiser[::-1],\n            input=img_batch\n        )[-1]\n        amort_accs += jpc.compute_accuracy(label_batch, preds)\n\n    img_preds = jpc.init_activities_with_ffwd(\n        model=generator,\n        input=label_batch\n    )[-1]\n\n    return (\n        amort_accs / len(test_loader),\n        label_batch,\n        img_preds\n    )\n\n\ndef train(\n      seed,\n      input_dim,\n      width,\n      depth,\n      output_dim,\n      act_fn,\n      batch_size,\n      activity_lr,\n      param_lr,\n      test_every,\n      n_train_iters\n):\n    key = jax.random.PRNGKey(seed)\n    gen_key, amort_key = jax.random.split(key, 2)\n\n    # models (NOTE: input and output are inverted for the amortiser)\n    layer_sizes = [input_dim] + [width]*(depth-1) + [output_dim]\n    generator = jpc.make_mlp(\n        gen_key, \n        input_dim=input_dim,\n        width=width,\n        depth=depth,\n        output_dim=output_dim,\n        act_fn=act_fn\n    )\n    amortiser = jpc.make_mlp(\n        amort_key,\n        input_dim=output_dim,\n        width=width,\n        depth=depth,\n        output_dim=input_dim,\n        act_fn=act_fn\n    )[::-1]\n\n    # optimisers\n    activity_optim = optax.sgd(activity_lr)\n    gen_optim = optax.adam(param_lr)\n    amort_optim = optax.adam(param_lr)\n    optims = [gen_optim, amort_optim]\n\n    gen_opt_state = gen_optim.init(eqx.filter(generator, eqx.is_array))\n    amort_opt_state = amort_optim.init(eqx.filter(amortiser, eqx.is_array))\n    opt_states = [gen_opt_state, amort_opt_state]\n\n    # data\n    train_loader, test_loader = get_mnist_loaders(batch_size)\n\n    for iter, (img_batch, label_batch) in enumerate(train_loader):\n        img_batch, label_batch = img_batch.numpy(), label_batch.numpy()\n\n        # discriminative loss\n        amort_activities = jpc.init_activities_with_ffwd(\n            model=amortiser[::-1],\n            input=img_batch\n        )\n        amort_loss = jpc.mse_loss(amort_activities[-1], label_batch)\n\n        # generative loss &amp; initialisation\n        activities = jpc.init_activities_with_ffwd(\n            model=generator,\n            input=label_batch\n        )\n        gen_loss = jpc.mse_loss(activities[-1], img_batch)\n        activity_opt_state = activity_optim.init(activities)\n\n        # inference\n        for t in range(depth-1):\n            activity_update_result = jpc.update_bpc_activities(\n                top_down_model=generator,\n                bottom_up_model=amortiser,\n                activities=activities,\n                optim=activity_optim,\n                opt_state=activity_opt_state,\n                output=img_batch,\n                input=label_batch\n            )\n            activities = activity_update_result[\"activities\"]\n            activity_opt_state = activity_update_result[\"opt_state\"]\n\n        # learning\n        param_update_result = jpc.update_bpc_params(\n            top_down_model=generator,\n            bottom_up_model=amortiser,\n            activities=activities,\n            top_down_optim=gen_optim,\n            bottom_up_optim=amort_optim,\n            top_down_opt_state=gen_opt_state,\n            bottom_up_opt_state=amort_opt_state,\n            output=img_batch,\n            input=label_batch\n        )\n        generator, amortiser = param_update_result[\"models\"]\n        gen_opt_state, amort_opt_state  = param_update_result[\"opt_states\"]\n\n        if ((iter+1) % test_every) == 0:\n            amort_acc, label_batch, img_preds = evaluate(\n                generator,\n                amortiser,\n                test_loader\n            )\n            print(\n                f\"Iter {iter+1}, gen loss={gen_loss:4f}, \"\n                f\"amort loss={amort_loss:4f}, \"\n                f\"avg amort test accuracy={amort_acc:4f}\"\n            )\n\n            if (iter+1) &gt;= n_train_iters:\n                break\n\n    plot_mnist_imgs(img_preds, label_batch)\n</code></pre>"},{"location":"examples/bidirectional_pc/#run","title":"Run","text":"<pre><code>train(\n    seed=SEED,\n    input_dim=INPUT_DIM,\n    width=WIDTH,\n    depth=DEPTH,\n    output_dim=OUTPUT_DIM,\n    act_fn=ACT_FN,\n    batch_size=BATCH_SIZE,\n    activity_lr=ACTIVITY_LR,\n    param_lr=PARAM_LR,\n    test_every=TEST_EVERY,\n    n_train_iters=N_TRAIN_ITERS\n)\n</code></pre> <pre><code>Iter 100, gen loss=0.304481, amort loss=0.019504, avg amort test accuracy=82.952721\nIter 200, gen loss=0.289191, amort loss=0.021266, avg amort test accuracy=81.881012\nIter 300, gen loss=0.255959, amort loss=0.016586, avg amort test accuracy=83.143028\n</code></pre>"},{"location":"examples/discriminative_pc/","title":"Discriminative PC on MNIST","text":"<p>This notebook demonstrates how to train a simple feedforward network with predictive coding (PC) to discriminate or classify MNIST digits.</p> <pre><code>%%capture\n!pip install torch==2.3.1\n!pip install torchvision==0.18.1\n</code></pre> <pre><code>import jpc\n\nimport jax\nimport equinox as eqx\nimport equinox.nn as nn\nimport optax\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\n\nimport warnings\nwarnings.simplefilter('ignore')  # ignore warnings\n</code></pre>"},{"location":"examples/discriminative_pc/#hyperparameters","title":"Hyperparameters","text":"<p>We define some global parameters, including the network architecture, learning rate, batch size, etc.</p> <pre><code>SEED = 0\n\nINPUT_DIM = 784\nWIDTH = 300\nDEPTH = 3\nOUTPUT_DIM = 10\nACT_FN = \"relu\"\n\nLEARNING_RATE = 1e-3\nBATCH_SIZE = 64\nTEST_EVERY = 100\nN_TRAIN_ITERS = 300\n</code></pre>"},{"location":"examples/discriminative_pc/#dataset","title":"Dataset","text":"<p>Some utils to fetch MNIST.</p> <pre><code>def get_mnist_loaders(batch_size):\n    train_data = MNIST(train=True, normalise=True)\n    test_data = MNIST(train=False, normalise=True)\n    train_loader = DataLoader(\n        dataset=train_data,\n        batch_size=batch_size,\n        shuffle=True,\n        drop_last=True\n    )\n    test_loader = DataLoader(\n        dataset=test_data,\n        batch_size=batch_size,\n        shuffle=True,\n        drop_last=True\n    )\n    return train_loader, test_loader\n\n\nclass MNIST(datasets.MNIST):\n    def __init__(self, train, normalise=True, save_dir=\"data\"):\n        if normalise:\n            transform = transforms.Compose(\n                [\n                    transforms.ToTensor(),\n                    transforms.Normalize(\n                        mean=(0.1307), std=(0.3081)\n                    )\n                ]\n            )\n        else:\n            transform = transforms.Compose([transforms.ToTensor()])\n        super().__init__(save_dir, download=True, train=train, transform=transform)\n\n    def __getitem__(self, index):\n        img, label = super().__getitem__(index)\n        img = torch.flatten(img)\n        label = one_hot(label)\n        return img, label\n\n\ndef one_hot(labels, n_classes=10):\n    arr = torch.eye(n_classes)\n    return arr[labels]\n</code></pre>"},{"location":"examples/discriminative_pc/#network","title":"Network","text":"<p>For <code>jpc</code> to work, we need to provide a network with callable layers. This is easy to do with the PyTorch-like <code>nn.Sequential()</code> in equinox. For example, we can define a ReLU MLP with two hidden layers as follows</p> <pre><code>key = jax.random.PRNGKey(SEED)\n_, *subkeys = jax.random.split(key, 4)\nnetwork = [\n    nn.Sequential(\n        [\n            nn.Linear(784, 300, key=subkeys[0]),\n            nn.Lambda(jax.nn.relu)\n        ],\n    ),\n    nn.Sequential(\n        [\n            nn.Linear(300, 300, key=subkeys[1]),\n            nn.Lambda(jax.nn.relu)\n        ],\n    ),\n    nn.Linear(300, 10, key=subkeys[2]),\n]\n</code></pre> <p>You can also use <code>jpc.make_mlp()</code> to define a multi-layer perceptron (MLP) or fully connected network.</p> <pre><code>network = jpc.make_mlp(\n    key,\n    input_dim=INPUT_DIM,\n    width=WIDTH,\n    depth=DEPTH,\n    output_dim=OUTPUT_DIM,\n    act_fn=ACT_FN,\n    use_bias=True\n)\nprint(network)\n</code></pre> <pre><code>[Sequential(\n  layers=(\n    Lambda(fn=Identity()),\n    Linear(\n      weight=f32[300,784],\n      bias=f32[300],\n      in_features=784,\n      out_features=300,\n      use_bias=True\n    )\n  )\n), Sequential(\n  layers=(\n    Lambda(fn=&lt;PjitFunction of &lt;function relu at 0x10cea9c60&gt;&gt;),\n    Linear(\n      weight=f32[300,300],\n      bias=f32[300],\n      in_features=300,\n      out_features=300,\n      use_bias=True\n    )\n  )\n), Sequential(\n  layers=(\n    Lambda(fn=&lt;PjitFunction of &lt;function relu at 0x10cea9c60&gt;&gt;),\n    Linear(\n      weight=f32[10,300],\n      bias=f32[10],\n      in_features=300,\n      out_features=10,\n      use_bias=True\n    )\n  )\n)]\n</code></pre>"},{"location":"examples/discriminative_pc/#train-and-test","title":"Train and test","text":"<p>A PC network can be updated in a single line of code with <code>jpc.make_pc_step()</code>. Similarly, we can use <code>jpc.test_discriminative_pc()</code> to compute the network accuracy. Note that these functions are already \"jitted\" for optimised performance. Below we simply wrap each of these functions in training and test loops, respectively.</p> <pre><code>def evaluate(model, test_loader):\n    avg_test_loss, avg_test_acc = 0, 0\n    for _, (img_batch, label_batch) in enumerate(test_loader):\n        img_batch, label_batch = img_batch.numpy(), label_batch.numpy()\n\n        test_loss, test_acc = jpc.test_discriminative_pc(\n            model=model,\n            input=img_batch,\n            output=label_batch\n        )\n        avg_test_loss += test_loss\n        avg_test_acc += test_acc\n\n    return avg_test_loss / len(test_loader), avg_test_acc / len(test_loader)\n\n\ndef train(\n      model,\n      lr,\n      batch_size,\n      test_every,\n      n_train_iters\n):\n    optim = optax.adam(lr)\n    opt_state = optim.init(\n        (eqx.filter(model, eqx.is_array), None)\n    )\n    train_loader, test_loader = get_mnist_loaders(batch_size)\n\n    for iter, (img_batch, label_batch) in enumerate(train_loader):\n        img_batch, label_batch = img_batch.numpy(), label_batch.numpy()\n\n        result = jpc.make_pc_step(\n            model=model,\n            optim=optim,\n            opt_state=opt_state,\n            output=label_batch,\n            input=img_batch\n        )\n        model, opt_state = result[\"model\"], result[\"opt_state\"]\n        train_loss = result[\"loss\"]\n        if ((iter+1) % test_every) == 0:\n            _, avg_test_acc = evaluate(model, test_loader)\n            print(\n                f\"Train iter {iter+1}, train loss={train_loss:4f}, \"\n                f\"avg test accuracy={avg_test_acc:4f}\"\n            )\n            if (iter+1) &gt;= n_train_iters:\n                break\n</code></pre>"},{"location":"examples/discriminative_pc/#run","title":"Run","text":"<pre><code>train(\n    model=network,\n    lr=LEARNING_RATE,\n    batch_size=BATCH_SIZE,\n    test_every=TEST_EVERY,\n    n_train_iters=N_TRAIN_ITERS\n)\n</code></pre> <pre><code>Train iter 100, train loss=0.007197, avg test accuracy=93.309296\nTrain iter 200, train loss=0.005052, avg test accuracy=95.462738\nTrain iter 300, train loss=0.006984, avg test accuracy=95.903442\n</code></pre>"},{"location":"examples/hybrid_pc/","title":"Hybrid PC on MNIST","text":"<p>This notebook demonstrates how to train a hybrid predictive coding network (Tschantz et al., 2023) that can both generate and classify MNIST digits.</p> <pre><code>%%capture\n!pip install torch==2.3.1\n!pip install torchvision==0.18.1\n!pip install matplotlib==3.0.0\n</code></pre> <pre><code>import jpc\n\nimport jax\nimport jax.numpy as jnp\nimport equinox as eqx\nimport equinox.nn as nn\nimport optax\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\n\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.simplefilter('ignore')  # ignore warnings\n</code></pre>"},{"location":"examples/hybrid_pc/#hyperparameters","title":"Hyperparameters","text":"<p>We define some global parameters, including the network architecture, learning rate, batch size, etc.</p> <pre><code>SEED = 0\n\nINPUT_DIM = 10\nWIDTH = 300\nDEPTH = 3\nOUTPUT_DIM = 784\nACT_FN = \"relu\"\n\nLEARNING_RATE = 1e-3\nBATCH_SIZE = 64\nMAX_T1 = 50\nTEST_EVERY = 100\nN_TRAIN_ITERS = 300\n</code></pre>"},{"location":"examples/hybrid_pc/#dataset","title":"Dataset","text":"<p>Some utils to fetch MNIST.</p> <pre><code>def get_mnist_loaders(batch_size):\n    train_data = MNIST(train=True, normalise=True)\n    test_data = MNIST(train=False, normalise=True)\n    train_loader = DataLoader(\n        dataset=train_data,\n        batch_size=batch_size,\n        shuffle=True,\n        drop_last=True\n    )\n    test_loader = DataLoader(\n        dataset=test_data,\n        batch_size=batch_size,\n        shuffle=True,\n        drop_last=True\n    )\n    return train_loader, test_loader\n\n\nclass MNIST(datasets.MNIST):\n    def __init__(self, train, normalise=True, save_dir=\"data\"):\n        if normalise:\n            transform = transforms.Compose(\n                [\n                    transforms.ToTensor(),\n                    transforms.Normalize(\n                        mean=(0.1307), std=(0.3081)\n                    )\n                ]\n            )\n        else:\n            transform = transforms.Compose([transforms.ToTensor()])\n        super().__init__(save_dir, download=True, train=train, transform=transform)\n\n    def __getitem__(self, index):\n        img, label = super().__getitem__(index)\n        img = torch.flatten(img)\n        label = one_hot(label)\n        return img, label\n\n\ndef one_hot(labels, n_classes=10):\n    arr = torch.eye(n_classes)\n    return arr[labels]\n\n\ndef plot_mnist_imgs(imgs, labels, n_imgs=10):\n    plt.figure(figsize=(20, 2))\n    for i in range(n_imgs):\n        plt.subplot(1, n_imgs, i + 1)\n        plt.xticks([])\n        plt.yticks([])\n        plt.grid(False)\n        plt.imshow(imgs[i].reshape(28, 28), cmap=plt.cm.binary_r)\n        plt.xlabel(jnp.argmax(labels, axis=1)[i])\n    plt.show()\n</code></pre>"},{"location":"examples/hybrid_pc/#train-and-test","title":"Train and test","text":"<p>Similar to a standard PC network, a hybrid model can be trained in a single line of code with <code>jpc.make_hpc_step()</code>. Similarly, we can use <code>jpc.test_hpc()</code> to compute different test metrics. Note that these functions are already \"jitted\" for optimised performance. Below we simply wrap each of these functions in training and test loops, respectively.</p> <pre><code>def evaluate(\n      key,\n      layer_sizes,\n      batch_size,\n      generator,\n      amortiser,\n      test_loader\n):\n    amort_accs, hpc_accs, gen_accs = 0, 0, 0\n    for _, (img_batch, label_batch) in enumerate(test_loader):\n        img_batch, label_batch = img_batch.numpy(), label_batch.numpy()\n\n        amort_acc, hpc_acc, gen_acc, img_preds = jpc.test_hpc(\n            key=key,\n            layer_sizes=layer_sizes,\n            batch_size=batch_size,\n            generator=generator,\n            amortiser=amortiser,\n            input=label_batch,\n            output=img_batch\n        )\n        amort_accs += amort_acc\n        hpc_accs += hpc_acc\n        gen_accs += gen_acc\n\n    return (\n        amort_accs / len(test_loader),\n        hpc_accs / len(test_loader),\n        gen_accs / len(test_loader),\n        label_batch,\n        img_preds\n    )\n\n\ndef train(\n      seed,\n      input_dim,\n      width,\n      depth,\n      output_dim,\n      act_fn,\n      batch_size,\n      lr,\n      max_t1,\n      test_every,\n      n_train_iters\n):\n    key = jax.random.PRNGKey(seed)\n    key, *subkey = jax.random.split(key, 3)\n\n    layer_sizes = [input_dim] + [width]*(depth-1) + [output_dim]\n    generator = jpc.make_mlp(\n        subkey[0], \n        input_dim=input_dim,\n        width=width,\n        depth=depth,\n        output_dim=output_dim,\n        act_fn=act_fn\n    )\n    # NOTE: input and output are inverted for the amortiser\n    amortiser = jpc.make_mlp(\n        subkey[1],\n        input_dim=output_dim,\n        width=width,\n        depth=depth,\n        output_dim=input_dim,\n        act_fn=act_fn\n    )\n\n    gen_optim = optax.adam(lr)\n    amort_optim = optax.adam(lr)\n    optims = [gen_optim, amort_optim]\n\n    gen_opt_state = gen_optim.init(\n        (eqx.filter(generator, eqx.is_array), None)\n    )\n    amort_opt_state = amort_optim.init(eqx.filter(amortiser, eqx.is_array))\n    opt_states = [gen_opt_state, amort_opt_state]\n\n    train_loader, test_loader = get_mnist_loaders(batch_size)\n    for iter, (img_batch, label_batch) in enumerate(train_loader):\n        img_batch, label_batch = img_batch.numpy(), label_batch.numpy()\n\n        result = jpc.make_hpc_step(\n            generator=generator,\n            amortiser=amortiser,\n            optims=optims,\n            opt_states=opt_states,\n            input=label_batch,\n            output=img_batch,\n            max_t1=max_t1\n        )\n        generator, amortiser = result[\"generator\"], result[\"amortiser\"]\n        gen_loss, amort_loss = result[\"losses\"]\n        if ((iter+1) % test_every) == 0:\n            amort_acc, hpc_acc, gen_acc, label_batch, img_preds = evaluate(\n                key,\n                layer_sizes,\n                batch_size,\n                generator,\n                amortiser,\n                test_loader\n            )\n            print(\n                f\"Iter {iter+1}, gen loss={gen_loss:4f}, \"\n                f\"amort loss={amort_loss:4f}, \"\n                f\"avg amort test accuracy={amort_acc:4f}, \"\n                f\"avg hpc test accuracy={hpc_acc:4f}, \"\n                f\"avg gen test accuracy={gen_acc:4f}, \"\n            )\n            if (iter+1) &gt;= n_train_iters:\n                break\n\n    plot_mnist_imgs(img_preds, label_batch)\n    return amortiser, generator\n</code></pre>"},{"location":"examples/hybrid_pc/#run","title":"Run","text":"<pre><code>network = train(\n    seed=SEED,\n    input_dim=INPUT_DIM,\n    width=WIDTH,\n    depth=DEPTH,\n    output_dim=OUTPUT_DIM,\n    act_fn=ACT_FN,\n    batch_size=BATCH_SIZE,\n    lr=LEARNING_RATE,\n    max_t1=MAX_T1,\n    test_every=TEST_EVERY,\n    n_train_iters=N_TRAIN_ITERS\n)\n</code></pre> <pre><code>Iter 100, gen loss=0.617566, amort loss=0.052470, avg amort test accuracy=74.719551, avg hpc test accuracy=81.500404, avg gen test accuracy=81.390221, \nIter 200, gen loss=0.573021, amort loss=0.052784, avg amort test accuracy=80.669067, avg hpc test accuracy=82.341743, avg gen test accuracy=82.331734, \nIter 300, gen loss=0.531935, amort loss=0.041603, avg amort test accuracy=82.121391, avg hpc test accuracy=83.022835, avg gen test accuracy=83.203125,\n</code></pre>"},{"location":"examples/linear_net_theoretical_energy/","title":"Theoretical PC energy of deep linear networks","text":"<p>This notebook demonstrates how to compute the theoretical PC energy at the inference equilibrium \\(\\mathcal{F}^*\\) when \\(\\nabla_{\\mathbf{z}} \\mathcal{F} = \\mathbf{0}\\) for deep linear networks (Innocenti et al., 2024). For a set of inputs and outputs \\(\\{(\\mathbf{x}_i, \\mathbf{y}_i)\\}_{i=1}^N\\), this is given by </p> \\[\\begin{equation}     \\mathcal{F}^* = \\frac{1}{2N} \\sum_{i=1}^N (\\mathbf{y}_i - \\mathbf{W}_{L:1}\\mathbf{x}_i)^T \\mathbf{S}^{-1}(\\mathbf{y}_i - \\mathbf{W}_{L:1}\\mathbf{x}_i) \\end{equation}\\] <p>where \\(\\mathbf{S} = \\mathbf{I}_{d_y} + \\sum_{\\ell=2}^L (\\mathbf{W}_{L:\\ell})(\\mathbf{W}_{L:\\ell})^T\\) and \\(\\mathbf{W}_{k:\\ell} = \\mathbf{W}_k \\dots \\mathbf{W}_\\ell\\) for \\(\\ell, k \\in 1,\\dots, L\\). This result can be generalised to any linear layer transformation \\(\\mathbf{B}_\\ell\\), e.g. for a ResNet \\(\\mathbf{B}_\\ell = \\mathbf{I} + \\mathbf{W}_\\ell\\) (see Innocenti et al., 2025).</p> <pre><code>%%capture\n!pip install torch==2.3.1\n!pip install torchvision==0.18.1\n!pip install plotly==5.11.0\n!pip install -U kaleido\n</code></pre> <pre><code>import jpc\n\nimport jax\nimport jax.numpy as jnp\nimport equinox as eqx\nimport optax\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\n\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.simplefilter('ignore')  # ignore warnings\n</code></pre>"},{"location":"examples/linear_net_theoretical_energy/#hyperparameters","title":"Hyperparameters","text":"<p>We define some global parameters, including the network architecture, learning rate, batch size, etc.</p> <pre><code>SEED = 0\n\nINPUT_DIM = 784\nWIDTH = 300\nDEPTH = 5\nOUTPUT_DIM = 10\nACT_FN = \"linear\"\n\nLEARNING_RATE = 1e-3\nBATCH_SIZE = 64\nMAX_T1 = 300\nTEST_EVERY = 10\nN_TRAIN_ITERS = 100\n</code></pre>"},{"location":"examples/linear_net_theoretical_energy/#dataset","title":"Dataset","text":"<p>Some utils to fetch MNIST.</p> <pre><code>def get_mnist_loaders(batch_size):\n    train_data = MNIST(train=True, normalise=True)\n    test_data = MNIST(train=False, normalise=True)\n    train_loader = DataLoader(\n        dataset=train_data,\n        batch_size=batch_size,\n        shuffle=True,\n        drop_last=True\n    )\n    test_loader = DataLoader(\n        dataset=test_data,\n        batch_size=batch_size,\n        shuffle=True,\n        drop_last=True\n    )\n    return train_loader, test_loader\n\n\nclass MNIST(datasets.MNIST):\n    def __init__(self, train, normalise=True, save_dir=\"data\"):\n        if normalise:\n            transform = transforms.Compose(\n                [\n                    transforms.ToTensor(),\n                    transforms.Normalize(\n                        mean=(0.1307), std=(0.3081)\n                    )\n                ]\n            )\n        else:\n            transform = transforms.Compose([transforms.ToTensor()])\n        super().__init__(save_dir, download=True, train=train, transform=transform)\n\n    def __getitem__(self, index):\n        img, label = super().__getitem__(index)\n        img = torch.flatten(img)\n        label = one_hot(label)\n        return img, label\n\n\ndef one_hot(labels, n_classes=10):\n    arr = torch.eye(n_classes)\n    return arr[labels]\n</code></pre>"},{"location":"examples/linear_net_theoretical_energy/#plotting","title":"Plotting","text":"<pre><code>def plot_total_energies(energies):\n    n_train_iters = len(energies[\"theory\"])\n    train_iters = [b+1 for b in range(n_train_iters)]\n\n    _, ax = plt.subplots(figsize=(6, 3))\n\n    for energy_type, energy in energies.items():\n        is_theory = energy_type == \"theory\"\n        line_style = \"--\" if is_theory else \"-\"\n        color = \"black\" if is_theory else \"#00CC96\"  #\"rgb(27, 158, 119)\"\n\n        if color.startswith(\"rgb\"):\n            rgb = tuple(int(x)/255 for x in color[4:-1].split(\",\"))\n        else:\n            rgb = color\n\n        ax.plot(\n            train_iters, \n            energy, \n            label=energy_type, \n            linewidth=3 if is_theory else 2,\n            linestyle=line_style,\n            color=rgb\n        )\n\n    ax.legend(fontsize=16)\n    ax.set_xlabel(\"Training Iteration\", fontsize=18, labelpad=10)\n    ax.set_ylabel(\"Energy\", fontsize=18, labelpad=10)\n    ax.tick_params(axis='both', labelsize=14)\n    plt.grid(True)\n    plt.show()\n</code></pre>"},{"location":"examples/linear_net_theoretical_energy/#train-and-test","title":"Train and test","text":"<p>To compute the theoretical energy, we can use <code>jpc.compute_linear_equilib_energy()</code> which as clear from the equation above just takes a linear network and some data.</p> <pre><code>def evaluate(model, test_loader):\n    avg_test_loss, avg_test_acc = 0, 0\n    for _, (img_batch, label_batch) in enumerate(test_loader):\n        img_batch, label_batch = img_batch.numpy(), label_batch.numpy()\n\n        test_loss, test_acc = jpc.test_discriminative_pc(\n            model=model,\n            output=label_batch,\n            input=img_batch\n        )\n        avg_test_loss += test_loss\n        avg_test_acc += test_acc\n\n    return avg_test_loss / len(test_loader), avg_test_acc / len(test_loader)\n\n\ndef train( \n      input_dim,\n      width,\n      depth,\n      output_dim,\n      act_fn,\n      lr,\n      batch_size,\n      max_t1,\n      test_every,\n      n_train_iters\n):\n    key = jax.random.PRNGKey(0)\n\n    # NOTE: act_fn is linear and we use no biases \n    model = jpc.make_mlp(\n        key, \n        input_dim=input_dim,\n        width=width,\n        depth=depth,\n        output_dim=output_dim,\n        act_fn=act_fn,\n        use_bias=False\n    )\n    optim = optax.adam(lr)\n    opt_state = optim.init(\n        (eqx.filter(model, eqx.is_array), None)\n    )\n    train_loader, test_loader = get_mnist_loaders(batch_size)\n\n    num_total_energies, theory_total_energies = [], []\n    for iter, (img_batch, label_batch) in enumerate(train_loader):\n        img_batch, label_batch = img_batch.numpy(), label_batch.numpy()\n\n        theory_total_energies.append(\n            jpc.compute_linear_equilib_energy(\n                network=model, \n                x=img_batch, \n                y=label_batch\n            )\n        )\n        result = jpc.make_pc_step(\n            model,\n            optim,\n            opt_state,\n            output=label_batch,\n            input=img_batch,\n            max_t1=max_t1,\n            record_energies=True\n        )\n        model, opt_state = result[\"model\"], result[\"opt_state\"]\n        train_loss, t_max = result[\"loss\"], result[\"t_max\"]\n        num_total_energies.append(result[\"energies\"][:, t_max-1].sum())\n\n        if ((iter+1) % test_every) == 0:\n            _, avg_test_acc = evaluate(model, test_loader)\n            print(\n                f\"Train iter {iter+1}, train loss={train_loss:4f}, \"\n                f\"avg test accuracy={avg_test_acc:4f}\"\n            )\n            if (iter+1) &gt;= n_train_iters:\n                break\n\n    return {\n        \"theory\": jnp.array(theory_total_energies),\n        \"experiment\": jnp.array(num_total_energies)\n    }\n</code></pre>"},{"location":"examples/linear_net_theoretical_energy/#run","title":"Run","text":"<p>Below we plot the theoretical energy against the numerical one.</p> <pre><code>energies = train(\n    input_dim=INPUT_DIM,\n    width=WIDTH,\n    depth=DEPTH,\n    output_dim=OUTPUT_DIM,\n    act_fn=ACT_FN,\n    lr=LEARNING_RATE,\n    batch_size=BATCH_SIZE,\n    test_every=TEST_EVERY,\n    max_t1=MAX_T1,\n    n_train_iters=N_TRAIN_ITERS\n)\nplot_total_energies(energies)\n</code></pre> <pre><code>Train iter 10, train loss=0.028995, avg test accuracy=74.729568\nTrain iter 20, train loss=0.029823, avg test accuracy=79.076523\nTrain iter 30, train loss=0.025171, avg test accuracy=78.315308\nTrain iter 40, train loss=0.024210, avg test accuracy=82.892632\nTrain iter 50, train loss=0.025814, avg test accuracy=81.700722\nTrain iter 60, train loss=0.026777, avg test accuracy=81.710739\nTrain iter 70, train loss=0.027313, avg test accuracy=82.481972\nTrain iter 80, train loss=0.026225, avg test accuracy=81.209938\nTrain iter 90, train loss=0.025579, avg test accuracy=81.280045\nTrain iter 100, train loss=0.022040, avg test accuracy=78.806091\n</code></pre> <p></p>"},{"location":"examples/mupc/","title":"\u03bcPC","text":"<p>This notebook demonstrates how to train residual networks with \u03bcPC, a reparameterisation of PC that allows stable training of very deep (100+ layer) networks while also enabling zero-shot hyperparameter transfer (see Innocenti et al., 2025).</p> <pre><code>%%capture\n!pip install torch==2.3.1\n!pip install torchvision==0.18.1\n</code></pre> <pre><code>import jpc\n\nimport jax.random as jr\nimport equinox as eqx\nimport equinox.nn as nn\nimport optax\n\nimport math\nimport random\nimport numpy as np\nfrom typing import List, Callable\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\n\nimport warnings\nwarnings.simplefilter('ignore')  # ignore warnings\n</code></pre> <pre><code># for reproducibility\ndef set_global_seed(seed):\n    torch.manual_seed(seed)             \n    torch.cuda.manual_seed(seed)            \n    torch.cuda.manual_seed_all(seed)        \n    np.random.seed(seed)                  \n    random.seed(seed)                       \n    torch.backends.cudnn.deterministic = True \n    torch.backends.cudnn.benchmark = False\n</code></pre>"},{"location":"examples/mupc/#hyperparameters","title":"Hyperparameters","text":"<p>We define some global parameters, including the network architecture, learning rate, batch size, etc. We choose a network with \"only\" 30 layers and 128 hidden neurons so that it can run relatively fast on a CPU, but feel free to try deeper and wider networks.</p> <pre><code>SEED = 4329\n\nINPUT_DIM = 784\nWIDTH = 128\nDEPTH = 30\nOUTPUT_DIM = 10\nACT_FN = \"relu\"\n\nACTIVITY_LR = 5e-1\nPARAM_LR = 1e-1\nBATCH_SIZE = 64\nTEST_EVERY = 100\nN_TRAIN_ITERS = 900\n</code></pre>"},{"location":"examples/mupc/#dataset","title":"Dataset","text":"<p>Some utils to fetch MNIST.</p> <pre><code>def get_mnist_loaders(batch_size):\n    train_data = MNIST(train=True, normalise=True)\n    test_data = MNIST(train=False, normalise=True)\n    train_loader = DataLoader(\n        dataset=train_data,\n        batch_size=batch_size,\n        shuffle=True,\n        drop_last=True\n    )\n    test_loader = DataLoader(\n        dataset=test_data,\n        batch_size=batch_size,\n        shuffle=True,\n        drop_last=True\n    )\n    return train_loader, test_loader\n\n\nclass MNIST(datasets.MNIST):\n    def __init__(self, train, normalise=True, save_dir=\"data\"):\n        if normalise:\n            transform = transforms.Compose(\n                [\n                    transforms.ToTensor(),\n                    transforms.Normalize(\n                        mean=(0.1307), std=(0.3081)\n                    )\n                ]\n            )\n        else:\n            transform = transforms.Compose([transforms.ToTensor()])\n        super().__init__(save_dir, download=True, train=train, transform=transform)\n\n    def __getitem__(self, index):\n        img, label = super().__getitem__(index)\n        img = torch.flatten(img)\n        label = one_hot(label)\n        return img, label\n\n\ndef one_hot(labels, n_classes=10):\n    arr = torch.eye(n_classes)\n    return arr[labels]\n</code></pre>"},{"location":"examples/mupc/#creating-a-pc-model","title":"Creating a \u03bcPC model","text":"<p>To parameterise a model with \u03bcPC, one can use a few convenience functions of <code>jpc</code> to create an MLP or fully connected network with <code>jpc.make_mlp()</code> and an associated skip model with <code>jpc.make_skip model()</code>. Note that \u03bcPC works only for a specific type of ResNet, namely one with one-layer skip connections at every layer except from the input to the next layer and from the penultimate layer to the output (see Innocenti et al., 2025), as shown below.</p> <pre><code>key = jr.PRNGKey(SEED)\n\n# MLP\nmodel = jpc.make_mlp(\n    key,\n    input_dim=INPUT_DIM,\n    width=WIDTH,\n    depth=DEPTH,\n    output_dim=OUTPUT_DIM,\n    act_fn=ACT_FN,\n    param_type=\"mupc\"\n)\n\n# skip model\nskip_model = jpc.make_skip_model(DEPTH)\n</code></pre> <p>At training and test time we would need to pass both models to relevant <code>jpc</code> functions and change the argument <code>param_type = \"mupc\"</code> (default is <code>\"sp\"</code> for standard parameterisation). </p> <p>Alternatively, one could define a model class embedding the parameterisation itself and leave the above arguments to their default. This solution is more elegant but it can be harder to debug, at least for a fully connected architecture. However, if you would like to experiment with different parameterisations and more complex architectures (e.g. CNNs), we recommend this approach. </p> <pre><code>class ScaledLinear(eqx.Module):\n    \"\"\"Scaled linear transformation.\"\"\"\n    linear: nn.Linear\n    scaling: float = eqx.static_field()\n\n    def __init__(\n            self,\n            in_features,\n            out_features,\n            *,\n            key,\n            scaling=1.,\n            param_type=\"sp\",\n            use_bias=False\n    ):\n        keys = jr.split(key, 2)\n        linear = nn.Linear(\n            in_features, \n            out_features, \n            use_bias=use_bias,\n            key=keys[0]\n        )\n        if param_type == \"mupc\":\n            W = jr.normal(keys[1], linear.weight.shape)\n            linear = eqx.tree_at(lambda l: l.weight, linear, W)\n\n        self.linear = linear\n        self.scaling = scaling\n\n    def __call__(self, x):\n        return self.scaling * self.linear(x)\n\n\nclass ResNetBlock(eqx.Module):\n    \"\"\"Identity residual block applying activation and a scaled linear layer.\"\"\"\n    act_fn: Callable = eqx.static_field()\n    scaled_linear: ScaledLinear\n\n    def __init__(\n        self,\n        in_features,\n        out_features,\n        *,\n        key,\n        scaling=1.,\n        param_type=\"sp\",\n        use_bias=False,\n        act_fn=\"linear\"\n    ):\n        self.act_fn = act_fn\n        self.scaled_linear = ScaledLinear(\n            in_features=in_features,\n            out_features=out_features,\n            key=key,\n            scaling=scaling,\n            param_type=param_type,\n            use_bias=use_bias\n        )\n\n    def __call__(self, x):\n        res_path = x\n        x = self.act_fn(x)\n        return self.scaled_linear(x) + res_path\n\n\nclass Readout(eqx.Module):\n    \"\"\"Final network layer applying activation and a scaled linear layer.\"\"\"\n    act_fn: Callable = eqx.static_field()\n    scaled_linear: ScaledLinear\n\n    def __init__(\n        self,\n        in_features,\n        out_features,\n        *,\n        key,\n        scaling=1.,\n        param_type=\"sp\",\n        use_bias=False,\n        act_fn=\"linear\"\n    ):\n        self.act_fn = act_fn\n        self.scaled_linear = ScaledLinear(\n            in_features=in_features,\n            out_features=out_features,\n            key=key,\n            scaling=scaling,\n            param_type=param_type,\n            use_bias=use_bias\n        )\n\n    def __call__(self, x):\n        x = self.act_fn(x)\n        return self.scaled_linear(x)\n\n\nclass FCResNet(eqx.Module):\n    \"\"\"Fully-connected ResNet compatible with different parameterisations.\"\"\"\n    layers: List[eqx.Module]\n\n    def __init__(\n            self, \n            *,\n            key, \n            in_dim, \n            width, \n            depth, \n            out_dim, \n            act_fn=\"linear\", \n            use_bias=False,\n            param_type=\"sp\"\n        ):\n        act_fn = jpc.get_act_fn(act_fn)\n        if param_type == \"sp\":\n            in_scaling = 1.\n            hidden_scaling = 1.\n            out_scaling = 1.\n\n        elif param_type == \"mupc\":\n            in_scaling = 1 / math.sqrt(in_dim)\n            hidden_scaling = 1 / math.sqrt(width * depth)\n            out_scaling = 1 / width\n\n        keys = jr.split(key, depth)\n        self.layers = [\n            ScaledLinear(\n                key=keys[0],\n                in_features=in_dim,\n                out_features=width,\n                scaling=in_scaling,\n                param_type=param_type,\n                use_bias=use_bias\n            )\n        ]\n\n        for i in range(1, depth - 1):\n            self.layers.append(\n                ResNetBlock(\n                    key=keys[i],\n                    in_features=width,\n                    out_features=width,\n                    scaling=hidden_scaling,\n                    param_type=param_type,\n                    use_bias=use_bias,\n                    act_fn=act_fn\n                )\n            )\n\n        self.layers.append(\n            Readout(\n                key=keys[-1],\n                in_features=width,\n                out_features=out_dim,\n                scaling=out_scaling,\n                param_type=param_type,\n                use_bias=use_bias,\n                act_fn=act_fn\n            )\n        )\n\n    def __call__(self, x):\n        for f in self.layers:\n            x = f(x)      \n        return x\n\n    def __len__(self):\n        return len(self.layers)\n\n    def __getitem__(self, idx):\n        return self.layers[idx]\n</code></pre> <pre><code>mupc_model = FCResNet(\n    key=key, \n    in_dim=INPUT_DIM, \n    width=WIDTH, \n    depth=DEPTH, \n    out_dim=OUTPUT_DIM, \n    act_fn=ACT_FN, \n    use_bias=False, \n    param_type=\"mupc\"\n)\n</code></pre> <p>The following makes sure that the models have identical weights.</p> <pre><code>mupc_model = FCResNet(\n    key=key, \n    in_dim=INPUT_DIM, \n    width=WIDTH, \n    depth=DEPTH, \n    out_dim=OUTPUT_DIM, \n    act_fn=ACT_FN, \n    use_bias=False, \n    param_type=\"mupc\"\n)\nmupc_model = eqx.tree_at(\n    where=lambda tree: tree[0].linear.weight,\n    pytree=mupc_model,\n    replace=model[0][1].weight\n)\nfor l in range(1, len(model)):\n    mupc_model = eqx.tree_at(\n        where=lambda tree: tree[l].scaled_linear.linear.weight,\n        pytree=mupc_model,\n        replace=model[l][1].weight\n    )\n</code></pre>"},{"location":"examples/mupc/#train-and-test","title":"Train and test","text":"<p>For training, we use the advanced API including the functions <code>jpc.init_activities_with_ffwd()</code> to initialise the activities, <code>jpc.update_activities()</code> to perform PC inference, and <code>jpc.update_params()</code> to update the weights. All these functions accept <code>skip_model</code> and <code>param_type</code> as arguments. Note, however, that one can replace these functions with <code>jpc.make_pc_step()</code>. For testing, we use <code>jpc.test_discriminative_pc()</code>.</p> <pre><code>def evaluate(model, skip_model, test_loader, param_type):\n    avg_test_acc = 0\n    for _, (img_batch, label_batch) in enumerate(test_loader):\n        img_batch, label_batch = img_batch.numpy(), label_batch.numpy()\n\n        _, test_acc = jpc.test_discriminative_pc(\n            model=model,\n            input=img_batch,\n            output=label_batch,\n            skip_model=skip_model,\n            param_type=param_type\n        )\n        avg_test_acc += test_acc\n\n    return avg_test_acc / len(test_loader)\n\n\ndef train(\n      seed,  \n      model,\n      skip_model,\n      param_type,\n      activity_lr,  \n      param_lr,\n      batch_size,\n      test_every,\n      n_train_iters\n):  \n    set_global_seed(seed)\n    activity_optim = optax.sgd(activity_lr)\n    param_optim = optax.adam(param_lr)\n    param_opt_state = param_optim.init(\n        (eqx.filter(model, eqx.is_array), skip_model)\n    )\n    train_loader, test_loader = get_mnist_loaders(batch_size)\n\n    for iter, (img_batch, label_batch) in enumerate(train_loader):\n        img_batch, label_batch = img_batch.numpy(), label_batch.numpy()\n\n        # initialise activities\n        activities = jpc.init_activities_with_ffwd(\n            model=model,\n            input=img_batch,\n            skip_model=skip_model,\n            param_type=param_type\n        )\n        activity_opt_state = activity_optim.init(activities)\n        train_loss = jpc.mse_loss(activities[-1], label_batch)\n\n        # inference\n        for t in range(len(model)):\n            activity_update_result = jpc.update_activities(\n                params=(model, skip_model),\n                activities=activities,\n                optim=activity_optim,\n                opt_state=activity_opt_state,\n                output=label_batch,\n                input=img_batch,\n                param_type=param_type\n            )\n            activities = activity_update_result[\"activities\"]\n            activity_opt_state = activity_update_result[\"opt_state\"]\n\n        # learning\n        param_update_result = jpc.update_params(\n            params=(model, skip_model),\n            activities=activities,\n            optim=param_optim,\n            opt_state=param_opt_state,\n            output=label_batch,\n            input=img_batch,\n            param_type=param_type\n        )\n        model = param_update_result[\"model\"]\n        skip_model = param_update_result[\"skip_model\"]\n        param_opt_state = param_update_result[\"opt_state\"]\n\n        if np.isinf(train_loss) or np.isnan(train_loss):\n            print(\n                f\"Stopping training because of divergence, train loss={train_loss}\"\n            )\n            break\n\n        if ((iter+1) % test_every) == 0:\n            avg_test_acc = evaluate(\n                model=model,\n                skip_model=skip_model, \n                test_loader=test_loader, \n                param_type=param_type\n            )\n            print(\n                f\"Train iter {iter+1}, train loss={train_loss:4f}, \"\n                f\"avg test accuracy={avg_test_acc:4f}\"\n            )\n            if (iter+1) &gt;= n_train_iters:\n                break\n</code></pre>"},{"location":"examples/mupc/#run","title":"Run","text":"<p>Note that on a CPU the script below should take about a minute to complete.</p> <pre><code>train(\n    seed=SEED,\n    model=model,\n    skip_model=skip_model,\n    param_type=\"mupc\",\n    activity_lr=ACTIVITY_LR,\n    param_lr=PARAM_LR,\n    batch_size=BATCH_SIZE,\n    test_every=TEST_EVERY,\n    n_train_iters=N_TRAIN_ITERS\n)\n</code></pre> <pre><code>Train iter 100, train loss=0.016015, avg test accuracy=85.827324\nTrain iter 200, train loss=0.012215, avg test accuracy=88.541664\nTrain iter 300, train loss=0.009235, avg test accuracy=90.805290\nTrain iter 400, train loss=0.008675, avg test accuracy=91.286057\nTrain iter 500, train loss=0.011475, avg test accuracy=91.836937\nTrain iter 600, train loss=0.007697, avg test accuracy=92.177483\nTrain iter 700, train loss=0.007377, avg test accuracy=92.778442\nTrain iter 800, train loss=0.009710, avg test accuracy=92.477966\nTrain iter 900, train loss=0.009722, avg test accuracy=93.259216\n</code></pre> <p>For comparison, try to change to the standard parameterisation with <code>param_type = \"sp\"</code>. </p> <p>If you are using your own \u03bcPC-parameterised model class, then you can leave the default <code>skip_model = None</code> and <code>param_type = \"sp\"</code>, as shown below.</p> <pre><code>train(\n    seed=SEED,\n    model=mupc_model,\n    skip_model=None,\n    param_type=\"sp\",\n    activity_lr=ACTIVITY_LR,\n    param_lr=PARAM_LR,\n    batch_size=BATCH_SIZE,\n    test_every=TEST_EVERY,\n    n_train_iters=N_TRAIN_ITERS\n)\n</code></pre> <pre><code>Train iter 100, train loss=0.016063, avg test accuracy=85.787262\nTrain iter 200, train loss=0.012327, avg test accuracy=88.571716\nTrain iter 300, train loss=0.009621, avg test accuracy=90.875404\nTrain iter 400, train loss=0.009056, avg test accuracy=91.336136\nTrain iter 500, train loss=0.011603, avg test accuracy=92.007210\nTrain iter 600, train loss=0.007781, avg test accuracy=91.887016\nTrain iter 700, train loss=0.006997, avg test accuracy=92.938705\nTrain iter 800, train loss=0.010020, avg test accuracy=93.129005\nTrain iter 900, train loss=0.009978, avg test accuracy=93.279243\n</code></pre>"},{"location":"examples/supervised_generative_pc/","title":"Supervised Generative PC on MNIST","text":"<p>This notebook demonstrates how to train a simple feedforward network with predictive coding to generate MNIST digits.</p> <pre><code>%%capture\n!pip install torch==2.3.1\n!pip install torchvision==0.18.1\n!pip install matplotlib==3.0.0\n</code></pre> <pre><code>import jpc\n\nimport jax\nimport jax.numpy as jnp\nimport equinox as eqx\nimport equinox.nn as nn\nimport optax\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\n\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.simplefilter('ignore')  # ignore warnings\n</code></pre>"},{"location":"examples/supervised_generative_pc/#hyperparameters","title":"Hyperparameters","text":"<p>We define some global parameters, including the network architecture, learning rate, batch size, etc.</p> <pre><code>SEED = 0\n\nINPUT_DIM = 10\nWIDTH = 300\nDEPTH = 3\nOUTPUT_DIM = 784\nACT_FN = \"relu\"\n\nLEARNING_RATE = 1e-3\nBATCH_SIZE = 64\nMAX_T1 = 100\nTEST_EVERY = 50\nN_TRAIN_ITERS = 200\n</code></pre>"},{"location":"examples/supervised_generative_pc/#dataset","title":"Dataset","text":"<p>Some utils to fetch and plot MNIST.</p> <pre><code>def get_mnist_loaders(batch_size):\n    train_data = MNIST(train=True, normalise=True)\n    test_data = MNIST(train=False, normalise=True)\n    train_loader = DataLoader(\n        dataset=train_data,\n        batch_size=batch_size,\n        shuffle=True,\n        drop_last=True\n    )\n    test_loader = DataLoader(\n        dataset=test_data,\n        batch_size=batch_size,\n        shuffle=True,\n        drop_last=True\n    )\n    return train_loader, test_loader\n\n\nclass MNIST(datasets.MNIST):\n    def __init__(self, train, normalise=True, save_dir=\"data\"):\n        if normalise:\n            transform = transforms.Compose(\n                [\n                    transforms.ToTensor(),\n                    transforms.Normalize(\n                        mean=(0.1307), std=(0.3081)\n                    )\n                ]\n            )\n        else:\n            transform = transforms.Compose([transforms.ToTensor()])\n        super().__init__(save_dir, download=True, train=train, transform=transform)\n\n    def __getitem__(self, index):\n        img, label = super().__getitem__(index)\n        img = torch.flatten(img)\n        label = one_hot(label)\n        return img, label\n\n\ndef one_hot(labels, n_classes=10):\n    arr = torch.eye(n_classes)\n    return arr[labels]\n\n\ndef plot_mnist_img_preds(imgs, labels, n_imgs=10):\n    plt.figure(figsize=(20, 2))\n    for i in range(n_imgs):\n        plt.subplot(1, n_imgs, i + 1)\n        plt.xticks([])\n        plt.yticks([])\n        plt.grid(False)\n        plt.imshow(imgs[i].reshape(28, 28), cmap=plt.cm.binary_r)\n        plt.xlabel(jnp.argmax(labels, axis=1)[i], fontsize=16)\n    plt.show()\n</code></pre>"},{"location":"examples/supervised_generative_pc/#network","title":"Network","text":"<p>For <code>jpc</code> to work, we need to provide a network with callable layers. This is easy to do with the PyTorch-like <code>nn.Sequential()</code> in equinox. For example, we can define a ReLU MLP with two hidden layers as follows</p> <pre><code>key = jax.random.PRNGKey(SEED)\nkey, *subkeys = jax.random.split(key, 4)\nnetwork = [\n    nn.Sequential(\n        [\n            nn.Linear(10, 300, key=subkeys[0]),\n            nn.Lambda(jax.nn.relu)\n        ],\n    ),\n    nn.Sequential(\n        [\n            nn.Linear(300, 300, key=subkeys[1]),\n            nn.Lambda(jax.nn.relu)\n        ],\n    ),\n    nn.Linear(300, 784, key=subkeys[2]),\n]\n</code></pre> <p>You can also use <code>jpc.make_mlp()</code> to define a multi-layer perceptron (MLP) or fully connected network.</p> <pre><code>network = jpc.make_mlp(\n    key,\n    input_dim=INPUT_DIM,\n    width=WIDTH,\n    depth=DEPTH,\n    output_dim=OUTPUT_DIM,\n    act_fn=ACT_FN,\n    use_bias=True\n)\nprint(network)\n</code></pre> <pre><code>[Sequential(\n  layers=(\n    Lambda(fn=Identity()),\n    Linear(\n      weight=f32[300,10],\n      bias=f32[300],\n      in_features=10,\n      out_features=300,\n      use_bias=True\n    )\n  )\n), Sequential(\n  layers=(\n    Lambda(fn=&lt;PjitFunction of &lt;function relu at 0x117801bd0&gt;&gt;),\n    Linear(\n      weight=f32[300,300],\n      bias=f32[300],\n      in_features=300,\n      out_features=300,\n      use_bias=True\n    )\n  )\n), Sequential(\n  layers=(\n    Lambda(fn=&lt;PjitFunction of &lt;function relu at 0x117801bd0&gt;&gt;),\n    Linear(\n      weight=f32[784,300],\n      bias=f32[784],\n      in_features=300,\n      out_features=784,\n      use_bias=True\n    )\n  )\n)]\n</code></pre>"},{"location":"examples/supervised_generative_pc/#train-and-test","title":"Train and test","text":"<p>A PC network can be updated in a single line of code with <code>jpc.make_pc_step()</code>. Similarly, we can use <code>jpc.test_generative_pc()</code> to compute the network accuracy. Note that these functions are already \"jitted\" for optimised performance. Below we simply wrap each of these functions in training and test loops, respectively. </p> <p>Note that to train in an unsupervised way, you would simply need to remove the <code>input</code> from <code>jpc.make_pc_step()</code> and the <code>evaluate()</code> script. See this example notebook.</p> <pre><code>def evaluate(key, layer_sizes, batch_size, network, test_loader, max_t1):\n    test_acc = 0\n    for _, (img_batch, label_batch) in enumerate(test_loader):\n        img_batch, label_batch = img_batch.numpy(), label_batch.numpy()\n\n        acc, img_preds = jpc.test_generative_pc(\n            model=network,\n            input=label_batch,\n            output=img_batch,\n            key=key,\n            layer_sizes=layer_sizes,\n            batch_size=batch_size,\n            max_t1=max_t1\n        )\n        test_acc += acc\n\n    avg_test_acc = test_acc / len(test_loader)\n\n    return avg_test_acc, label_batch, img_preds\n\n\ndef train(\n      key,\n      input_dim,\n      width,\n      depth,\n      output_dim,\n      batch_size,\n      network,\n      lr,\n      max_t1,\n      test_every,\n      n_train_iters\n):\n    layer_sizes = [input_dim] + [width]*(depth-1) + [output_dim]\n    optim = optax.adam(lr)\n    opt_state = optim.init(\n        (eqx.filter(network, eqx.is_array), None)\n    )\n    train_loader, test_loader = get_mnist_loaders(batch_size)\n\n    for iter, (img_batch, label_batch) in enumerate(train_loader):\n        img_batch, label_batch = img_batch.numpy(), label_batch.numpy()\n\n        result = jpc.make_pc_step(\n            model=network,\n            optim=optim,\n            opt_state=opt_state,\n            input=label_batch,\n            output=img_batch,\n            max_t1=max_t1\n        )\n        network, opt_state = result[\"model\"], result[\"opt_state\"]\n        train_loss = result[\"loss\"]\n        if ((iter+1) % test_every) == 0:\n            avg_test_acc, test_label_batch, img_preds = evaluate(\n                key,\n                layer_sizes,\n                batch_size,\n                network,\n                test_loader,\n                max_t1=max_t1\n            )\n            print(\n                f\"Train iter {iter+1}, train loss={train_loss:4f}, \"\n                f\"avg test accuracy={avg_test_acc:4f}\"\n            )\n            if (iter+1) &gt;= n_train_iters:\n                break\n\n    plot_mnist_img_preds(img_preds, test_label_batch)\n    return network\n</code></pre>"},{"location":"examples/supervised_generative_pc/#run","title":"Run","text":"<pre><code>network = train(\n    key=key,\n    input_dim=INPUT_DIM,\n    width=WIDTH,\n    depth=DEPTH,\n    output_dim=OUTPUT_DIM,\n    batch_size=BATCH_SIZE,\n    network=network,\n    lr=LEARNING_RATE,\n    max_t1=MAX_T1,\n    test_every=TEST_EVERY,\n    n_train_iters=N_TRAIN_ITERS\n)\n</code></pre> <pre><code>Train iter 50, train loss=0.312354, avg test accuracy=79.717545\nTrain iter 100, train loss=0.275381, avg test accuracy=83.794067\nTrain iter 150, train loss=0.293271, avg test accuracy=84.755608\nTrain iter 200, train loss=0.297628, avg test accuracy=84.785660\n</code></pre>"},{"location":"examples/unsupervised_generative_pc/","title":"Unsupervised Generative PC on MNIST","text":"<p>This notebook demonstrates how to train a simple feedforward network with predictive coding to encode MNIST digits in an unsupervised manner.</p> <pre><code>%%capture\n!pip install torch==2.3.1\n!pip install torchvision==0.18.1\n!pip install matplotlib==3.0.0\n</code></pre> <pre><code>import jpc\n\nimport jax\nimport equinox as eqx\nimport equinox.nn as nn\nimport optax\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\n\nimport matplotlib.pyplot as plt\nimport matplotlib.colors as mcolors\n\nimport warnings\nwarnings.simplefilter('ignore')  # ignore warnings\n</code></pre>"},{"location":"examples/unsupervised_generative_pc/#hyperparameters","title":"Hyperparameters","text":"<p>We define some global parameters, including the network architecture, learning rate, batch size, etc.</p> <pre><code>SEED = 0\n\nINPUT_DIM = 50\nWIDTH = 300\nDEPTH = 3\nOUTPUT_DIM = 784\nACT_FN = \"relu\"\n\nLEARNING_RATE = 1e-3\nBATCH_SIZE = 64\nMAX_T1 = 100\nN_TRAIN_ITERS = 300\n</code></pre>"},{"location":"examples/unsupervised_generative_pc/#dataset","title":"Dataset","text":"<p>Some utils to fetch and plot MNIST.</p> <pre><code>def get_mnist_loaders(batch_size):\n    train_data = MNIST(train=True, normalise=True)\n    test_data = MNIST(train=False, normalise=True)\n    train_loader = DataLoader(\n        dataset=train_data,\n        batch_size=batch_size,\n        shuffle=True,\n        drop_last=True\n    )\n    test_loader = DataLoader(\n        dataset=test_data,\n        batch_size=batch_size,\n        shuffle=True,\n        drop_last=True\n    )\n    return train_loader, test_loader\n\n\nclass MNIST(datasets.MNIST):\n    def __init__(self, train, normalise=True, save_dir=\"data\"):\n        if normalise:\n            transform = transforms.Compose(\n                [\n                    transforms.ToTensor(),\n                    transforms.Normalize(\n                        mean=(0.1307), std=(0.3081)\n                    )\n                ]\n            )\n        else:\n            transform = transforms.Compose([transforms.ToTensor()])\n        super().__init__(save_dir, download=True, train=train, transform=transform)\n\n    def __getitem__(self, index):\n        img, _ = super().__getitem__(index)\n        img = torch.flatten(img)\n        return img\n</code></pre>"},{"location":"examples/unsupervised_generative_pc/#plotting","title":"Plotting","text":"<pre><code>def plot_train_energies(energies, ts):\n    t_max = int(ts[0])\n    norm = mcolors.Normalize(vmin=0, vmax=len(energies)-1)\n    fig, ax = plt.subplots(figsize=(8, 4))\n\n    cmap_blues = plt.get_cmap(\"Blues\")\n    cmap_reds = plt.get_cmap(\"Reds\")\n    cmap_greens = plt.get_cmap(\"Greens\")\n\n    legend_handles = []\n    legend_labels = []\n\n    for t, energies_iter in enumerate(energies):\n        line1, = ax.plot(energies_iter[0, :t_max], color=cmap_blues(norm(t)))\n        line2, = ax.plot(energies_iter[1, :t_max], color=cmap_reds(norm(t)))\n        line3, = ax.plot(energies_iter[2, :t_max], color=cmap_greens(norm(t)))\n\n        if t == 70:\n            legend_handles.append(line1)\n            legend_labels.append(\"$\\ell_1$\")\n            legend_handles.append(line2)\n            legend_labels.append(\"$\\ell_2$\")\n            legend_handles.append(line3)\n            legend_labels.append(\"$\\ell_3$\")\n\n    ax.legend(legend_handles, legend_labels, loc=\"best\", fontsize=16)\n    sm = plt.cm.ScalarMappable(cmap=plt.get_cmap(\"Greys\"), norm=norm)\n    sm._A = []\n    cbar = fig.colorbar(sm, ax=ax)\n    cbar.set_label(\"Training iteration\", fontsize=16, labelpad=14)\n    cbar.ax.tick_params(labelsize=14) \n    plt.gca().tick_params(axis=\"both\", which=\"major\", labelsize=16)\n\n    ax.set_xlabel(\"Inference iterations\", fontsize=18, labelpad=14)\n    ax.set_ylabel(\"Energy\", fontsize=18, labelpad=14)\n    ax.set_yscale(\"log\")\n    plt.show()\n</code></pre>"},{"location":"examples/unsupervised_generative_pc/#network","title":"Network","text":"<p>For <code>jpc</code> to work, we need to provide a network with callable layers. This is easy to do with the PyTorch-like <code>nn.Sequential()</code> in equinox. For example, we can define a ReLU MLP with two hidden layers as follows</p> <pre><code>key = jax.random.PRNGKey(SEED)\nkey, *subkeys = jax.random.split(key, 4)\nnetwork = [\n    nn.Sequential(\n        [\n            nn.Linear(10, 300, key=subkeys[0]),\n            nn.Lambda(jax.nn.relu)\n        ],\n    ),\n    nn.Sequential(\n        [\n            nn.Linear(300, 300, key=subkeys[1]),\n            nn.Lambda(jax.nn.relu)\n        ],\n    ),\n    nn.Linear(300, 784, key=subkeys[2]),\n]\n</code></pre> <p>You can also use <code>jpc.make_mlp()</code> to define a multi-layer perceptron (MLP) or fully connected network.</p> <pre><code>network = jpc.make_mlp(\n    key,\n    input_dim=INPUT_DIM,\n    width=WIDTH,\n    depth=DEPTH,\n    output_dim=OUTPUT_DIM,\n    act_fn=ACT_FN,\n    use_bias=True\n)\nprint(network)\n</code></pre> <pre><code>[Sequential(\n  layers=(\n    Lambda(fn=Identity()),\n    Linear(\n      weight=f32[300,50],\n      bias=f32[300],\n      in_features=50,\n      out_features=300,\n      use_bias=True\n    )\n  )\n), Sequential(\n  layers=(\n    Lambda(fn=&lt;PjitFunction of &lt;function relu at 0x12db01e10&gt;&gt;),\n    Linear(\n      weight=f32[300,300],\n      bias=f32[300],\n      in_features=300,\n      out_features=300,\n      use_bias=True\n    )\n  )\n), Sequential(\n  layers=(\n    Lambda(fn=&lt;PjitFunction of &lt;function relu at 0x12db01e10&gt;&gt;),\n    Linear(\n      weight=f32[784,300],\n      bias=f32[784],\n      in_features=300,\n      out_features=784,\n      use_bias=True\n    )\n  )\n)]\n</code></pre>"},{"location":"examples/unsupervised_generative_pc/#train","title":"Train","text":"<p>A PC network can be updated in a single line of code with <code>jpc.make_pc_step()</code>, which is already \"jitted\" for optimised performance. To train in an unsupervised way, we simply avoid providing an <code>input</code> to <code>jpc.make_pc_step()</code>. To test the learned encoding or representation for downstream accuracy, you could simply add a classifier.</p> <pre><code>def train(\n      key,\n      input_dim,\n      width,\n      depth,\n      output_dim,\n      batch_size,\n      network,\n      lr,\n      max_t1,\n      n_train_iters\n):\n    layer_sizes = [input_dim] + [width]*(depth-1) + [output_dim]\n    optim = optax.adam(lr)\n    opt_state = optim.init(\n        (eqx.filter(network, eqx.is_array), None)\n    )\n    train_loader, _ = get_mnist_loaders(batch_size)\n\n    train_energies, ts = [], []\n    for iter, img_batch in enumerate(train_loader):\n        img_batch = img_batch.numpy()\n\n        result = jpc.make_pc_step(\n            key=key,\n            layer_sizes=layer_sizes,\n            batch_size=batch_size,\n            model=network,\n            optim=optim,\n            opt_state=opt_state,\n            output=img_batch,\n            max_t1=max_t1,\n            record_activities=True,\n            record_energies=True\n        )\n        network, opt_state = result[\"model\"], result[\"opt_state\"]\n        train_energies.append(result[\"energies\"])\n        ts.append(result[\"t_max\"])\n        if (iter+1) &gt;= n_train_iters:\n            break\n\n    return result[\"model\"], train_energies, ts\n</code></pre>"},{"location":"examples/unsupervised_generative_pc/#run","title":"Run","text":"<p>Below we simply plot the energy dynamics of each layer during both inference and learning.</p> <pre><code>network, energies, ts = train(\n    key=key,\n    input_dim=INPUT_DIM,\n    width=WIDTH,\n    depth=DEPTH,\n    output_dim=OUTPUT_DIM,\n    batch_size=BATCH_SIZE,\n    network=network,\n    lr=LEARNING_RATE,\n    max_t1=MAX_T1,\n    n_train_iters=N_TRAIN_ITERS\n)\nplot_train_energies(energies, ts)\n</code></pre> <p></p>"}]}