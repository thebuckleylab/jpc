{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"/]+|(?!\\b)(?=[A-Z][a-z])|\\.(?!\\d)|&[lg]t;","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Getting started","text":"<p>JPC is a JAX library for training neural  networks with Predictive Coding (PC). </p> <p>JPC provides a simple, fast and flexible API for  training of a variety of PCNs including discriminative, generative and hybrid  models. </p> <ul> <li> <p>Like JAX, JPC is completely functional in design, and the core library code is  &lt;1000 lines of code. </p> </li> <li> <p>Unlike existing implementations, JPC provides a wide range of optimisers, both  discrete and continuous, to solve the inference dynamics of PC, including  ordinary differential equation (ODE) solvers.</p> </li> <li> <p>JPC also provides some analytical tools that can be used to study and potentially diagnose issues with PCNs.</p> </li> </ul> <p>If you're new to JPC, we recommend starting from the  example notebooks and checking the documentation.</p>"},{"location":"#installation","title":"\ud83d\udcbb Installation","text":"<p>Clone the repo and in the project's directory run <pre><code>pip install .\n</code></pre></p> <p>Requires Python 3.10+ and JAX 0.4.38\u20130.5.2 (inclusive). For GPU usage, upgrade  jax to the appropriate cuda version (12 as an example here).</p> <pre><code>pip install --upgrade \"jax[cuda12]\"\n</code></pre>"},{"location":"#quick-example","title":"\u26a1\ufe0f Quick example","text":"<p>Use <code>jpc.make_pc_step()</code> to update the parameters of any neural network  compatible with PC updates (see examples) <pre><code>import jax.random as jr\nimport jax.numpy as jnp\nimport equinox as eqx\nimport optax\nimport jpc\n\n# toy data\nx = jnp.array([1., 1., 1.])\ny = -x\n\n# define model and optimiser\nkey = jr.PRNGKey(0)\nmodel = jpc.make_mlp(\n    key, \n    input_dim=3,\n    width=50,\n    depth=5,\n    output_dim=3\n    act_fn=\"relu\"\n)\noptim = optax.adam(1e-3)\nopt_state = optim.init(\n    (eqx.filter(model, eqx.is_array), None)\n)\n\n# perform one training step with PC\nresult = jpc.make_pc_step(\n    model=model,\n    optim=optim,\n    opt_state=opt_state,\n    output=y,\n    input=x\n)\n\n# updated model and optimiser\nmodel, opt_state = result[\"model\"], result[\"opt_state\"]\n</code></pre> Under the hood, <code>jpc.make_pc_step()</code></p> <ol> <li>integrates the inference (activity) dynamics using a diffrax ODE solver, and</li> <li>updates model parameters at the numerical solution of the activities with a given optax optimiser.</li> </ol> <p>NOTE: All convenience training and test functions such as <code>make_pc_step()</code>  are already \"jitted\" (for optimised performance) for the user's convenience.</p>"},{"location":"#advanced-usage","title":"\ud83d\ude80 Advanced usage","text":"<p>Advanced users can access all the underlying functions of <code>jpc.make_pc_step()</code>  as well as additional features. A custom PC training step looks like the  following: <pre><code>import jpc\n\n# 1. initialise activities with a feedforward pass\nactivities = jpc.init_activities_with_ffwd(model=model, input=x)\n\n# 2. perform inference (state optimisation)\nactivity_opt_state = activity_optim.init(activities)\nfor _ in range(len(model)):\n    activity_update_result = jpc.update_pc_activities(\n        params=(model, None),\n        activities=activities,\n        optim=activity_optim,\n        opt_state=activity_opt_state,\n        output=y,\n        input=x\n    )\n    activities = activity_update_result[\"activities\"]\n    activity_opt_state = activity_update_result[\"opt_state\"]\n\n# 3. update parameters at the activities' solution with PC\nresult = jpc.update_params(\n    params=(model, None), \n    activities=equilibrated_activities,\n    optim=optim,\n    opt_state=opt_state,\n    output=y, \n    input=x\n)\n</code></pre> which can be embedded in a jitted function with any other additional  computations.</p>"},{"location":"#citation","title":"\ud83d\udcc4 Citation","text":"<p>If you found this library useful in your work, please cite (paper link):</p> <p><pre><code>@article{innocenti2024jpc,\n  title={JPC: Flexible Inference for Predictive Coding Networks in JAX},\n  author={Innocenti, Francesco and Kinghorn, Paul and Yun-Farmbrough, Will and Varona, Miguel De Llanza and Singh, Ryan and Buckley, Christopher L},\n  journal={arXiv preprint arXiv:2412.03676},\n  year={2024}\n}\n</code></pre> Also consider starring the project on GitHub! \u2b50\ufe0f </p>"},{"location":"#acknowledgements","title":"\ud83d\ude4f Acknowledgements","text":"<p>We are grateful to Patrick Kidger for early advice on how to use Diffrax.</p>"},{"location":"#see-also-other-pc-libraries","title":"See also: other PC libraries","text":"<ul> <li>ngc-learn (jax &amp; pytorch)</li> <li>pcx (jax)</li> <li>pyhgf (jax)</li> <li>Torch2PC (pytorch)</li> <li>pypc (pytorch)</li> <li>pybrid (pytorch)</li> </ul>"},{"location":"advanced_usage/","title":"Advanced usage","text":"<p>Advanced users can access all the underlying functions of <code>jpc.make_pc_step()</code>  as well as additional features. A custom PC training step looks like the  following: <pre><code>import jpc\n\n# 1. initialise activities with a feedforward pass\nactivities = jpc.init_activities_with_ffwd(model=model, input=x)\n\n# 2. run inference to equilibrium\nequilibrated_activities = jpc.solve_inference(\n    params=(model, None), \n    activities=activities, \n    output=y, \n    input=x\n)\n\n# 3. update parameters at the activities' solution with PC\nparam_update_result = jpc.update_params(\n    params=(model, None), \n    activities=equilibrated_activities,\n    optim=param_optim,\n    opt_state=param_opt_state,\n    output=y, \n    input=x\n)\n\n# updated model and optimiser\nmodel = param_update_result[\"model\"]\nparam_opt_state = param_update_result[\"opt_state\"]\n</code></pre> which can be embedded in a jitted function with any other additional  computations. One can also use any optax  optimiser to  equilibrate the inference dynamics by replacing the function in step 2, as  shown below. <pre><code>activity_optim = optax.adam(1e-3)\n\n# 1. initialise activities\n...\n\n# 2. infer with adam\nactivity_opt_state = activity_optim.init(activities)\n\nfor t in range(T):\n    activity_update_result = jpc.update_activities(\n        params=(model, None),\n        activities=activities,\n        optim=activity_optim,\n        opt_state=activity_opt_state,\n        output=y,\n        input=x\n    )\n    # updated activities and optimiser\n    activities = activity_update_result[\"activities\"]\n    activity_opt_state = activity_update_result[\"opt_state\"]\n\n# 3. update parameters at the activities' solution with PC\n...\n</code></pre> See the updates docs  for more details. JPC also  comes with some analytical tools that can be used to study and potentially  diagnose issues with PCNs  (see docs   and example notebook ).</p>"},{"location":"basic_usage/","title":"Basic usage","text":"<p>JPC provides two types of API depending on the use case:</p> <ul> <li>a simple, high-level API that allows to train and test models with predictive  coding in a few lines of code, and</li> <li>a more advanced API offering greater flexibility as well as additional features.</li> </ul>"},{"location":"basic_usage/#basic-usage","title":"Basic usage","text":"<p>At a high level, JPC provides a single convenience function <code>jpc.make_pc_step()</code>  to update the parameters of a neural network with PC. <pre><code>import jax.random as jr\nimport jax.numpy as jnp\nimport equinox as eqx\nimport optax\nimport jpc\n\n# toy data\nx = jnp.array([1., 1., 1.])\ny = -x\n\n# define model and optimiser\nkey = jr.PRNGKey(0)\nmodel = jpc.make_mlp(\n    key, \n    input_dim=3,\n    width=50,\n    depth=5,\n    output_dim=3\n    act_fn=\"relu\"\n)\noptim = optax.adam(1e-3)\nopt_state = optim.init(\n    (eqx.filter(model, eqx.is_array), None)\n)\n\n# perform one training step with PC\nupdate_result = jpc.make_pc_step(\n    model=model,\n    optim=optim,\n    opt_state=opt_state,\n    output=y,\n    input=x\n)\n\n# updated model and optimiser\nmodel, opt_state = update_result[\"model\"], update_result[\"opt_state\"]\n</code></pre> As shown above, at a minimum <code>jpc.make_pc_step()</code> takes a model, an optax  optimiser and its  state, and some data. The model needs to be compatible with PC updates in the  sense that it's split into callable layers (see the  example notebooks ). Also note  that the <code>input</code> is actually not needed for unsupervised training. In fact,  <code>jpc.make_pc_step()</code> can be used for classification and generation tasks, for  supervised as well as unsupervised training (again see the example notebooks ). </p> <p>Under the hood, <code>jpc.make_pc_step()</code> uses diffrax  to solve the activity (inference)  dynamics of PC. Many default arguments, for example related to the ODE solver, can be changed, including the ODE solver, and there is an option to record a  variety of metrics such as loss, accuracy, and energies. See the docs  for more  details.</p> <p>A similar convenience function <code>jpc.make_hpc_step()</code> is provided for updating the parameters of a hybrid PCN (Tschantz et al., 2023 ). <pre><code>import jax.random as jr\nimport equinox as eqx\nimport optax\nimport jpc\n\n# models\nkey = jr.PRNGKey(0)\nsubkeys = jr.split(key, 2)\n\ninput_dim, output_dim = 10, 3\nwidth, depth = 100, 5\ngenerator = jpc.make_mlp(\n    subkeys[0], \n    input_dim=input_dim,\n    width=width,\n    depth=depth,\n    output_dim=output_dim\n    act_fn=\"tanh\"\n)\n# NOTE that the input and output of the amortiser are reversed\namortiser = jpc.make_mlp(\n    subkeys[0], \n    input_dim=output_dim,\n    width=width,\n    depth=depth,\n    output_dim=input_dim\n    act_fn=\"tanh\"\n)\n\n# optimisers\ngen_optim = optax.adam(1e-3)\namort_optim = optax.adam(1e-3)\ngen_pt_state = gen_optim.init(\n    (eqx.filter(generator, eqx.is_array), None)\n)\namort_opt_state = amort_optim.init(\n    eqx.filter(amortiser, eqx.is_array)\n)\n\nupdate_result = jpc.make_hpc_step(\n    generator=generator,\n    amortiser=amortiser,\n    optims=[gen_optim, amort_optim],\n    opt_states=[gen_opt_state, amort_opt_state],\n    output=y,\n    input=x\n)\ngenerator, amortiser = update_result[\"generator\"], update_result[\"amortiser\"]\nopt_states = update_result[\"opt_states\"]\ngen_loss, amort_loss = update_result[\"losses\"]\n</code></pre> See the docs  and the example notebook  for more details.</p>"},{"location":"api/Continuous-time%20Inference/","title":"Continuous-time inference","text":"<p>The inference or activity dynamics of PC networks can be solved in either  discrete or continuous time. jpc.solve_inference()  leverages ODE solvers to integrate the continuous-time dynamics.</p>"},{"location":"api/Continuous-time%20Inference/#jpc.solve_inference","title":"<code>jpc.solve_inference(params: typing.Tuple[jaxtyping.PyTree[typing.Callable], typing.Optional[jaxtyping.PyTree[typing.Callable]]], activities: PyTree[jax.Array | numpy.ndarray | numpy.bool | numpy.number | bool | int | float | complex], output: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex], *, input: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex, NoneType] = None, loss_id: str = 'mse', param_type: str = 'sp', solver: AbstractSolver = Heun(), max_t1: int = 20, dt: float | int = None, stepsize_controller: AbstractStepSizeController = PIDController(rtol=0.001, atol=0.001), weight_decay: Shaped[Array, ''] = 0.0, spectral_penalty: Shaped[Array, ''] = 0.0, activity_decay: Shaped[Array, ''] = 0.0, gamma: typing.Optional[jaxtyping.Shaped[Array, '']] = None, record_iters: bool = False, record_every: int = None) -&gt; PyTree[jax.Array]</code>","text":"<p>Solves the inference (activity) dynamics of a predictive coding network.</p> <p>This is a wrapper around <code>diffrax.diffeqsolve()</code>  to integrate the gradient ODE system <code>jpc.neg_activity_grad()</code>  defining the PC inference dynamics.</p> \\[ d\\mathbf{z} / dt = - \u2207_{\\mathbf{z}} \\mathcal{F} \\] <p>where \\(\\mathcal{F}\\) is the free energy, \\(\\mathbf{z}\\) are the activities, with \\(\\mathbf{z}_L\\) clamped to some target and \\(\\mathbf{z}_0\\) optionally set to some prior.</p> <p>Main arguments:</p> <ul> <li><code>params</code>: Tuple with callable model layers and optional skip connections.</li> <li><code>activities</code>: List of activities for each layer free to vary.</li> <li><code>output</code>: Observation or target of the generative model.</li> </ul> <p>Other arguments:</p> <ul> <li><code>input</code>: Optional prior of the generative model.</li> <li><code>loss_id</code>: Loss function to use at the output layer. Options are mean squared      error <code>\"mse\"</code> (default) or cross-entropy <code>\"ce\"</code>.</li> <li><code>param_type</code>: Determines the parameterisation. Options are <code>\"sp\"</code>      (standard parameterisation), <code>\"mupc\"</code> (\u03bcPC),      or <code>\"ntp\"</code> (neural tangent parameterisation).      See <code>_get_param_scalings()</code>      for the specific scalings of these different parameterisations. Defaults     to <code>\"sp\"</code>.</li> <li><code>solver</code>: diffrax ODE solver      to be used. Default is <code>Heun</code>,      a 2nd order explicit Runge--Kutta method.</li> <li><code>max_t1</code>: Maximum end of integration region (20 by default).</li> <li><code>dt</code>: Integration step size. Defaults to <code>None</code> since the default     <code>stepsize_controller</code> will automatically determine it.</li> <li><code>stepsize_controller</code>: diffrax controller      for step size integration. Defaults to <code>PIDController</code>.      Note that the relative and absolute tolerances of the controller will      also determine the steady state to terminate the solver.</li> <li><code>weight_decay</code>: \\(\\ell^2\\) regulariser for the weights (0 by default).</li> <li><code>spectral_penalty</code>: Weight spectral penalty of the form      \\(||\\mathbf{I} - \\mathbf{W}_\\ell^T \\mathbf{W}_\\ell||^2\\) (0 by default).</li> <li><code>activity_decay</code>: \\(\\ell^2\\) regulariser for the activities (0 by default).</li> <li><code>gamma</code>: Optional scaling factor for the output layer. If provided, the output      layer scaling is multiplied by <code>1/gamma</code>. Defaults to <code>None</code> (no additional scaling).</li> <li><code>record_iters</code>: If <code>True</code>, returns all integration steps.</li> <li><code>record_every</code>: int determining the sampling frequency of the integration     steps.</li> </ul> <p>Returns:</p> <p>List with solution of the activity dynamics for each layer.</p>"},{"location":"api/Discrete%20updates/","title":"Discrete updates","text":"<p>JPC provides access to standard discrete optimisers to update the parameters of  PC networks (jpc.update_pc_params), and to both discrete (jpc.update_pc_activities)  and continuous optimisers (jpc.solve_inference)  to solve the PC inference or activity dynamics.</p>"},{"location":"api/Discrete%20updates/#jpc.update_pc_activities","title":"<code>jpc.update_pc_activities(params: typing.Tuple[jaxtyping.PyTree[typing.Callable], typing.Optional[jaxtyping.PyTree[typing.Callable]]], activities: PyTree[jax.Array | numpy.ndarray | numpy.bool | numpy.number | bool | int | float | complex], optim: optax._src.base.GradientTransformation | optax._src.base.GradientTransformationExtraArgs, opt_state: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, typing.Iterable[ForwardRef(ArrayTree)], typing.Mapping[typing.Any, ForwardRef(ArrayTree)]], output: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex], *, input: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex, NoneType] = None, loss_id: str = 'mse', param_type: str = 'sp', weight_decay: Shaped[Array, ''] = 0.0, spectral_penalty: Shaped[Array, ''] = 0.0, activity_decay: Shaped[Array, ''] = 0.0, gamma: typing.Optional[jaxtyping.Shaped[Array, '']] = None) -&gt; typing.Dict</code>","text":"<p>Updates activities of a predictive coding network with a given  optax optimiser.</p> <p>Warning</p> <p><code>param_type = \"mupc\"</code> (\u03bcPC) assumes  that one is using <code>jpc.make_mlp()</code>  to create the model.</p> <p>Main arguments:</p> <ul> <li><code>params</code>: Tuple with callable model layers and optional skip connections.</li> <li><code>activities</code>: List of activities for each layer free to vary.</li> <li><code>optim</code>: optax optimiser, e.g. <code>optax.sgd()</code>.</li> <li><code>opt_state</code>: State of optax optimiser.</li> <li><code>output</code>: Observation or target of the generative model.</li> </ul> <p>Other arguments:</p> <ul> <li><code>input</code>: Optional prior of the generative model.</li> <li><code>loss_id</code>: Loss function to use at the output layer. Options are mean squared      error <code>\"mse\"</code> (default) or cross-entropy <code>\"ce\"</code>.</li> <li><code>param_type</code>: Determines the parameterisation. Options are <code>\"sp\"</code>      (standard parameterisation), <code>\"mupc\"</code> (\u03bcPC),      or <code>\"ntp\"</code> (neural tangent parameterisation).      See <code>_get_param_scalings()</code>      for the specific scalings of these different parameterisations. Defaults     to <code>\"sp\"</code>.</li> <li><code>weight_decay</code>: Weight decay for the weights (0 by default).</li> <li><code>spectral_penalty</code>: Weight spectral penalty of the form      \\(||\\mathbf{I} - \\mathbf{W}_\\ell^T \\mathbf{W}_\\ell||^2\\) (0 by default).</li> <li><code>activity_decay</code>: Activity decay for the activities (0 by default).</li> <li><code>gamma</code>: Optional scaling factor for the output layer. If provided, the output      layer scaling is multiplied by <code>1/gamma</code>. Defaults to <code>None</code> (no additional scaling).</li> </ul> <p>Returns:</p> <p>Dictionary with energy, updated activities, activity gradients, and  optimiser state.</p>"},{"location":"api/Discrete%20updates/#jpc.update_pc_params","title":"<code>jpc.update_pc_params(params: typing.Tuple[jaxtyping.PyTree[typing.Callable], typing.Optional[jaxtyping.PyTree[typing.Callable]]], activities: PyTree[jax.Array | numpy.ndarray | numpy.bool | numpy.number | bool | int | float | complex], optim: optax._src.base.GradientTransformation | optax._src.base.GradientTransformationExtraArgs, opt_state: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, typing.Iterable[ForwardRef(ArrayTree)], typing.Mapping[typing.Any, ForwardRef(ArrayTree)]], output: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex], *, input: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex, NoneType] = None, loss_id: str = 'mse', param_type: str = 'sp', weight_decay: Shaped[Array, ''] = 0.0, spectral_penalty: Shaped[Array, ''] = 0.0, activity_decay: Shaped[Array, ''] = 0.0, gamma: typing.Optional[jaxtyping.Shaped[Array, '']] = None) -&gt; typing.Dict</code>","text":"<p>Updates parameters of a predictive coding network with a given  optax optimiser.</p> <p>Warning</p> <p><code>param_type = \"mupc\"</code> (\u03bcPC) assumes  that one is using <code>jpc.make_mlp()</code>  to create the model.</p> <p>Main arguments:</p> <ul> <li><code>params</code>: Tuple with callable model layers and optional skip connections.</li> <li><code>activities</code>: List of activities for each layer free to vary.</li> <li><code>optim</code>: optax optimiser, e.g. <code>optax.sgd()</code>.</li> <li><code>opt_state</code>: State of optax optimiser.</li> <li><code>output</code>: Observation or target of the generative model.</li> </ul> <p>Other arguments:</p> <ul> <li><code>input</code>: Optional prior of the generative model.</li> <li><code>loss_id</code>: Loss function to use at the output layer. Options are mean squared      error <code>\"mse\"</code> (default) or cross-entropy <code>\"ce\"</code>.</li> <li><code>param_type</code>: Determines the parameterisation. Options are <code>\"sp\"</code>      (standard parameterisation), <code>\"mupc\"</code> (\u03bcPC),      or <code>\"ntp\"</code> (neural tangent parameterisation).      See <code>_get_param_scalings()</code>      for the specific scalings of these different parameterisations. Defaults     to <code>\"sp\"</code>.</li> <li><code>weight_decay</code>: Weight decay for the weights (0 by default).</li> <li><code>spectral_penalty</code>: Weight spectral penalty of the form      \\(||\\mathbf{I} - \\mathbf{W}_\\ell^T \\mathbf{W}_\\ell||^2\\) (0 by default).</li> <li><code>activity_decay</code>: Activity decay for the activities (0 by default).</li> <li><code>gamma</code>: Optional scaling factor for the output layer. If provided, the output      layer scaling is multiplied by <code>1/gamma</code>. Defaults to <code>None</code> (no additional scaling).</li> </ul> <p>Returns:</p> <p>Dictionary with model and optional skip model with updated parameters, parameter gradients, and optimiser state.</p>"},{"location":"api/Discrete%20updates/#jpc.update_bpc_activities","title":"<code>jpc.update_bpc_activities(top_down_model: PyTree[typing.Callable], bottom_up_model: PyTree[typing.Callable], activities: PyTree[jax.Array | numpy.ndarray | numpy.bool | numpy.number | bool | int | float | complex], optim: optax._src.base.GradientTransformation | optax._src.base.GradientTransformationExtraArgs, opt_state: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, typing.Iterable[ForwardRef(ArrayTree)], typing.Mapping[typing.Any, ForwardRef(ArrayTree)]], output: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex], input: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex], *, skip_model: typing.Optional[jaxtyping.PyTree[typing.Callable]] = None, param_type: str = 'sp', backward_energy_weight: Shaped[Array, ''] = 1.0, forward_energy_weight: Shaped[Array, ''] = 1.0) -&gt; typing.Dict</code>","text":"<p>Updates activities of a bidirectional PC network.</p> <p>Main arguments:</p> <ul> <li><code>top_down_model</code>: List of callable model (e.g. neural network) layers for      the forward model.</li> <li><code>bottom_up_model</code>: List of callable model (e.g. neural network) layers for      the backward model.</li> <li><code>activities</code>: List of activities for each layer free to vary.</li> <li><code>optim</code>: optax optimiser, e.g. <code>optax.sgd()</code>.</li> <li><code>opt_state</code>: State of optax optimiser.</li> <li><code>output</code>: Target of the <code>top_down_model</code> and input to the <code>bottom_up_model</code>.</li> </ul> <p>Other arguments:</p> <ul> <li><code>input</code>: Input to the <code>top_down_model</code> and target of the <code>bottom_up_model</code>.</li> <li><code>skip_model</code>: Optional skip connection model.</li> <li><code>param_type</code>: Determines the parameterisation. Options are <code>\"sp\"</code>      (standard parameterisation), <code>\"mupc\"</code> (\u03bcPC),      or <code>\"ntp\"</code> (neural tangent parameterisation).      See <code>_get_param_scalings()</code>      for the specific scalings of these different parameterisations. Defaults     to <code>\"sp\"</code>.</li> <li><code>backward_energy_weight</code>: Scalar weighting for the backward energy terms.      Defaults to <code>1.0</code>.</li> <li><code>forward_energy_weight</code>: Scalar weighting for the forward energy terms.      Defaults to <code>1.0</code>.</li> </ul> <p>Returns:</p> <p>Dictionary with energy, updated activities, activity gradients, and  optimiser state.</p>"},{"location":"api/Discrete%20updates/#jpc.update_bpc_params","title":"<code>jpc.update_bpc_params(top_down_model: PyTree[typing.Callable], bottom_up_model: PyTree[typing.Callable], activities: PyTree[jax.Array | numpy.ndarray | numpy.bool | numpy.number | bool | int | float | complex], top_down_optim: optax._src.base.GradientTransformation | optax._src.base.GradientTransformationExtraArgs, bottom_up_optim: optax._src.base.GradientTransformation | optax._src.base.GradientTransformationExtraArgs, top_down_opt_state: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, typing.Iterable[ForwardRef(ArrayTree)], typing.Mapping[typing.Any, ForwardRef(ArrayTree)]], bottom_up_opt_state: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, typing.Iterable[ForwardRef(ArrayTree)], typing.Mapping[typing.Any, ForwardRef(ArrayTree)]], output: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex], input: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex], *, skip_model: typing.Optional[jaxtyping.PyTree[typing.Callable]] = None, param_type: str = 'sp', backward_energy_weight: Shaped[Array, ''] = 1.0, forward_energy_weight: Shaped[Array, ''] = 1.0) -&gt; typing.Dict</code>","text":"<p>Updates parameters of a bidirectional PC network.</p> <p>Main arguments:</p> <ul> <li><code>top_down_model</code>: List of callable model (e.g. neural network) layers for      the forward model.</li> <li><code>bottom_up_model</code>: List of callable model (e.g. neural network) layers for      the backward model.</li> <li><code>activities</code>: List of activities for each layer free to vary.</li> <li><code>top_down_optim</code>: optax optimiser for the top-down model.</li> <li><code>bottom_up_optim</code>: optax optimiser for the bottom-up model.</li> <li><code>top_down_opt_state</code>: State of the top-down optimiser.</li> <li><code>bottom_up_opt_state</code>: State of the bottom-up optimiser.</li> <li><code>output</code>: Target of the <code>top_down_model</code> and input to the <code>bottom_up_model</code>.</li> </ul> <p>Other arguments:</p> <ul> <li><code>input</code>: Input to the <code>top_down_model</code> and target of the <code>bottom_up_model</code>.</li> <li><code>skip_model</code>: Optional skip connection model.</li> <li><code>param_type</code>: Determines the parameterisation. Options are <code>\"sp\"</code>      (standard parameterisation), <code>\"mupc\"</code> (\u03bcPC),      or <code>\"ntp\"</code> (neural tangent parameterisation).      See <code>_get_param_scalings()</code>      for the specific scalings of these different parameterisations. Defaults     to <code>\"sp\"</code>.</li> <li><code>backward_energy_weight</code>: Scalar weighting for the backward energy terms.      Defaults to <code>1.0</code>.</li> <li><code>forward_energy_weight</code>: Scalar weighting for the forward energy terms.      Defaults to <code>1.0</code>.</li> </ul> <p>Returns:</p> <p>Dictionary with models with updated parameters, parameter gradients, and  optimiser states.</p>"},{"location":"api/Discrete%20updates/#jpc.update_epc_errors","title":"<code>jpc.update_epc_errors(params: typing.Tuple[jaxtyping.PyTree[typing.Callable], typing.Optional[jaxtyping.PyTree[typing.Callable]]], errors: PyTree[jax.Array | numpy.ndarray | numpy.bool | numpy.number | bool | int | float | complex], optim: optax._src.base.GradientTransformation | optax._src.base.GradientTransformationExtraArgs, opt_state: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, typing.Iterable[ForwardRef(ArrayTree)], typing.Mapping[typing.Any, ForwardRef(ArrayTree)]], output: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex], *, input: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex, NoneType] = None, loss_id: str = 'mse', param_type: str = 'sp') -&gt; typing.Dict</code>","text":"<p>Updates errors of an error-reparameterised Predictive Coding (ePC) network with a given  optax optimiser.</p> <p>Note</p> <p>In ePC, errors are updated during inference rather than activities.</p> <p>Warning</p> <p><code>param_type = \"mupc\"</code> (\u03bcPC) assumes  that one is using <code>jpc.make_mlp()</code>  to create the model.</p> <p>Main arguments:</p> <ul> <li><code>params</code>: Tuple with callable model layers and optional skip connections.</li> <li><code>errors</code>: List of errors for each layer free to vary.</li> <li><code>optim</code>: optax optimiser, e.g. <code>optax.sgd()</code>.</li> <li><code>opt_state</code>: State of optax optimiser.</li> <li><code>output</code>: Observation or target of the generative model.</li> </ul> <p>Other arguments:</p> <ul> <li><code>input</code>: Optional prior of the generative model.</li> <li><code>loss_id</code>: Loss function to use at the output layer. Options are mean squared      error <code>\"mse\"</code> (default) or cross-entropy <code>\"ce\"</code>.</li> <li><code>param_type</code>: Determines the parameterisation. Options are <code>\"sp\"</code>      (standard parameterisation), <code>\"mupc\"</code> (\u03bcPC),      or <code>\"ntp\"</code> (neural tangent parameterisation).      See <code>_get_param_scalings()</code>      for the specific scalings of these different parameterisations. Defaults     to <code>\"sp\"</code>.</li> </ul> <p>Returns:</p> <p>Dictionary with energy, updated errors, error gradients, and  optimiser state.</p>"},{"location":"api/Discrete%20updates/#jpc.update_epc_params","title":"<code>jpc.update_epc_params(params: typing.Tuple[jaxtyping.PyTree[typing.Callable], typing.Optional[jaxtyping.PyTree[typing.Callable]]], errors: PyTree[jax.Array | numpy.ndarray | numpy.bool | numpy.number | bool | int | float | complex], optim: optax._src.base.GradientTransformation | optax._src.base.GradientTransformationExtraArgs, opt_state: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, typing.Iterable[ForwardRef(ArrayTree)], typing.Mapping[typing.Any, ForwardRef(ArrayTree)]], output: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex], *, input: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex, NoneType] = None, loss_id: str = 'mse', param_type: str = 'sp') -&gt; typing.Dict</code>","text":"<p>Updates parameters of an error-reparameterised Predictive Coding (ePC) network with a given  optax optimiser.</p> <p>Note</p> <p>In ePC, errors are updated during inference rather than activities.</p> <p>Warning</p> <p><code>param_type = \"mupc\"</code> (\u03bcPC) assumes  that one is using <code>jpc.make_mlp()</code>  to create the model.</p> <p>Main arguments:</p> <ul> <li><code>params</code>: Tuple with callable model layers and optional skip connections.</li> <li><code>errors</code>: List of errors for each layer free to vary.</li> <li><code>optim</code>: optax optimiser, e.g. <code>optax.sgd()</code>.</li> <li><code>opt_state</code>: State of optax optimiser.</li> <li><code>output</code>: Observation or target of the generative model.</li> </ul> <p>Other arguments:</p> <ul> <li><code>input</code>: Optional prior of the generative model.</li> <li><code>loss_id</code>: Loss function to use at the output layer. Options are mean squared      error <code>\"mse\"</code> (default) or cross-entropy <code>\"ce\"</code>.</li> <li><code>param_type</code>: Determines the parameterisation. Options are <code>\"sp\"</code>      (standard parameterisation), <code>\"mupc\"</code> (\u03bcPC),      or <code>\"ntp\"</code> (neural tangent parameterisation).      See <code>_get_param_scalings()</code>      for the specific scalings of these different parameterisations. Defaults     to <code>\"sp\"</code>.</li> </ul> <p>Returns:</p> <p>Dictionary with model and optional skip model with updated parameters, parameter gradients, and optimiser state.</p>"},{"location":"api/Energy%20functions/","title":"Energy functions","text":"<p>JPC provides three main PC energy functions:</p> <ul> <li>jpc.pc_energy_fn()  for standard PC networks,</li> <li>jpc.hpc_energy_fn()  for hybrid PC models (Tscshantz et al., 2023),</li> <li>jpc.bpc_energy_fn()  for bidirectional PC models (Oliviers et al., 2025), and</li> <li>jpc.epc_energy_fn()  for error-reparameterised PC (Goemaere et al., 2025).</li> </ul>"},{"location":"api/Energy%20functions/#jpc.pc_energy_fn","title":"<code>jpc.pc_energy_fn(params: typing.Tuple[jaxtyping.PyTree[typing.Callable], typing.Optional[jaxtyping.PyTree[typing.Callable]]], activities: PyTree[jax.Array | numpy.ndarray | numpy.bool | numpy.number | bool | int | float | complex], y: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex], *, x: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex, NoneType] = None, loss: str = 'mse', param_type: str = 'sp', weight_decay: Shaped[Array, ''] = 0.0, spectral_penalty: Shaped[Array, ''] = 0.0, activity_decay: Shaped[Array, ''] = 0.0, record_layers: bool = False, gamma: typing.Optional[jaxtyping.Shaped[Array, '']] = None) -&gt; jaxtyping.Shaped[Array, ''] | jax.Array</code>","text":"<p>Computes the PC energy for a neural network of the form</p> \\[ \\mathcal{F}(\\mathbf{z}; \u03b8) = 1/2N \\sum_i^N \\sum_{\\ell=1}^L || \\mathbf{z}_{i, \\ell} - f_\\ell(\\mathbf{z}_{i, \\ell-1}; \u03b8_\\ell) ||^2 \\] <p>given parameters \\(\u03b8\\), activities \\(\\mathbf{z}\\), output  \\(\\mathbf{z}_L = \\mathbf{y}\\), and optional input \\(\\mathbf{z}_0 = \\mathbf{x}\\) for supervised training. The activity of each layer \\(\\mathbf{z}_\\ell\\) is some function of the previous layer, e.g. ReLU\\((\\mathbf{W}_\\ell \\mathbf{z}_{\\ell-1} + \\mathbf{b}_\\ell)\\) for a fully  connected layer with biases and ReLU as activation.</p> <p>Note</p> <p>The input \\(x\\) and output \\(y\\) correspond to the prior and observation of the generative model, respectively.</p> <p>Main arguments:</p> <ul> <li><code>params</code>: Tuple with callable model (e.g. neural network) layers and     optional skip connections.</li> <li><code>activities</code>: List of activities for each layer free to vary.</li> <li><code>y</code>: Observation or target of the generative model.</li> </ul> <p>Other arguments:</p> <ul> <li><code>x</code>: Optional prior of the generative model (for supervised training).</li> <li><code>loss</code>: Loss function to use at the output layer. Options are mean squared      error <code>\"mse\"</code> (default) or cross-entropy <code>\"ce\"</code>.</li> <li><code>param_type</code>: Determines the parameterisation. Options are <code>\"sp\"</code>      (standard parameterisation), <code>\"mupc\"</code> (\u03bcPC),      or <code>\"ntp\"</code> (neural tangent parameterisation).      See <code>_get_param_scalings()</code>      for the specific scalings of these different parameterisations. Defaults     to <code>\"sp\"</code>.</li> <li><code>weight_decay</code>: \\(\\ell^2\\) regulariser for the weights (0 by default).</li> <li><code>spectral_penalty</code>: Weight spectral penalty of the form      \\(||\\mathbf{I} - \\mathbf{W}_\\ell^T \\mathbf{W}_\\ell||^2\\) (0 by default).</li> <li><code>activity_decay</code>: \\(\\ell^2\\) regulariser for the activities (0 by default).</li> <li><code>record_layers</code>: If <code>True</code>, returns the energy of each layer.</li> <li><code>gamma</code>: Optional scaling factor for the output layer. If provided, the output      layer scaling is multiplied by <code>1/gamma</code>.  Defaults to <code>None</code> (no additional scaling).</li> </ul> <p>Returns:</p> <p>The total or layer-wise energy normalised by the batch size.</p>"},{"location":"api/Energy%20functions/#jpc.hpc_energy_fn","title":"<code>jpc.hpc_energy_fn(model: PyTree[typing.Callable], equilib_activities: PyTree[jax.Array | numpy.ndarray | numpy.bool | numpy.number | bool | int | float | complex], amort_activities: PyTree[jax.Array | numpy.ndarray | numpy.bool | numpy.number | bool | int | float | complex], x: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex], y: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex, NoneType] = None, record_layers: bool = False) -&gt; jaxtyping.Shaped[Array, ''] | jax.Array</code>","text":"<p>Computes the energy of an amortised PC network (Tscshantz et al., 2023)</p> \\[ \\mathcal{F}(\\mathbf{z}^*, \\hat{\\mathbf{z}}; \u03b8) = 1/N \\sum_i^N \\sum_{\\ell=1}^L || \\mathbf{z}^*_{i, \\ell} - f_\\ell(\\hat{\\mathbf{z}}_{i, \\ell-1}; \u03b8_\\ell) ||^2 \\] <p>given the equilibrated activities of the generator \\(\\mathbf{z}^*\\) (target for the amortiser), the feedforward guesses of the amortiser \\(\\hat{\\mathbf{z}}\\), the amortiser's parameters \\(\u03b8\\), input \\(\\mathbf{z}_0 = \\mathbf{x}\\), and optional output \\(\\mathbf{z}_L = \\mathbf{y}\\) for supervised training.</p> <p>Note</p> <p>The input \\(x\\) and output \\(y\\) are reversed compared to <code>pc_energy_fn()</code> (\\(x\\) is the generator's target and \\(y\\) is its optional input or prior). Just think of \\(x\\) and \\(y\\) as the actual input and output of the amortiser, respectively.</p> Reference <pre><code>@article{tscshantz2023hybrid,\n    title={Hybrid predictive coding: Inferring, fast and slow},\n    author={Tscshantz, Alexander and Millidge, Beren and Seth, Anil K and Buckley, Christopher L},\n    journal={PLoS computational biology},\n    volume={19},\n    number={8},\n    pages={e1011280},\n    year={2023},\n    publisher={Public Library of Science San Francisco, CA USA}\n}\n</code></pre> <p>Main arguments:</p> <ul> <li><code>model</code>: List of callable model (e.g. neural network) layers.</li> <li><code>equilib_activities</code>: List of equilibrated activities reached by the     generator and target for the amortiser.</li> <li><code>amort_activities</code>: List of amortiser's feedforward guesses     (initialisation) for the network activities.</li> <li><code>x</code>: Input to the amortiser.</li> <li><code>y</code>: Optional target of the amortiser (for supervised training).</li> </ul> <p>Other arguments:</p> <ul> <li><code>record_layers</code>: If <code>True</code>, returns energies for each layer.</li> </ul> <p>Returns:</p> <p>The total or layer-wise energy normalised by batch size.</p>"},{"location":"api/Energy%20functions/#jpc.bpc_energy_fn","title":"<code>jpc.bpc_energy_fn(top_down_model: PyTree[typing.Callable], bottom_up_model: PyTree[typing.Callable], activities: PyTree[jax.Array | numpy.ndarray | numpy.bool | numpy.number | bool | int | float | complex], y: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex], x: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex], *, skip_model: typing.Optional[jaxtyping.PyTree[typing.Callable]] = None, param_type: str = 'sp', record_layers: bool = False, backward_energy_weight: Shaped[Array, ''] = 1.0, forward_energy_weight: Shaped[Array, ''] = 1.0) -&gt; jaxtyping.Shaped[Array, ''] | jax.Array</code>","text":"<p>Computes the energy of a bidirectional PC network (BPC, Oliviers et al., 2025) of the form</p> \\[ \\mathcal{F}(\\mathbf{z}; \u03b8) = 1/N \\sum_i^N \\left[ \\alpha_f \\sum_{\\ell=1}^L || \\mathbf{z}_{i, \\ell} - f_\\ell(\\mathbf{z}_{i, \\ell-1}; \\mathbf{W}_\\ell) ||^2/2 + \\alpha_b \\sum_{\\ell=0}^{L-1} || \\mathbf{z}_{i, \\ell} - g_{\\ell+1}(\\mathbf{z}_{i, \\ell+1}; \\mathbf{V}_{\\ell+1}) ||^2/2 \\right] \\] <p>where \\(f_\\ell(\\cdot)\\) and \\(g_{\\ell+1}(\\cdot)\\) are the forward (top-down) and  backward (bottom-up) layer-wise transformations, \\(\\mathbf{W}_\\ell\\) and  \\(\\mathbf{V}_{\\ell+1}\\) as forward and backward weights, and \\((\\alpha_f, \\alpha_b)\\)  as weightings of the forward and backward energies, respectively.  See the reference below for more details.</p> Reference <pre><code>@article{oliviers2025bidirectional,\n    title={Bidirectional predictive coding},\n    author={Oliviers, Gaspard and Tang, Mufeng and Bogacz, Rafal},\n    journal={arXiv preprint arXiv:2505.23415},\n    year={2025}\n}\n</code></pre> <p>Main arguments:</p> <ul> <li><code>top_down_model</code>: List of callable model (e.g. neural network) layers for      the forward model.</li> <li><code>bottom_up_model</code>: List of callable model (e.g. neural network) layers for      the backward model.</li> <li><code>activities</code>: List of activities for each layer free to vary.</li> <li><code>y</code>: Target of the <code>top_down_model</code> and input to the <code>bottom_up_model</code>.</li> <li><code>x</code>: Input to the <code>top_down_model</code> and target of the <code>bottom_up_model</code>.</li> </ul> <p>Other arguments:</p> <ul> <li><code>skip_model</code>: Optional skip connection model.</li> <li><code>param_type</code>: Determines the parameterisation. Options are <code>\"sp\"</code>      (standard parameterisation), <code>\"mupc\"</code> (\u03bcPC),      or <code>\"ntp\"</code> (neural tangent parameterisation).      See <code>_get_param_scalings()</code>      for the specific scalings of these different parameterisations. Defaults     to <code>\"sp\"</code>.</li> <li><code>record_layers</code>: If <code>True</code>, returns the energy of each layer.</li> <li><code>backward_energy_weight</code>: Scalar weighting for the backward energy terms.      Defaults to <code>1.0</code>.</li> <li><code>forward_energy_weight</code>: Scalar weighting for the forward energy terms.      Defaults to <code>1.0</code>.</li> </ul> <p>Returns:</p> <p>The total or layer-wise BPC energy normalised by batch size.</p>"},{"location":"api/Energy%20functions/#jpc.epc_energy_fn","title":"<code>jpc.epc_energy_fn(params: typing.Tuple[jaxtyping.PyTree[typing.Callable], typing.Optional[jaxtyping.PyTree[typing.Callable]]], errors: PyTree[jax.Array | numpy.ndarray | numpy.bool | numpy.number | bool | int | float | complex], y: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex], *, x: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex, NoneType] = None, loss: str = 'mse', param_type: str = 'sp') -&gt; jaxtyping.Shaped[Array, ''] | jax.Array</code>","text":"<p>Computes the error-reparameterised PC (ePC) energy for a neural network (Goemaere et al., 2025) of the form</p> \\[ \\mathcal{F} = \\sum_{\\ell=1}^{L-1} \\frac{1}{2} ||\\epsilon_\\ell||^2 + \\frac{1}{2}||\\mathbf{y} - \\tilde{\\mathbf{f}}(\\mathbf{z}_0, \\epsilon_1, \\dots, \\epsilon_{L-1})||^2 \\] <p>where \\(\\epsilon_\\ell = \\mathbf{z}_\\ell - f_\\ell(\\mathbf{W}_\\ell \\mathbf{z}_{\\ell-1})\\)  are the prediction errors at each layer, and  \\(\\tilde{\\mathbf{f}}(\\mathbf{z}_0, \\epsilon)\\) is an error-perturbed forward  pass. In ePC, errors are the variables updated during inference rather than  activities. See the reference below for more details.</p> Reference <pre><code>@article{goemaere2025error,\n    title={Error Optimization: Overcoming Exponential Signal Decay in Deep Predictive Coding Networks},\n    author={Goemaere, C{\\'e}dric and Oliviers, Gaspard and Bogacz, Rafal and Demeester, Thomas},\n    journal={arXiv preprint arXiv:2505.20137},\n    year={2025}\n}\n</code></pre> <p>Main arguments:</p> <ul> <li><code>params</code>: Tuple with callable model (e.g. neural network) layers and     optional skip connections.</li> <li><code>errors</code>: List of predictionerrors for each layer.</li> <li><code>y</code>: Observation or target of the generative model.</li> </ul> <p>Other arguments:</p> <ul> <li><code>x</code>: Optional prior of the generative model (for supervised training).</li> <li><code>loss</code>: Loss function to use at the output layer. Options are mean squared      error <code>\"mse\"</code> (default) or cross-entropy <code>\"ce\"</code>.</li> <li><code>param_type</code>: Determines the parameterisation. Options are <code>\"sp\"</code>      (standard parameterisation), <code>\"mupc\"</code> (\u03bcPC),      or <code>\"ntp\"</code> (neural tangent parameterisation).      See <code>_get_param_scalings()</code>      for the specific scalings of these different parameterisations. Defaults     to <code>\"sp\"</code>.</li> </ul> <p>Returns:</p> <p>The total ePC energy normalised by the batch size.</p>"},{"location":"api/Energy%20functions/#jpc._get_param_scalings","title":"<code>jpc._get_param_scalings(model: PyTree[typing.Callable], input: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex], *, skip_model: typing.Optional[jaxtyping.PyTree[typing.Callable]] = None, param_type: str = 'sp', gamma: typing.Optional[jaxtyping.Shaped[Array, '']] = None) -&gt; list[float]</code>","text":"<p>Gets layer scalings for a given parameterisation.</p> <p>Warning</p> <p><code>param_type = \"mupc\"</code> (\u03bcPC) assumes  that one is using <code>jpc.make_mlp()</code>  to create the model.</p> <p>Main arguments:</p> <ul> <li><code>model</code>: List of callable model (e.g. neural network) layers.</li> <li><code>input</code>: input to the model.</li> </ul> <p>Other arguments:</p> <ul> <li><code>skip_model</code>: Optional skip connection model.</li> <li><code>param_type</code>: Determines the parameterisation. Options are <code>\"sp\"</code>      (standard parameterisation), <code>\"mupc\"</code> (\u03bcPC),      or <code>\"ntp\"</code> (neural tangent parameterisation). Defaults to <code>\"sp\"</code>.</li> <li><code>gamma</code>: Optional scaling factor for the output layer. If provided, the output      layer scaling is multiplied by <code>1/gamma</code>. Defaults to <code>None</code> (no additional scaling).</li> </ul> <p>Returns:</p> <p>List with scalings for each layer.</p>"},{"location":"api/Gradients/","title":"Gradients","text":"<p>Info</p> <p>There are two similar functions to compute the gradient of the energy with respect to the activities of a standard PC energy: <code>jpc.neg_pc_activity_grad()</code>  and <code>jpc.compute_pc_activity_grad()</code>.  The first is used by <code>jpc.solve_inference()</code>  as gradient flow, while the second is for compatibility with discrete  optax optimisers such as  gradient descent.</p>"},{"location":"api/Gradients/#jpc.neg_pc_activity_grad","title":"<code>jpc.neg_pc_activity_grad(t: float | int, activities: PyTree[jax.Array | numpy.ndarray | numpy.bool | numpy.number | bool | int | float | complex], args: typing.Tuple[typing.Tuple[jaxtyping.PyTree[typing.Callable], typing.Optional[jaxtyping.PyTree[typing.Callable]]], typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex], typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex, NoneType], str, str, jaxtyping.Shaped[Array, ''], jaxtyping.Shaped[Array, ''], jaxtyping.Shaped[Array, ''], typing.Optional[jaxtyping.Shaped[Array, '']], diffrax._step_size_controller.base.AbstractStepSizeController]) -&gt; PyTree[jax.Array]</code>","text":"<p>Computes the negative gradient of the PC energy  with respect to the activities \\(- \u2207_{\\mathbf{z}} \\mathcal{F}\\).</p> <p>This defines an ODE system to be integrated by <code>jpc.solve_pc_inference()</code>.</p> <p>Main arguments:</p> <ul> <li><code>t</code>: Time step of the ODE system, used for downstream integration by     <code>diffrax.diffeqsolve()</code>.</li> <li><code>activities</code>: List of activities for each layer free to vary.</li> <li> <p><code>args</code>: 10-Tuple with:</p> <p>(i) Tuple with callable model layers and optional skip connections,</p> <p>(ii) model output (observation),</p> <p>(iii) model input (prior),</p> <p>(iv) loss specified at the output layer (<code>\"mse\"</code> as default or <code>\"ce\"</code>),</p> <p>(v) parameterisation type (<code>\"sp\"</code> as default, <code>\"mupc\"</code>, or <code>\"ntp\"</code>),</p> <p>(vi) \\(\\ell^2\\) regulariser for the weights (0 by default),</p> <p>(vii) spectral penalty for the weights (0 by default),</p> <p>(viii) \\(\\ell^2\\) regulariser for the activities (0 by default),</p> <p>(ix) optional scaling factor for the output layer (<code>None</code> by default), and</p> <p>(x) diffrax controller for step size integration.</p> </li> </ul> <p>Returns:</p> <p>List of negative gradients of the energy with respect to the activities.</p>"},{"location":"api/Gradients/#jpc.compute_pc_activity_grad","title":"<code>jpc.compute_pc_activity_grad(params: typing.Tuple[jaxtyping.PyTree[typing.Callable], typing.Optional[jaxtyping.PyTree[typing.Callable]]], activities: PyTree[jax.Array | numpy.ndarray | numpy.bool | numpy.number | bool | int | float | complex], y: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex], *, x: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex, NoneType], loss_id: str = 'mse', param_type: str = 'sp', weight_decay: Shaped[Array, ''] = 0.0, spectral_penalty: Shaped[Array, ''] = 0.0, activity_decay: Shaped[Array, ''] = 0.0, gamma: typing.Optional[jaxtyping.Shaped[Array, '']] = None) -&gt; PyTree[jax.Array]</code>","text":"<p>Computes the gradient of the PC energy with respect to the activities \\(\u2207_{\\mathbf{z}} \\mathcal{F}\\).</p> <p>Note</p> <p>This function differs from <code>jpc.neg_activity_grad()</code>  only in the sign of the gradient (positive as opposed to negative) and  is called in <code>jpc.update_activities()</code>  for use with any optax  optimiser.</p> <p>Main arguments:</p> <ul> <li><code>params</code>: Tuple with callable model layers and optional skip connections.</li> <li><code>activities</code>: List of activities for each layer free to vary.</li> <li><code>y</code>: Observation or target of the generative model.</li> </ul> <p>Other arguments:</p> <ul> <li><code>x</code>: Optional prior of the generative model.</li> <li><code>loss_id</code>: Loss function to use at the output layer. Options are mean squared      error <code>\"mse\"</code> (default) or cross-entropy <code>\"ce\"</code>.</li> <li><code>param_type</code>: Determines the parameterisation. Options are <code>\"sp\"</code>      (standard parameterisation), <code>\"mupc\"</code> (\u03bcPC),      or <code>\"ntp\"</code> (neural tangent parameterisation).      See <code>_get_param_scalings()</code>      for the specific scalings of these different parameterisations. Defaults     to <code>\"sp\"</code>.</li> <li><code>weight_decay</code>: \\(\\ell^2\\) regulariser for the weights (0 by default).</li> <li><code>spectral_penalty</code>: Weight spectral penalty of the form      \\(||\\mathbf{I} - \\mathbf{W}_\\ell^T \\mathbf{W}_\\ell||^2\\) (0 by default).</li> <li><code>activity_decay</code>: \\(\\ell^2\\) regulariser for the activities (0 by default).</li> <li><code>gamma</code>: Optional scaling factor for the output layer. If provided, the output      layer scaling is multiplied by <code>1/gamma</code>. Defaults to <code>None</code> (no additional scaling).</li> </ul> <p>Returns:</p> <p>The energy and its gradient with respect to the activities.</p>"},{"location":"api/Gradients/#jpc.compute_bpc_activity_grad","title":"<code>jpc.compute_bpc_activity_grad(top_down_model: PyTree[typing.Callable], bottom_up_model: PyTree[typing.Callable], activities: PyTree[jax.Array | numpy.ndarray | numpy.bool | numpy.number | bool | int | float | complex], y: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex], x: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex], *, skip_model: typing.Optional[jaxtyping.PyTree[typing.Callable]] = None, param_type: str = 'sp', backward_energy_weight: Shaped[Array, ''] = 1.0, forward_energy_weight: Shaped[Array, ''] = 1.0) -&gt; PyTree[jax.Array]</code>","text":"<p>Computes the gradient of the BPC energy with respect to the activities \\(\u2207_{\\mathbf{z}} \\mathcal{F}\\).</p> <p>Main arguments:</p> <ul> <li><code>top_down_model</code>: List of callable model (e.g. neural network) layers for      the forward model.</li> <li><code>bottom_up_model</code>: List of callable model (e.g. neural network) layers for      the backward model.</li> <li><code>activities</code>: List of activities for each layer free to vary.</li> <li><code>y</code>: Target of the <code>top_down_model</code> and input to the <code>bottom_up_model</code>.</li> <li><code>x</code>: Input to the <code>top_down_model</code> and target of the <code>bottom_up_model</code>.</li> </ul> <p>Other arguments:</p> <ul> <li><code>skip_model</code>: Optional skip connection model.</li> <li><code>param_type</code>: Determines the parameterisation. Options are <code>\"sp\"</code>      (standard parameterisation), <code>\"mupc\"</code> (\u03bcPC),      or <code>\"ntp\"</code> (neural tangent parameterisation).      See <code>_get_param_scalings()</code>      for the specific scalings of these different parameterisations. Defaults     to <code>\"sp\"</code>.</li> <li><code>backward_energy_weight</code>: Scalar weighting for the backward energy terms.      Defaults to <code>1.0</code>.</li> <li><code>forward_energy_weight</code>: Scalar weighting for the forward energy terms.      Defaults to <code>1.0</code>.</li> </ul> <p>Returns:</p> <p>The energy and its gradient with respect to the activities.</p>"},{"location":"api/Gradients/#jpc.compute_epc_error_grad","title":"<code>jpc.compute_epc_error_grad(params: typing.Tuple[jaxtyping.PyTree[typing.Callable], typing.Optional[jaxtyping.PyTree[typing.Callable]]], errors: PyTree[jax.Array | numpy.ndarray | numpy.bool | numpy.number | bool | int | float | complex], y: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex], *, x: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex, NoneType] = None, loss_id: str = 'mse', param_type: str = 'sp') -&gt; typing.Tuple[jaxtyping.Shaped[Array, ''], jaxtyping.PyTree[jax.Array]]</code>","text":"<p>Computes the gradient of the ePC energy with respect to the errors \\(\u2207_{\\epsilon} \\mathcal{F}\\).</p> <p>Note</p> <p>In ePC, errors are updated during inference rather than activities.</p> <p>Main arguments:</p> <ul> <li><code>params</code>: Tuple with callable model layers and optional skip connections.</li> <li><code>errors</code>: List of errors for each layer free to vary.</li> <li><code>y</code>: Observation or target of the generative model.</li> </ul> <p>Other arguments:</p> <ul> <li><code>x</code>: Optional prior of the generative model.</li> <li><code>loss_id</code>: Loss function to use at the output layer. Options are mean squared      error <code>\"mse\"</code> (default) or cross-entropy <code>\"ce\"</code>.</li> <li><code>param_type</code>: Determines the parameterisation. Options are <code>\"sp\"</code>      (standard parameterisation), <code>\"mupc\"</code> (\u03bcPC),      or <code>\"ntp\"</code> (neural tangent parameterisation).      See <code>_get_param_scalings()</code>      for the specific scalings of these different parameterisations. Defaults     to <code>\"sp\"</code>.</li> </ul> <p>Returns:</p> <p>The energy and its gradient with respect to the errors.</p>"},{"location":"api/Gradients/#jpc.compute_pc_param_grads","title":"<code>jpc.compute_pc_param_grads(params: typing.Tuple[jaxtyping.PyTree[typing.Callable], typing.Optional[jaxtyping.PyTree[typing.Callable]]], activities: PyTree[jax.Array | numpy.ndarray | numpy.bool | numpy.number | bool | int | float | complex], y: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex], *, x: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex, NoneType] = None, loss_id: str = 'mse', param_type: str = 'sp', weight_decay: Shaped[Array, ''] = 0.0, spectral_penalty: Shaped[Array, ''] = 0.0, activity_decay: Shaped[Array, ''] = 0.0, gamma: typing.Optional[jaxtyping.Shaped[Array, '']] = None) -&gt; typing.Tuple[jaxtyping.PyTree[jax.Array], jaxtyping.PyTree[jax.Array]]</code>","text":"<p>Computes the gradient of the PC energy with respect to model parameters \\(\u2207_\u03b8 \\mathcal{F}\\).</p> <p>Main arguments:</p> <ul> <li><code>params</code>: Tuple with callable model layers and optional skip connections.</li> <li><code>activities</code>: List of activities for each layer free to vary.</li> <li><code>y</code>: Observation or target of the generative model.</li> </ul> <p>Other arguments:</p> <ul> <li><code>x</code>: Optional prior of the generative model.</li> <li><code>loss_id</code>: Loss function to use at the output layer. Options are mean squared      error <code>\"mse\"</code> (default) or cross-entropy <code>\"ce\"</code>.</li> <li><code>param_type</code>: Determines the parameterisation. Options are <code>\"sp\"</code>      (standard parameterisation), <code>\"mupc\"</code> (\u03bcPC),      or <code>\"ntp\"</code> (neural tangent parameterisation).      See <code>_get_param_scalings()</code>      for the specific scalings of these different parameterisations. Defaults     to <code>\"sp\"</code>.</li> <li><code>weight_decay</code>: \\(\\ell^2\\) regulariser for the weights (0 by default).</li> <li><code>spectral_penalty</code>: Weight spectral penalty of the form      \\(||\\mathbf{I} - \\mathbf{W}_\\ell^T \\mathbf{W}_\\ell||^2\\) (0 by default).</li> <li><code>activity_decay</code>: \\(\\ell^2\\) regulariser for the activities (0 by default).</li> <li><code>gamma</code>: Optional scaling factor for the output layer. If provided, the output      layer scaling is multiplied by <code>1/gamma</code>. Defaults to <code>None</code> (no additional scaling).</li> </ul> <p>Returns:</p> <p>List of parameter gradients for each model layer.</p>"},{"location":"api/Gradients/#jpc.compute_hpc_param_grads","title":"<code>jpc.compute_hpc_param_grads(model: PyTree[typing.Callable], equilib_activities: PyTree[jax.Array | numpy.ndarray | numpy.bool | numpy.number | bool | int | float | complex], amort_activities: PyTree[jax.Array | numpy.ndarray | numpy.bool | numpy.number | bool | int | float | complex], x: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex], y: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex, NoneType] = None) -&gt; PyTree[jax.Array]</code>","text":"<p>Computes the gradient of the hybrid PC energy  with respect to the amortiser's parameters \\(\u2207_\u03b8 \\mathcal{F}\\).</p> <p>Warning</p> <p>The input \\(x\\) and output \\(y\\) are reversed compared to  <code>jpc.compute_pc_param_grads()</code>  (\\(x\\) is the generator's target and \\(y\\) is its optional input or prior).  Just think of \\(x\\) and \\(y\\) as the actual input and output of the  amortiser, respectively.</p> <p>Main arguments:</p> <ul> <li><code>model</code>: List of callable model (e.g. neural network) layers.</li> <li><code>equilib_activities</code>: List of equilibrated activities reached by the     generator and target for the amortiser.</li> <li><code>amort_activities</code>: List of amortiser's feedforward guesses     (initialisation) for the model activities.</li> <li><code>x</code>: Input to the amortiser.</li> <li><code>y</code>: Optional target of the amortiser (for supervised training).</li> </ul> <p>Returns:</p> <p>List of parameter gradients for each model layer.</p>"},{"location":"api/Gradients/#jpc.compute_bpc_param_grads","title":"<code>jpc.compute_bpc_param_grads(top_down_model: PyTree[typing.Callable], bottom_up_model: PyTree[typing.Callable], activities: PyTree[jax.Array | numpy.ndarray | numpy.bool | numpy.number | bool | int | float | complex], y: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex], x: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex], *, skip_model: typing.Optional[jaxtyping.PyTree[typing.Callable]] = None, param_type: str = 'sp', backward_energy_weight: Shaped[Array, ''] = 1.0, forward_energy_weight: Shaped[Array, ''] = 1.0) -&gt; typing.Tuple[jaxtyping.PyTree[jax.Array], jaxtyping.PyTree[jax.Array]]</code>","text":"<p>Computes the gradient of the BPC energy with respect to all the model parameters \\(\u2207_\u03b8 \\mathcal{F}\\).</p> <p>Main arguments:</p> <ul> <li><code>top_down_model</code>: List of callable model (e.g. neural network) layers for      the forward model.</li> <li><code>bottom_up_model</code>: List of callable model (e.g. neural network) layers for      the backward model.</li> <li><code>activities</code>: List of activities for each layer free to vary.</li> <li><code>y</code>: Target of the <code>top_down_model</code> and input to the <code>bottom_up_model</code>.</li> <li><code>x</code>: Input to the <code>top_down_model</code> and target of the <code>bottom_up_model</code>.</li> </ul> <p>Other arguments:</p> <ul> <li><code>skip_model</code>: Optional skip connection model.</li> <li><code>param_type</code>: Determines the parameterisation. Options are <code>\"sp\"</code>      (standard parameterisation), <code>\"mupc\"</code> (\u03bcPC),      or <code>\"ntp\"</code> (neural tangent parameterisation).      See <code>_get_param_scalings()</code>      for the specific scalings of these different parameterisations. Defaults     to <code>\"sp\"</code>.</li> <li><code>backward_energy_weight</code>: Scalar weighting for the backward energy terms.      Defaults to <code>1.0</code>.</li> <li><code>forward_energy_weight</code>: Scalar weighting for the forward energy terms.      Defaults to <code>1.0</code>.</li> </ul> <p>Returns:</p> <p>Tuple of parameter gradients for the top-down and bottom-up models.</p>"},{"location":"api/Gradients/#jpc.compute_epc_param_grads","title":"<code>jpc.compute_epc_param_grads(params: typing.Tuple[jaxtyping.PyTree[typing.Callable], typing.Optional[jaxtyping.PyTree[typing.Callable]]], errors: PyTree[jax.Array | numpy.ndarray | numpy.bool | numpy.number | bool | int | float | complex], y: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex], *, x: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex, NoneType] = None, loss_id: str = 'mse', param_type: str = 'sp') -&gt; typing.Tuple[jaxtyping.PyTree[jax.Array], jaxtyping.PyTree[jax.Array]]</code>","text":"<p>Computes the gradient of the ePC energy with respect to model parameters \\(\u2207_\u03b8 \\mathcal{F}\\).</p> <p>Main arguments:</p> <ul> <li><code>params</code>: Tuple with callable model layers and optional skip connections.</li> <li><code>errors</code>: List of errors for each layer free to vary.</li> <li><code>y</code>: Observation or target of the generative model.</li> </ul> <p>Other arguments:</p> <ul> <li><code>x</code>: Optional prior of the generative model.</li> <li><code>loss_id</code>: Loss function to use at the output layer. Options are mean squared      error <code>\"mse\"</code> (default) or cross-entropy <code>\"ce\"</code>.</li> <li><code>param_type</code>: Determines the parameterisation. Options are <code>\"sp\"</code>      (standard parameterisation), <code>\"mupc\"</code> (\u03bcPC),      or <code>\"ntp\"</code> (neural tangent parameterisation).      See <code>_get_param_scalings()</code>      for the specific scalings of these different parameterisations. Defaults     to <code>\"sp\"</code>.</li> </ul> <p>Returns:</p> <p>List of parameter gradients for each model layer.</p>"},{"location":"api/Initialisation/","title":"Initialisation","text":"<p>JPC provides 4 ways of initialising the activities of a PC network: </p> <ul> <li>jpc.init_activities_with_ffwd() for a feedforward pass (standard), </li> <li>jpc.init_activities_from_normal() for random initialisation, </li> <li>jpc.init_activities_with_amort() for use of an amortised network, and</li> <li>jpc.init_epc_errors() for zero-initialisation of prediction errors in ePC.</li> </ul>"},{"location":"api/Initialisation/#jpc.init_activities_with_ffwd","title":"<code>jpc.init_activities_with_ffwd(model: PyTree[typing.Callable], input: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex], *, skip_model: typing.Optional[jaxtyping.PyTree[typing.Callable]] = None, param_type: str = 'sp', gamma: typing.Optional[jaxtyping.Shaped[Array, '']] = None) -&gt; PyTree[jax.Array]</code>","text":"<p>Initialises the layers' activity with a feedforward pass \\(\\{ f_\\ell(\\mathbf{z}_{\\ell-1}) \\}_{\\ell=1}^L\\) where \\(f_\\ell(\\cdot)\\) is some callable layer transformation and \\(\\mathbf{z}_0 = \\mathbf{x}\\) is the input.</p> <p>Warning</p> <p><code>param_type = \"mupc\"</code> (\u03bcPC) assumes  that one is using <code>jpc.make_mlp()</code>  to create the model.</p> <p>Main arguments:</p> <ul> <li><code>model</code>: List of callable model (e.g. neural network) layers.</li> <li><code>input</code>: input to the model.</li> </ul> <p>Other arguments:</p> <ul> <li><code>skip_model</code>: Optional skip connection model.</li> <li><code>param_type</code>: Determines the parameterisation. Options are <code>\"sp\"</code>      (standard parameterisation), <code>\"mupc\"</code> (\u03bcPC),      or <code>\"ntp\"</code> (neural tangent parameterisation).      See <code>_get_param_scalings()</code>      for the specific scalings of these different parameterisations. Defaults     to <code>\"sp\"</code>.</li> <li><code>gamma</code>: Optional scaling factor for the output layer. If provided, the output      layer scaling is multiplied by <code>1/gamma</code>. Defaults to <code>None</code> (no additional scaling).</li> </ul> <p>Returns:</p> <p>List with activity values of each layer.</p>"},{"location":"api/Initialisation/#jpc.init_activities_from_normal","title":"<code>jpc.init_activities_from_normal(key: typing.Union[jaxtyping.Key[Array, ''], jaxtyping.UInt32[Array, 2]], layer_sizes: PyTree[int], mode: str, batch_size: int, sigma: Shaped[Array, ''] = 0.05) -&gt; PyTree[jax.Array]</code>","text":"<p>Initialises network activities from a zero-mean Gaussian  \\(z_i \\sim \\mathcal{N}(0, \\sigma^2)\\).</p> <p>Main arguments:</p> <ul> <li><code>key</code>: <code>jax.random.PRNGKey</code> for sampling.</li> <li><code>layer_sizes</code>: List with dimension of all layers (input, hidden and     output).</li> <li><code>mode</code>: If <code>\"supervised\"</code>, all hidden layers are initialised. If     <code>\"unsupervised\"</code> the input layer \\(\\mathbf{z}_0\\) is also initialised.</li> <li><code>batch_size</code>: Dimension of data batch.</li> <li><code>sigma</code>: Standard deviation for Gaussian to sample activities from.     Defaults to 5e-2.</li> </ul> <p>Returns:</p> <p>List of randomly initialised activities for each layer.</p>"},{"location":"api/Initialisation/#jpc.init_activities_with_amort","title":"<code>jpc.init_activities_with_amort(amortiser: PyTree[typing.Callable], generator: PyTree[typing.Callable], input: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex]) -&gt; PyTree[jax.Array]</code>","text":"<p>Initialises layers' activity with an amortised network \\(\\{ f_{L-\\ell+1}(\\mathbf{z}_{L-\\ell}) \\}_{\\ell=1}^L\\) where \\(\\mathbf{z}_0 = \\mathbf{y}\\) is the input or generator's target.</p> <p>Note</p> <p>The output order is reversed for downstream use by the generator.</p> <p>Main arguments:</p> <ul> <li><code>amortiser</code>: List of callable layers for model amortising the inference     of the <code>generator</code>.</li> <li><code>generator</code>: List of callable layers for the generative model.</li> <li><code>input</code>: Input to the amortiser.</li> </ul> <p>Returns:</p> <p>List with amortised initialisation of each layer.</p>"},{"location":"api/Initialisation/#jpc.init_epc_errors","title":"<code>jpc.init_epc_errors(layer_sizes: PyTree[int], batch_size: int, mode: str = 'supervised') -&gt; PyTree[jax.Array]</code>","text":"<p>Initialises zero errors for use with ePC \\(\\{ \\epsilon_\\ell = 0 \\}_{l=1}^L\\).</p> <p>Main arguments:</p> <ul> <li><code>layer_sizes</code>: List with dimension of all layers (input, hidden and     output).</li> <li><code>batch_size</code>: Dimension of data batch.</li> <li><code>mode</code>: If <code>\"supervised\"</code>, errors are initialised for layers 1 to L-1     (hidden layers only). If <code>\"unsupervised\"</code>, errors are initialised for     layer 0 (input) and layers 1 to L-1. Defaults to <code>\"supervised\"</code>.</li> </ul> <p>Returns:</p> <p>List of zero-initialised error arrays for each layer.</p>"},{"location":"api/Testing/","title":"Testing","text":"<p>JPC provides a few convenience functions to test different types of PC network (PCN):</p> <ul> <li>jpc.test_discriminative_pc()  for test loss and accuracy of discriminative PCNs;</li> <li>jpc.test_generative_pc()  for accuracy and output predictions of generative PCNs; and</li> <li>jpc.test_hpc() for accuracy of all models (amortiser, generator, &amp; hybrid) as well as output predictions.</li> </ul>"},{"location":"api/Testing/#jpc.test_discriminative_pc","title":"<code>jpc.test_discriminative_pc(model: PyTree[typing.Callable], output: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex], input: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex], *, skip_model: typing.Optional[jaxtyping.PyTree[typing.Callable]] = None, loss: str = 'mse', param_type: str = 'sp') -&gt; typing.Tuple[jaxtyping.Shaped[Array, ''], jaxtyping.Shaped[Array, '']]</code>","text":"<p>Computes test metrics for a discriminative predictive coding network.</p> <p>Main arguments:</p> <ul> <li><code>model</code>: List of callable model (e.g. neural network) layers.</li> <li><code>output</code>: Observation or target of the generative model.</li> <li><code>input</code>: Optional prior of the generative model.</li> </ul> <p>Other arguments:</p> <ul> <li><code>skip_model</code>: Optional skip connection model.</li> <li><code>loss</code>: Loss function to use at the output layer. Options are mean squared      error <code>\"mse\"</code> (default) or cross-entropy <code>\"ce\"</code>.</li> <li><code>param_type</code>: Determines the parameterisation. Options are <code>\"sp\"</code>      (standard parameterisation), <code>\"mupc\"</code> (\u03bcPC),      or <code>\"ntp\"</code> (neural tangent parameterisation). See <code>_get_param_scalings()</code>      for the specific scalings of these different parameterisations. Defaults     to <code>\"sp\"</code>.</li> </ul> <p>Returns:</p> <p>Test loss and accuracy of output predictions.</p>"},{"location":"api/Testing/#jpc.test_generative_pc","title":"<code>jpc.test_generative_pc(model: PyTree[typing.Callable], output: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex], input: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex], key: typing.Union[jaxtyping.Key[Array, ''], jaxtyping.UInt32[Array, 2]], layer_sizes: PyTree[int], batch_size: int, *, skip_model: typing.Optional[jaxtyping.PyTree[typing.Callable]] = None, loss_id: str = 'mse', param_type: str = 'sp', sigma: Shaped[Array, ''] = 0.05, ode_solver: AbstractSolver = Heun(), max_t1: int = 500, dt: jaxtyping.Shaped[Array, ''] | int = None, stepsize_controller: AbstractStepSizeController = PIDController(rtol=0.001, atol=0.001), weight_decay: Shaped[Array, ''] = 0.0, spectral_penalty: Shaped[Array, ''] = 0.0, activity_decay: Shaped[Array, ''] = 0.0) -&gt; typing.Tuple[jaxtyping.Shaped[Array, ''], jax.Array]</code>","text":"<p>Computes test metrics for a generative predictive coding network.</p> <p>Gets output predictions (e.g. of an image given a label) with a feedforward pass and calculates accuracy of inferred input (e.g. of a label given an image).</p> <p>Main arguments:</p> <ul> <li><code>model</code>: List of callable model (e.g. neural network) layers.</li> <li><code>output</code>: Observation or target of the generative model.</li> <li><code>input</code>: Prior of the generative model.</li> <li><code>key</code>: <code>jax.random.PRNGKey</code> for random initialisation of activities.</li> <li><code>layer_sizes</code>: Dimension of all layers (input, hidden and output).</li> <li><code>batch_size</code>: Dimension of data batch for activity initialisation.</li> </ul> <p>Other arguments:</p> <ul> <li><code>skip_model</code>: Optional skip connection model.</li> <li><code>loss_id</code>: Loss function to use at the output layer. Options are mean squared      error <code>\"mse\"</code> (default) or cross-entropy <code>\"ce\"</code>.</li> <li><code>param_type</code>: Determines the parameterisation. Options are <code>\"sp\"</code>      (standard parameterisation), <code>\"mupc\"</code> (\u03bcPC),      or <code>\"ntp\"</code> (neural tangent parameterisation). See <code>_get_param_scalings()</code>      for the specific scalings of these different parameterisations. Defaults     to <code>\"sp\"</code>.</li> <li><code>sigma</code>: Standard deviation for Gaussian to sample activities from.     Defaults to 5e-2.</li> <li><code>ode_solver</code>: diffrax ODE solver      to be used. Default is <code>Heun</code>,      a 2nd order explicit Runge--Kutta method.</li> <li><code>max_t1</code>: Maximum end of integration region (500 by default).</li> <li><code>dt</code>: Integration step size. Defaults to None since the default     <code>stepsize_controller</code> will automatically determine it.</li> <li><code>stepsize_controller</code>: diffrax controller      for step size integration. Defaults to <code>PIDController</code>.      Note that the relative and absolute tolerances of the controller will      also determine the steady state to terminate the solver.</li> <li><code>weight_decay</code>: Weight decay for the weights (0 by default).</li> <li><code>spectral_penalty</code>: Weight spectral penalty of the form      \\(||\\mathbf{I} - \\mathbf{W}_\\ell^T \\mathbf{W}_\\ell||^2\\) (0 by default).</li> <li><code>activity_decay</code>: Activity decay for the activities (0 by default).</li> </ul> <p>Returns:</p> <p>Accuracy and output predictions.</p>"},{"location":"api/Testing/#jpc.test_hpc","title":"<code>jpc.test_hpc(generator: PyTree[typing.Callable], amortiser: PyTree[typing.Callable], output: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex], input: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex], key: typing.Union[jaxtyping.Key[Array, ''], jaxtyping.UInt32[Array, 2]], layer_sizes: PyTree[int], batch_size: int, sigma: Shaped[Array, ''] = 0.05, ode_solver: AbstractSolver = Heun(), max_t1: int = 500, dt: jaxtyping.Shaped[Array, ''] | int = None, stepsize_controller: AbstractStepSizeController = PIDController(rtol=0.001, atol=0.001)) -&gt; typing.Tuple[jaxtyping.Shaped[Array, ''], jaxtyping.Shaped[Array, ''], jaxtyping.Shaped[Array, ''], jax.Array]</code>","text":"<p>Computes test metrics for hybrid predictive coding trained in a supervised manner.</p> <p>Calculates input accuracy of (i) amortiser, (ii) generator, and (iii) hybrid (amortiser + generator). Also returns output predictions (e.g. of an image given a label) with a feedforward pass of the generator.</p> <p>Note</p> <p>The input and output of the generator are the output and input of the amortiser, respectively.</p> <p>Main arguments:</p> <ul> <li><code>generator</code>: List of callable layers for the generative model.</li> <li><code>amortiser</code>: List of callable layers for model amortising the inference     of the <code>generator</code>.</li> <li><code>output</code>: Observation or target of the generative model.</li> <li><code>input</code>: Optional prior of the generator, target for the amortiser.</li> <li><code>key</code>: <code>jax.random.PRNGKey</code> for random initialisation of activities.</li> <li><code>layer_sizes</code>: Dimension of all layers (input, hidden and output).</li> <li><code>batch_size</code>: Dimension of data batch for initialisation of activities.</li> </ul> <p>Other arguments:</p> <ul> <li><code>sigma</code>: Standard deviation for Gaussian to sample activities from.     Defaults to 5e-2.</li> <li><code>ode_solver</code>: diffrax ODE solver      to be used. Default is <code>Heun</code>,      a 2nd order explicit Runge--Kutta method.</li> <li><code>max_t1</code>: Maximum end of integration region (500 by default).</li> <li><code>dt</code>: Integration step size. Defaults to None since the default     <code>stepsize_controller</code> will automatically determine it.</li> <li><code>stepsize_controller</code>: diffrax controller      for step size integration. Defaults to <code>PIDController</code>.      Note that the relative and absolute tolerances of the controller will      also determine the steady state to terminate the solver.</li> </ul> <p>Returns:</p> <p>Accuracies of all models and output predictions.</p>"},{"location":"api/Theoretical%20tools/","title":"Theoretical tools","text":"<p>JPC provides the following main theoretical tools that can be used to study  deep linear networks (DLNs) trained with PC: </p> <ul> <li>jpc.linear_equilib_energy()  to compute the theoretical PC energy at the solution of the activities for DLNs;</li> <li>jpc.compute_linear_activity_hessian()  to compute the theoretical Hessian of the energy with respect to the activities of DLNs; </li> <li>jpc.compute_linear_activity_solution()  to compute the analytical PC inference solution for DLNs.</li> </ul>"},{"location":"api/Theoretical%20tools/#jpc.linear_equilib_energy","title":"<code>jpc.linear_equilib_energy(params: typing.Tuple[jaxtyping.PyTree[typing.Callable], typing.Optional[jaxtyping.PyTree[typing.Callable]]], x: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex], y: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex], *, param_type: str = 'sp', gamma: typing.Optional[jaxtyping.Shaped[Array, '']] = None, return_rescaling: bool = False) -&gt; typing.Union[jax.Array, typing.Tuple[jax.Array, jax.Array]]</code>","text":"<p>Computes the theoretical PC energy  at the solution of the activities for a deep linear network (Innocenti et al. 2024):</p> \\[ \\mathcal{F}^* = 1/2N \\sum_i^N (\\mathbf{y}_i - \\mathbf{W}_{L:1}\\mathbf{x}_i)^T \\mathbf{S}^{-1}(\\mathbf{y}_i - \\mathbf{W}_{L:1}\\mathbf{x}_i) \\] <p>where \\(\\mathbf{S} = \\mathbf{I}_{d_y} + \\sum_{\\ell=2}^L (\\mathbf{W}_{L:\\ell})(\\mathbf{W}_{L:\\ell})^T\\) and \\(\\mathbf{W}_{k:\\ell} = \\mathbf{W}_k \\dots \\mathbf{W}_\\ell\\) for \\(\\ell, k \\in 1,\\dots, L\\).</p> <p>Note</p> <p>This expression assumes no biases. It could also be generalised to  other network architectures (e.g. ResNets) and parameterisations (see Innocenti et al. 2025).  However, note that the equilibrated energy for ResNets and other parameterisations can still be computed by getting the activity solution with <code>jpc.compute_linear_activity_solution()</code>  and then plugging this into the standard PC energy  jpc.pc_energy_fn().</p> <p>Example</p> <p>In practice, this means that if you run, at any point in training, the  inference dynamics of any PC linear network to equilibrium, then  <code>jpc.pc_energy_fn()</code>  will return the same energy value as this function. For a demonstration, see this example notebook.</p> <p></p> Reference <pre><code>@article{innocenti2025only,\n    title={Only Strict Saddles in the Energy Landscape of Predictive Coding Networks?},\n    author={Innocenti, Francesco and Achour, El Mehdi and Singh, Ryan and Buckley, Christopher L},\n    journal={Advances in Neural Information Processing Systems},\n    volume={37},\n    pages={53649--53683},\n    year={2025}\n}\n</code></pre> <p>Main arguments:</p> <ul> <li><code>params</code>: Tuple with callable network layers and optional skip connections.</li> <li><code>x</code>: Network input.</li> <li><code>y</code>: Network output.</li> </ul> <p>Other arguments:</p> <ul> <li><code>param_type</code>: Determines the parameterisation. Options are <code>\"sp\"</code>      (standard parameterisation), <code>\"mupc\"</code> (\u03bcPC),      or <code>\"ntp\"</code> (neural tangent parameterisation).      See <code>_get_param_scalings()</code>      for the specific scalings of these different parameterisations. Defaults     to <code>\"sp\"</code>.</li> <li><code>gamma</code>: Optional scaling factor for the output layer. If provided, the      output layer scaling is multiplied by <code>1/gamma</code>. Defaults to <code>None</code> (no      additional scaling).</li> <li><code>return_rescaling</code>: If <code>True</code>, also returns the rescaling matrix <code>S</code>.      Defaults to <code>False</code>.</li> </ul> <p>Returns:</p> <p>Mean total theoretical energy over a data batch and optionally the rescaling  matrix <code>S</code>.</p>"},{"location":"api/Theoretical%20tools/#jpc.compute_linear_equilib_energy_grads","title":"<code>jpc.compute_linear_equilib_energy_grads(params: typing.Tuple[jaxtyping.PyTree[typing.Callable], typing.Optional[jaxtyping.PyTree[typing.Callable]]], x: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex], y: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex], *, param_type: str = 'sp', gamma: typing.Optional[jaxtyping.Shaped[Array, '']] = None) -&gt; PyTree[jax.Array]</code>","text":"<p>Computes the gradient of the linear equilibrium energy with respect to model parameters \\(\u2207_\u03b8 \\mathcal{F}^*\\).</p> <p>Main arguments:</p> <ul> <li><code>params</code>: Tuple with callable model layers and optional skip connections.</li> <li><code>x</code>: Network input.</li> <li><code>y</code>: Network output.</li> </ul> <p>Other arguments:</p> <ul> <li><code>param_type</code>: Determines the parameterisation. Options are <code>\"sp\"</code>      (standard parameterisation), <code>\"mupc\"</code> (\u03bcPC),      or <code>\"ntp\"</code> (neural tangent parameterisation).      See <code>_get_param_scalings()</code>      for the specific scalings of these different parameterisations. Defaults     to <code>\"sp\"</code>.</li> <li><code>gamma</code>: Optional scaling factor for the output layer. If provided, the      output layer scaling is multiplied by <code>1/gamma</code>. Defaults to <code>None</code> (no      additional scaling).</li> </ul> <p>Returns:</p> <p>Parameter gradients for the network.</p>"},{"location":"api/Theoretical%20tools/#jpc.update_linear_equilib_energy_params","title":"<code>jpc.update_linear_equilib_energy_params(params: typing.Tuple[jaxtyping.PyTree[typing.Callable], typing.Optional[jaxtyping.PyTree[typing.Callable]]], optim: optax._src.base.GradientTransformation | optax._src.base.GradientTransformationExtraArgs, opt_state: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, typing.Iterable[ForwardRef(ArrayTree)], typing.Mapping[typing.Any, ForwardRef(ArrayTree)]], x: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex], y: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex], *, param_type: str = 'sp', gamma: typing.Optional[jaxtyping.Shaped[Array, '']] = None) -&gt; typing.Dict</code>","text":"<p>Updates parameters of a linear network by taking gradients of the  linear equilibrium energy with a given optax optimiser.</p> <p>Main arguments:</p> <ul> <li><code>params</code>: Tuple with callable model layers and optional skip connections.</li> <li><code>optim</code>: optax optimiser, e.g. <code>optax.sgd()</code>.</li> <li><code>opt_state</code>: State of optax optimiser.</li> <li><code>x</code>: Network input.</li> <li><code>y</code>: Network output.</li> </ul> <p>Other arguments:</p> <ul> <li><code>param_type</code>: Determines the parameterisation. Options are <code>\"sp\"</code>      (standard parameterisation), <code>\"mupc\"</code> (\u03bcPC),      or <code>\"ntp\"</code> (neural tangent parameterisation).      See <code>_get_param_scalings()</code>      for the specific scalings of these different parameterisations. Defaults     to <code>\"sp\"</code>.</li> <li><code>gamma</code>: Optional scaling factor for the output layer. If provided, the      output layer scaling is multiplied by <code>1/gamma</code>. Defaults to <code>None</code> (no      additional scaling).</li> </ul> <p>Returns:</p> <p>Dictionary with updated model, parameter gradients, and optimiser state.</p>"},{"location":"api/Theoretical%20tools/#jpc.compute_linear_activity_hessian","title":"<code>jpc.compute_linear_activity_hessian(Ws: PyTree[jax.Array], *, use_skips: bool = False, param_type: str = 'sp', activity_decay: bool = False, diag: bool = True, off_diag: bool = True, gamma: typing.Optional[jaxtyping.Shaped[Array, '']] = None) -&gt; Array</code>","text":"<p>Computes the theoretical Hessian matrix of the PC energy  with respect to the activities for a linear network,  \\((\\mathbf{H}_{\\mathbf{z}})_{\\ell k} := \\partial^2 \\mathcal{F} / \\partial \\mathbf{z}_\\ell \\partial \\mathbf{z}_k \\in \\mathbb{R}^{(NH)\u00d7(NH)}\\)  where \\(N\\) and \\(H\\) are the width and number of hidden layers, respectively (Innocenti et al., 2025).</p> <p>Info</p> <p>This function can be used (i) to study the inference landscape of linear PC networks and (ii) to compute the analytical solution with  <code>jpc.compute_linear_activity_solution()</code>.</p> <p>Warning</p> <p>This was highly hard-coded for quick experimental iteration with  different models and parameterisations. The computation of the blocks could be implemented much more elegantly by fetching the  transformation for each layer.</p> Reference <pre><code>@article{innocenti2025mu,\n    title={$$\backslash$mu $ PC: Scaling Predictive Coding to 100+ Layer Networks},\n    author={Innocenti, Francesco and Achour, El Mehdi and Buckley, Christopher L},\n    journal={arXiv preprint arXiv:2505.13124},\n    year={2025}\n}\n</code></pre> <p>Main arguments:</p> <ul> <li><code>Ws</code>: List of all the network weight matrices.</li> </ul> <p>Other arguments:</p> <ul> <li><code>use_skips</code>: Whether to assume one-layer skip connections at every layer      except from the input and to the output. <code>False</code> by default.</li> <li><code>param_type</code>: Determines the parameterisation. Options are <code>\"sp\"</code>      (standard parameterisation), <code>\"mupc\"</code> (\u03bcPC),      or <code>\"ntp\"</code> (neural tangent parameterisation).      See <code>_get_param_scalings()</code>      for the specific scalings of these different parameterisations. Defaults     to <code>\"sp\"</code>.</li> <li><code>activity_decay</code>: \\(\\ell^2\\) regulariser for the activities.</li> <li><code>diag</code>: Whether to compute the diagonal blocks of the Hessian.</li> <li><code>off-diag</code>: Whether to compute the off-diagonal blocks of the Hessian.</li> <li><code>gamma</code>: Optional scaling factor for the output layer. If provided, the output      layer scaling is multiplied by <code>1/gamma</code>. Defaults to <code>None</code> (no additional scaling).</li> </ul> <p>Returns:</p> <p>The activity Hessian matrix of size \\(NH\u00d7NH\\) where \\(N\\) is the width and \\(H\\)  is the number of hidden layers.</p>"},{"location":"api/Theoretical%20tools/#jpc.compute_linear_activity_solution","title":"<code>jpc.compute_linear_activity_solution(model: PyTree[equinox.nn._linear.Linear], x: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex], y: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex], *, use_skips: bool = False, param_type: str = 'sp', activity_decay: bool = False, gamma: typing.Optional[jaxtyping.Shaped[Array, '']] = None, epsilon: Shaped[Array, ''] = 0.0, hessian: typing.Optional[jax.Array] = None) -&gt; PyTree[jax.Array]</code>","text":"<p>Computes the theoretical solution for the PC activities of a linear network (Innocenti et al., 2025).</p> \\[ \\mathbf{z}^* = \\mathbf{H}_{\\mathbf{z}}^{-1}\\mathbf{b} \\] <p>where \\((\\mathbf{H}_{\\mathbf{z}})_{\\ell k} := \\partial^2 \\mathcal{F} / \\partial \\mathbf{z}_\\ell \\partial \\mathbf{z}_k \\in \\mathbb{R}^{(NH)\u00d7(NH)}\\)  is the Hessian of the energy with respect to the activities, and  \\(\\mathbf{b} \\in \\mathbb{R}^{NH}\\) is a sparse vector depending only on the  data and associated weights. The activity Hessian is computed analytically  using <code>jpc.compute_linear_activity_hessian()</code>.    </p> <p>Info</p> <p>This can be used to study how linear PC networks learn when they perform  perfect inference.</p> Reference <pre><code>@article{innocenti2025mu,\n    title={$$\backslash$mu $ PC: Scaling Predictive Coding to 100+ Layer Networks},\n    author={Innocenti, Francesco and Achour, El Mehdi and Buckley, Christopher L},\n    journal={arXiv preprint arXiv:2505.13124},\n    year={2025}\n}\n</code></pre> <p>Main arguments:</p> <ul> <li><code>model</code>: Linear network defined as a list of Equinox Linear layers.</li> <li><code>x</code>: Network input.</li> <li><code>y</code>: Network output.</li> </ul> <p>Other arguments:</p> <ul> <li><code>use_skips</code>: Whether to assume one-layer skip connections at every layer      except from the input and to the output. <code>False</code> by default.</li> <li><code>param_type</code>: Determines the parameterisation. Options are <code>\"sp\"</code>      (standard parameterisation), <code>\"mupc\"</code> (\u03bcPC),      or <code>\"ntp\"</code> (neural tangent parameterisation).      See <code>_get_param_scalings()</code>      for the specific scalings of these different parameterisations. Defaults     to <code>\"sp\"</code>.</li> <li><code>activity_decay</code>: \\(\\ell^2\\) regulariser for the activities.</li> <li><code>gamma</code>: Optional scaling factor for the output layer. If provided, the output      layer scaling is multiplied by <code>1/gamma</code>. Defaults to <code>None</code> (no additional scaling).</li> <li><code>epsilon</code>: Small regularization value added to the diagonal of the Hessian matrix      before inversion to improve numerical stability. Defaults to <code>0.</code>.</li> <li><code>hessian</code>: Optional Hessian matrix to use instead of computing it      analytically using <code>jpc.compute_linear_activity_hessian()</code>.      Defaults to <code>None</code>.</li> </ul> <p>Returns:</p> <p>List of theoretical activities for each layer.</p>"},{"location":"api/Training/","title":"Training","text":"<p>JPC provides 2 single convenience functions to update the parameters of any  PC-compatible model with PC:</p> <ul> <li>jpc.make_pc_step() to  perform an update using standard PC, and</li> <li>jpc.make_hpc_step()  to use hybrid PC (Tscshantz et al., 2023).</li> </ul>"},{"location":"api/Training/#jpc.make_pc_step","title":"<code>jpc.make_pc_step(model: PyTree[typing.Callable], optim: optax._src.base.GradientTransformation | optax._src.base.GradientTransformationExtraArgs, opt_state: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, typing.Iterable[ForwardRef(ArrayTree)], typing.Mapping[typing.Any, ForwardRef(ArrayTree)]], output: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex], *, input: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex, NoneType] = None, loss_id: str = 'mse', param_type: str = 'sp', ode_solver: AbstractSolver = Heun(), max_t1: int = 20, dt: jaxtyping.Shaped[Array, ''] | int = None, stepsize_controller: AbstractStepSizeController = PIDController(rtol=0.001, atol=0.001), skip_model: typing.Optional[jaxtyping.PyTree[typing.Callable]] = None, weight_decay: Shaped[Array, ''] = 0.0, spectral_penalty: Shaped[Array, ''] = 0.0, activity_decay: Shaped[Array, ''] = 0.0, key: typing.Union[jaxtyping.Key[Array, ''], jaxtyping.UInt32[Array, 2], NoneType] = None, layer_sizes: typing.Optional[jaxtyping.PyTree[int]] = None, batch_size: typing.Optional[int] = None, sigma: Shaped[Array, ''] = 0.05, record_activities: bool = False, record_energies: bool = False, record_every: int = None, activity_norms: bool = False, param_norms: bool = False, grad_norms: bool = False, calculate_accuracy: bool = False) -&gt; typing.Dict</code>","text":"<p>Performs one model parameter update with predictive coding.</p> <p>Main arguments:</p> <ul> <li><code>model</code>: List of callable model (e.g. neural network) layers.</li> <li><code>optim</code>: Optax optimiser, e.g. <code>optax.sgd()</code>.</li> <li><code>opt_state</code>: State of Optax optimiser.</li> <li><code>output</code>: Observation or target of the generative model.</li> </ul> <p>Note</p> <p><code>key</code>, <code>layer_sizes</code> and <code>batch_size</code> must be passed if <code>input = None</code>,  since unsupervised training will be assumed and activities need to be  initialised randomly.</p> <p>Other arguments:</p> <ul> <li><code>input</code>: Optional prior of the generative model.</li> <li><code>loss_id</code>: Loss function to use at the output layer. Options are mean squared      error <code>\"mse\"</code> (default) or cross-entropy <code>\"ce\"</code>.</li> <li><code>param_type</code>: Determines the parameterisation. Options are <code>\"sp\"</code>      (standard parameterisation), <code>\"mupc\"</code> (\u03bcPC),      or <code>\"ntp\"</code> (neural tangent parameterisation).      See <code>_get_param_scalings()</code>      for the specific scalings of these different parameterisations. Defaults     to <code>\"sp\"</code>.</li> <li><code>ode_solver</code>: diffrax ODE solver      to be used. Default is <code>Heun</code>,      a 2nd order explicit Runge--Kutta method.</li> <li><code>max_t1</code>: Maximum end of integration region (20 by default).</li> <li><code>dt</code>: Integration step size. Defaults to <code>None</code> since the default     <code>stepsize_controller</code> will automatically determine it.</li> <li><code>stepsize_controller</code>: diffrax controller      for step size integration. Defaults to <code>PIDController</code>.      Note that the relative and absolute tolerances of the controller will      also determine the steady state to terminate the solver.</li> <li><code>skip_model</code>: Optional list of callable skip connection functions.</li> <li><code>weight_decay</code>: Weight decay for the weights (0 by default).</li> <li><code>spectral_penalty</code>: Weight spectral penalty of the form      \\(||\\mathbf{I} - \\mathbf{W}_\\ell^T \\mathbf{W}_\\ell||^2\\) (0 by default).</li> <li><code>activity_decay</code>: Activity decay for the activities (0 by default).</li> <li><code>key</code>: <code>jax.random.PRNGKey</code> for random initialisation of activities.</li> <li><code>layer_sizes</code>: Dimension of all layers (input, hidden and output).</li> <li><code>batch_size</code>: Dimension of data batch for activity initialisation.</li> <li><code>sigma</code>: Standard deviation for Gaussian to sample activities from for     random initialisation. Defaults to 5e-2.</li> <li><code>record_activities</code>: If <code>True</code>, returns activities at every inference     iteration.</li> <li><code>record_energies</code>: If <code>True</code>, returns layer-wise energies at every     inference iteration.</li> <li><code>record_every</code>: int determining the sampling frequency the integration     steps.</li> <li><code>activity_norms</code>: If <code>True</code>, computes \\(\\ell^2\\) norm of the activities.</li> <li><code>param_norms</code>: If <code>True</code>, computes \\(\\ell^2\\) norm of the parameters.</li> <li><code>grad_norms</code>: If <code>True</code>, computes \\(\\ell^2\\) norm of parameter gradients.</li> <li><code>calculate_accuracy</code>: If <code>True</code>, computes the training accuracy.</li> </ul> <p>Returns:</p> <p>Dict including model (and optional skip model) with updated parameters,  updated optimiser state, loss, energies, activities, and optionally other  metrics (see other args above).</p> <p>Raises:</p> <ul> <li><code>ValueError</code> for inconsistent inputs and invalid losses.</li> </ul>"},{"location":"api/Training/#jpc.make_hpc_step","title":"<code>jpc.make_hpc_step(generator: PyTree[typing.Callable], amortiser: PyTree[typing.Callable], optims: typing.Tuple[optax._src.base.GradientTransformationExtraArgs], opt_states: typing.Tuple[typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, typing.Iterable[ForwardRef(ArrayTree)], typing.Mapping[typing.Any, ForwardRef(ArrayTree)]]], output: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex], *, input: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex, NoneType] = None, ode_solver: AbstractSolver = Heun(), max_t1: int = 300, dt: jaxtyping.Shaped[Array, ''] | int = None, stepsize_controller: AbstractStepSizeController = PIDController(rtol=0.001, atol=0.001), record_activities: bool = False, record_energies: bool = False) -&gt; typing.Dict</code>","text":"<p>Performs one update of the parameters of a hybrid predictive coding  network (Tscshantz et al., 2023).</p> Reference <pre><code>@article{tscshantz2023hybrid,\n  title={Hybrid predictive coding: Inferring, fast and slow},\n  author={Tscshantz, Alexander and Millidge, Beren and Seth, Anil K and Buckley, Christopher L},\n  journal={PLoS Computational Biology},\n  volume={19},\n  number={8},\n  pages={e1011280},\n  year={2023},\n  publisher={Public Library of Science San Francisco, CA USA}\n}\n</code></pre> <p>Note</p> <p>The input and output of the generator are the output and input of the amortiser, respectively.</p> <p>Main arguments:</p> <ul> <li><code>generator</code>: List of callable layers for the generative model.</li> <li><code>amortiser</code>: List of callable layers for model amortising the inference     of the <code>generator</code>.</li> <li><code>optims</code>: Optax optimisers (e.g. <code>optax.sgd()</code>), one for each model.</li> <li><code>opt_states</code>: State of Optax optimisers, one for each model.</li> <li><code>output</code>: Observation of the generator, input to the amortiser.</li> <li><code>input</code>: Optional prior of the generator, target for the amortiser.</li> </ul> <p>Other arguments:</p> <ul> <li><code>ode_solver</code>: diffrax ODE solver      to be used. Default is <code>Heun</code>,      a 2nd order explicit Runge--Kutta method.</li> <li><code>max_t1</code>: Maximum end of integration region (300 by default).</li> <li><code>dt</code>: Integration step size. Defaults to <code>None</code> since the default     <code>stepsize_controller</code> will automatically determine it.</li> <li><code>stepsize_controller</code>: diffrax controller      for step size integration. Defaults to <code>PIDController</code>.      Note that the relative and absolute tolerances of the controller will      also determine the steady state to terminate the solver.</li> <li><code>record_activities</code>: If <code>True</code>, returns activities at every inference     iteration.</li> <li><code>record_energies</code>: If <code>True</code>, returns layer-wise energies at every     inference iteration.</li> </ul> <p>Returns:</p> <p>Dict including models with updated parameters, optimiser state for each  model, model activities, last inference step for the generator, MSE losses, and energies.</p>"},{"location":"api/Utils/","title":"Utils","text":"<p>JPC provides several standard utilities for neural network training, including  creation of simple models, losses, and metrics.</p>"},{"location":"api/Utils/#jpc.make_mlp","title":"<code>jpc.make_mlp(key: typing.Union[jaxtyping.Key[Array, ''], jaxtyping.UInt32[Array, 2]], input_dim: int, width: int, depth: int, output_dim: int, act_fn: str, use_bias: bool = False, param_type: str = 'sp') -&gt; PyTree[typing.Callable]</code>","text":"<p>Creates a multi-layer perceptron compatible with predictive coding updates.</p> <p>Note</p> <p>This implementation places the activation function before the linear  transformation, \\(\\mathbf{W}_\\ell \\phi(\\mathbf{z}_{\\ell-1})\\), for  compatibility with the \u03bcPC  scalings when <code>param_type = \"mupc\"</code> in functions including  <code>jpc.init_activities_with_ffwd()</code>,  <code>jpc.update_activities()</code>,  and <code>jpc.update_params()</code>.</p> <p>Main arguments:</p> <ul> <li><code>key</code>: <code>jax.random.PRNGKey</code> for parameter initialisation.</li> <li><code>input_dim</code>: Input dimension.</li> <li><code>width</code>: Network width.</li> <li><code>depth</code>: Network depth.</li> <li><code>output_dim</code>: Output dimension.</li> <li><code>act_fn</code>: Activation function (for all layers except the output).</li> <li><code>use_bias</code>: <code>False</code> by default.</li> <li><code>param_type</code>: Determines the parameterisation. Options are <code>\"sp\"</code>      (standard parameterisation), <code>\"mupc\"</code> (\u03bcPC),      or <code>\"ntp\"</code> (neural tangent parameterisation). See <code>jpc._get_param_scalings()</code>      for the specific scalings of these different parameterisations. Defaults     to <code>\"sp\"</code>.</li> </ul> <p>Returns:</p> <p>List of callable fully connected layers.</p>"},{"location":"api/Utils/#jpc.make_skip_model","title":"<code>jpc.make_skip_model(depth: int) -&gt; PyTree[typing.Callable]</code>","text":"<p>Creates a residual network with one-layer skip connections at every layer  except from the input to the next layer and from the penultimate layer to  the output.</p> <p>This is used for compatibility with the \u03bcPC  parameterisation when <code>param_type = \"mupc\"</code> in functions including  <code>jpc.init_activities_with_ffwd()</code>,  <code>jpc.update_activities()</code>,  and <code>jpc.update_params()</code>.</p>"},{"location":"api/Utils/#jpc.get_act_fn","title":"<code>jpc.get_act_fn(name: str) -&gt; typing.Callable</code>","text":""},{"location":"api/Utils/#jpc.mse_loss","title":"<code>jpc.mse_loss(preds: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex], labels: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex]) -&gt; Shaped[Array, '']</code>","text":""},{"location":"api/Utils/#jpc.cross_entropy_loss","title":"<code>jpc.cross_entropy_loss(logits: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex], labels: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex]) -&gt; Shaped[Array, '']</code>","text":""},{"location":"api/Utils/#jpc.compute_accuracy","title":"<code>jpc.compute_accuracy(truths: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex], preds: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex]) -&gt; Shaped[Array, '']</code>","text":""},{"location":"api/Utils/#jpc.get_t_max","title":"<code>jpc.get_t_max(activities_iters: PyTree[jax.Array]) -&gt; Array</code>","text":""},{"location":"api/Utils/#jpc.compute_infer_energies","title":"<code>jpc.compute_infer_energies(params: typing.Tuple[jaxtyping.PyTree[typing.Callable], typing.Optional[jaxtyping.PyTree[typing.Callable]]], activities_iters: PyTree[jax.Array], t_max: Array, y: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex], *, x: typing.Union[jax.Array, numpy.ndarray, numpy.bool, numpy.number, bool, int, float, complex, NoneType] = None, loss: str = 'mse', param_type: str = 'sp', weight_decay: Shaped[Array, ''] = 0.0, spectral_penalty: Shaped[Array, ''] = 0.0, activity_decay: Shaped[Array, ''] = 0.0) -&gt; PyTree[jaxtyping.Shaped[Array, '']]</code>","text":"<p>Calculates layer energies during predictive coding inference.</p> <p>Main arguments:</p> <ul> <li><code>params</code>: Tuple with callable model layers and optional skip connections.</li> <li><code>activities_iters</code>: Layer-wise activities at every inference iteration.     Note that each set of activities will have 4096 steps as first     dimension by diffrax default.</li> <li><code>t_max</code>: Maximum number of inference iterations to compute energies for.</li> <li><code>y</code>: Observation or target of the generative model.</li> </ul> <p>Other arguments:</p> <ul> <li><code>x</code>: Optional prior of the generative model.</li> <li><code>loss</code>: Loss function to use at the output layer (mean squared error     <code>\"mse\"</code> vs cross-entropy <code>\"ce\"</code>).</li> <li><code>param_type</code>: Determines the parameterisation. Options are <code>\"sp\"</code>,      <code>\"mupc\"</code>, or <code>\"ntp\"</code>.</li> <li><code>weight_decay</code>: Weight decay for the weights.</li> <li><code>spectral_penalty</code>: Spectral penalty for the weights.</li> <li><code>activity_decay</code>: Activity decay for the activities.</li> </ul> <p>Returns:</p> <p>List of layer-wise energies at every inference iteration.</p>"},{"location":"api/Utils/#jpc.compute_activity_norms","title":"<code>jpc.compute_activity_norms(activities: PyTree[jax.Array]) -&gt; Array</code>","text":"<p>Calculates \\(\\ell^2\\) norm of activities at each layer.</p>"},{"location":"api/Utils/#jpc.compute_param_norms","title":"<code>jpc.compute_param_norms(params)</code>","text":"<p>Calculates \\(\\ell^2\\) norm of all model parameters.</p>"},{"location":"examples/bidirectional_pc/","title":"Bidirectional Predictive Coding","text":"<p>This notebook demonstrates how to train a bidirectional predictive coding network (BPC; Olivers et al., 2025) that can both generate and classify MNIST digits. </p> <p>Note: BPC has the same architecture as a hybrid PC network (HPC; Tschantz et al., 2023), but the neural (inference) dynamics are driven by both top-down and bottom-up prediction errors. By contrast, in HPC the amortiser (or bottom-up network) only serves to initialise the inference dynamics.</p> <pre><code>%%capture\n!pip install torch==2.3.1\n!pip install torchvision==0.18.1\n!pip install matplotlib==3.0.0\n</code></pre> <pre><code>import jpc\n\nimport jax\nimport jax.numpy as jnp\nimport equinox as eqx\nimport optax\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\n\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.simplefilter('ignore')  # ignore warnings\n</code></pre>"},{"location":"examples/bidirectional_pc/#hyperparameters","title":"Hyperparameters","text":"<p>We define some global parameters, including the network architecture, learning rate, batch size, etc.</p> <pre><code>SEED = 0\n\nINPUT_DIM = 10\nWIDTH = 256\nDEPTH = 2\nOUTPUT_DIM = 784\nACT_FN = \"relu\"\n\nACTIVITY_LR = 5e-1\nPARAM_LR = 1e-3\nBATCH_SIZE = 64\nTEST_EVERY = 50\nN_TRAIN_ITERS = 300\n</code></pre>"},{"location":"examples/bidirectional_pc/#dataset","title":"Dataset","text":"<p>Some utils to fetch MNIST.</p> <pre><code>def get_mnist_loaders(batch_size):\n    train_data = MNIST(train=True, normalise=True)\n    test_data = MNIST(train=False, normalise=True)\n    train_loader = DataLoader(\n        dataset=train_data,\n        batch_size=batch_size,\n        shuffle=True,\n        drop_last=True\n    )\n    test_loader = DataLoader(\n        dataset=test_data,\n        batch_size=batch_size,\n        shuffle=True,\n        drop_last=True\n    )\n    return train_loader, test_loader\n\n\nclass MNIST(datasets.MNIST):\n    def __init__(self, train, normalise=True, save_dir=\"data\"):\n        if normalise:\n            transform = transforms.Compose(\n                [\n                    transforms.ToTensor(),\n                    transforms.Normalize(\n                        mean=(0.1307), std=(0.3081)\n                    )\n                ]\n            )\n        else:\n            transform = transforms.Compose([transforms.ToTensor()])\n        super().__init__(save_dir, download=True, train=train, transform=transform)\n\n    def __getitem__(self, index):\n        img, label = super().__getitem__(index)\n        img = torch.flatten(img)\n        label = one_hot(label)\n        return img, label\n\n\ndef one_hot(labels, n_classes=10):\n    arr = torch.eye(n_classes)\n    return arr[labels]\n\n\ndef plot_mnist_imgs(imgs, labels, n_imgs=10):\n    plt.figure(figsize=(20, 2))\n    for i in range(n_imgs):\n        plt.subplot(1, n_imgs, i + 1)\n        plt.xticks([])\n        plt.yticks([])\n        plt.grid(False)\n        plt.imshow(imgs[i].reshape(28, 28), cmap=plt.cm.binary_r)\n        plt.xlabel(jnp.argmax(labels, axis=1)[i])\n    plt.show()\n</code></pre>"},{"location":"examples/bidirectional_pc/#train-and-test","title":"Train and test","text":"<pre><code>def evaluate(generator, amortiser, test_loader):\n    amort_accs = 0.\n    for _, (img_batch, label_batch) in enumerate(test_loader):\n        img_batch, label_batch = img_batch.numpy(), label_batch.numpy()\n\n        preds = jpc.init_activities_with_ffwd(\n            model=amortiser[::-1],\n            input=img_batch\n        )[-1]\n        amort_accs += jpc.compute_accuracy(label_batch, preds)\n\n    img_preds = jpc.init_activities_with_ffwd(\n        model=generator,\n        input=label_batch\n    )[-1]\n\n    return (\n        amort_accs / len(test_loader),\n        label_batch,\n        img_preds\n    )\n\n\ndef train(\n      seed,\n      input_dim,\n      width,\n      depth,\n      output_dim,\n      act_fn,\n      batch_size,\n      activity_lr,\n      param_lr,\n      test_every,\n      n_train_iters,\n      forward_energy_weight=1.0\n):\n    key = jax.random.PRNGKey(seed)\n    gen_key, amort_key = jax.random.split(key, 2)\n\n    # models (NOTE: input and output are inverted for the amortiser)\n    generator = jpc.make_mlp(\n        gen_key, \n        input_dim=input_dim,\n        width=width,\n        depth=depth,\n        output_dim=output_dim,\n        act_fn=act_fn\n    )\n    amortiser = jpc.make_mlp(\n        amort_key,\n        input_dim=output_dim,\n        width=width,\n        depth=depth,\n        output_dim=input_dim,\n        act_fn=act_fn\n    )[::-1]\n\n    # optimisers\n    activity_optim = optax.sgd(activity_lr)\n    gen_optim = optax.adam(param_lr)\n    amort_optim = optax.adam(param_lr)\n\n    gen_opt_state = gen_optim.init(eqx.filter(generator, eqx.is_array))\n    amort_opt_state = amort_optim.init(eqx.filter(amortiser, eqx.is_array))\n\n    # data\n    train_loader, test_loader = get_mnist_loaders(batch_size)\n\n    for iter, (img_batch, label_batch) in enumerate(train_loader):\n        img_batch, label_batch = img_batch.numpy(), label_batch.numpy()\n\n        # discriminative loss &amp; initialisation\n        activities = jpc.init_activities_with_ffwd(\n            model=amortiser[::-1],\n            input=img_batch\n        )\n        amort_loss = jpc.mse_loss(activities[-1], label_batch)\n        activity_opt_state = activity_optim.init(activities)\n\n        # generative loss\n        gen_activities = jpc.init_activities_with_ffwd(\n            model=generator,\n            input=label_batch\n        )\n        gen_loss = jpc.mse_loss(gen_activities[-1], img_batch)\n\n        # inference\n        for _ in range(depth - 1):\n            activity_update_result = jpc.update_bpc_activities(\n                top_down_model=generator,\n                bottom_up_model=amortiser,\n                activities=activities,\n                optim=activity_optim,\n                opt_state=activity_opt_state,\n                output=img_batch,\n                input=label_batch,\n                forward_energy_weight=forward_energy_weight\n            )\n            activities = activity_update_result[\"activities\"]\n            activity_opt_state = activity_update_result[\"opt_state\"]\n\n        # learning\n        param_update_result = jpc.update_bpc_params(\n            top_down_model=generator,\n            bottom_up_model=amortiser,\n            activities=activities,\n            top_down_optim=gen_optim,\n            bottom_up_optim=amort_optim,\n            top_down_opt_state=gen_opt_state,\n            bottom_up_opt_state=amort_opt_state,\n            output=img_batch,\n            input=label_batch,\n            forward_energy_weight=forward_energy_weight\n        )\n        generator, amortiser = param_update_result[\"models\"]\n        gen_opt_state, amort_opt_state  = param_update_result[\"opt_states\"]\n\n        if ((iter+1) % test_every) == 0:\n            amort_acc, label_batch, img_preds = evaluate(\n                generator,\n                amortiser,\n                test_loader\n            )\n            print(\n                f\"Iter {iter+1}, gen loss={gen_loss:4f}, \"\n                f\"amort loss={amort_loss:4f}, \"\n                f\"avg amort test accuracy={amort_acc:4f}\"\n            )\n\n            if (iter+1) &gt;= n_train_iters:\n                break\n\n    plot_mnist_imgs(img_preds, label_batch)\n</code></pre>"},{"location":"examples/bidirectional_pc/#run","title":"Run","text":"<pre><code>train(\n    seed=SEED,\n    input_dim=INPUT_DIM,\n    width=WIDTH,\n    depth=DEPTH,\n    output_dim=OUTPUT_DIM,\n    act_fn=ACT_FN,\n    batch_size=BATCH_SIZE,\n    activity_lr=ACTIVITY_LR,\n    param_lr=PARAM_LR,\n    test_every=TEST_EVERY,\n    n_train_iters=N_TRAIN_ITERS\n)\n</code></pre> <pre><code>Iter 50, gen loss=0.432310, amort loss=0.030540, avg amort test accuracy=69.821716\nIter 100, gen loss=0.388198, amort loss=0.024098, avg amort test accuracy=80.929489\nIter 150, gen loss=0.383346, amort loss=0.026668, avg amort test accuracy=83.203125\nIter 200, gen loss=0.350278, amort loss=0.019336, avg amort test accuracy=84.354965\nIter 250, gen loss=0.365861, amort loss=0.020387, avg amort test accuracy=85.236382\nIter 300, gen loss=0.315536, amort loss=0.020807, avg amort test accuracy=85.376602\n</code></pre> <p>To achieve SOTA classification performance, Olivers et al., 2025 showed that one can simply decrease the relative weight of the forward energies (tied to generation), or equivalently, upweight the backward energies (tied to classification). We reproduce this below by downscaling the forward energies by a factor of \\(\\alpha_f = 1e^{-3}\\).</p> <pre><code>train(\n    seed=SEED,\n    input_dim=INPUT_DIM,\n    width=WIDTH,\n    depth=DEPTH,\n    output_dim=OUTPUT_DIM,\n    act_fn=ACT_FN,\n    batch_size=BATCH_SIZE,\n    activity_lr=ACTIVITY_LR,\n    param_lr=PARAM_LR,\n    test_every=TEST_EVERY,\n    n_train_iters=N_TRAIN_ITERS,\n    forward_energy_weight=1e-3\n)\n</code></pre> <pre><code>Iter 50, gen loss=0.388206, amort loss=0.017003, avg amort test accuracy=90.174278\nIter 100, gen loss=0.408145, amort loss=0.013181, avg amort test accuracy=91.796875\nIter 150, gen loss=0.415482, amort loss=0.009785, avg amort test accuracy=93.800079\nIter 200, gen loss=0.401330, amort loss=0.010149, avg amort test accuracy=93.970352\nIter 250, gen loss=0.383768, amort loss=0.010281, avg amort test accuracy=93.760017\nIter 300, gen loss=0.383873, amort loss=0.011300, avg amort test accuracy=94.020432\n</code></pre> <p></p>"},{"location":"examples/discriminative_pc/","title":"Discriminative Predictive Coding","text":"<p>This notebook demonstrates how to train a simple feedforward network with predictive coding (PC) to discriminate or classify MNIST digits.</p> <pre><code>%%capture\n!pip install torch==2.3.1\n!pip install torchvision==0.18.1\n</code></pre> <pre><code>import jpc\n\nimport jax\nimport equinox as eqx\nimport equinox.nn as nn\nimport optax\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\n\nimport warnings\nwarnings.simplefilter('ignore')  # ignore warnings\n</code></pre>"},{"location":"examples/discriminative_pc/#hyperparameters","title":"Hyperparameters","text":"<p>We define some global parameters, including the network architecture, learning rate, batch size, etc.</p> <pre><code>SEED = 0\n\nINPUT_DIM = 784\nWIDTH = 300\nDEPTH = 3\nOUTPUT_DIM = 10\nACT_FN = \"relu\"\n\nLEARNING_RATE = 1e-3\nBATCH_SIZE = 64\nTEST_EVERY = 100\nN_TRAIN_ITERS = 300\n</code></pre>"},{"location":"examples/discriminative_pc/#dataset","title":"Dataset","text":"<p>Some utils to fetch MNIST.</p> <pre><code>def get_mnist_loaders(batch_size):\n    train_data = MNIST(train=True, normalise=True)\n    test_data = MNIST(train=False, normalise=True)\n    train_loader = DataLoader(\n        dataset=train_data,\n        batch_size=batch_size,\n        shuffle=True,\n        drop_last=True\n    )\n    test_loader = DataLoader(\n        dataset=test_data,\n        batch_size=batch_size,\n        shuffle=True,\n        drop_last=True\n    )\n    return train_loader, test_loader\n\n\nclass MNIST(datasets.MNIST):\n    def __init__(self, train, normalise=True, save_dir=\"data\"):\n        if normalise:\n            transform = transforms.Compose(\n                [\n                    transforms.ToTensor(),\n                    transforms.Normalize(\n                        mean=(0.1307), std=(0.3081)\n                    )\n                ]\n            )\n        else:\n            transform = transforms.Compose([transforms.ToTensor()])\n        super().__init__(save_dir, download=True, train=train, transform=transform)\n\n    def __getitem__(self, index):\n        img, label = super().__getitem__(index)\n        img = torch.flatten(img)\n        label = one_hot(label)\n        return img, label\n\n\ndef one_hot(labels, n_classes=10):\n    arr = torch.eye(n_classes)\n    return arr[labels]\n</code></pre>"},{"location":"examples/discriminative_pc/#network","title":"Network","text":"<p>For <code>jpc</code> to work, we need to provide a network with callable layers. This is easy to do with the PyTorch-like <code>nn.Sequential()</code> in equinox. For example, we can define a ReLU MLP with two hidden layers as follows</p> <pre><code>key = jax.random.PRNGKey(SEED)\n_, *subkeys = jax.random.split(key, 4)\nnetwork = [\n    nn.Sequential(\n        [\n            nn.Linear(784, 300, key=subkeys[0]),\n            nn.Lambda(jax.nn.relu)\n        ],\n    ),\n    nn.Sequential(\n        [\n            nn.Linear(300, 300, key=subkeys[1]),\n            nn.Lambda(jax.nn.relu)\n        ],\n    ),\n    nn.Linear(300, 10, key=subkeys[2]),\n]\n</code></pre> <p>You can also use <code>jpc.make_mlp()</code> to define a multi-layer perceptron (MLP) or fully connected network.</p> <pre><code>network = jpc.make_mlp(\n    key,\n    input_dim=INPUT_DIM,\n    width=WIDTH,\n    depth=DEPTH,\n    output_dim=OUTPUT_DIM,\n    act_fn=ACT_FN,\n    use_bias=True\n)\nprint(network)\n</code></pre> <pre><code>[Sequential(\n  layers=(\n    Lambda(fn=Identity()),\n    Linear(\n      weight=f32[300,784],\n      bias=f32[300],\n      in_features=784,\n      out_features=300,\n      use_bias=True\n    )\n  )\n), Sequential(\n  layers=(\n    Lambda(fn=&lt;PjitFunction of &lt;function relu at 0x10cea9c60&gt;&gt;),\n    Linear(\n      weight=f32[300,300],\n      bias=f32[300],\n      in_features=300,\n      out_features=300,\n      use_bias=True\n    )\n  )\n), Sequential(\n  layers=(\n    Lambda(fn=&lt;PjitFunction of &lt;function relu at 0x10cea9c60&gt;&gt;),\n    Linear(\n      weight=f32[10,300],\n      bias=f32[10],\n      in_features=300,\n      out_features=10,\n      use_bias=True\n    )\n  )\n)]\n</code></pre>"},{"location":"examples/discriminative_pc/#train-and-test","title":"Train and test","text":"<p>A PC network can be updated in a single line of code with <code>jpc.make_pc_step()</code>. Similarly, we can use <code>jpc.test_discriminative_pc()</code> to compute the network accuracy. Note that these functions are already \"jitted\" for optimised performance. Below we simply wrap each of these functions in training and test loops, respectively.</p> <pre><code>def evaluate(model, test_loader):\n    avg_test_loss, avg_test_acc = 0, 0\n    for _, (img_batch, label_batch) in enumerate(test_loader):\n        img_batch, label_batch = img_batch.numpy(), label_batch.numpy()\n\n        test_loss, test_acc = jpc.test_discriminative_pc(\n            model=model,\n            input=img_batch,\n            output=label_batch\n        )\n        avg_test_loss += test_loss\n        avg_test_acc += test_acc\n\n    return avg_test_loss / len(test_loader), avg_test_acc / len(test_loader)\n\n\ndef train(\n      model,\n      lr,\n      batch_size,\n      test_every,\n      n_train_iters\n):\n    optim = optax.adam(lr)\n    opt_state = optim.init(\n        (eqx.filter(model, eqx.is_array), None)\n    )\n    train_loader, test_loader = get_mnist_loaders(batch_size)\n\n    for iter, (img_batch, label_batch) in enumerate(train_loader):\n        img_batch, label_batch = img_batch.numpy(), label_batch.numpy()\n\n        result = jpc.make_pc_step(\n            model=model,\n            optim=optim,\n            opt_state=opt_state,\n            output=label_batch,\n            input=img_batch\n        )\n        model, opt_state = result[\"model\"], result[\"opt_state\"]\n        train_loss = result[\"loss\"]\n        if ((iter+1) % test_every) == 0:\n            _, avg_test_acc = evaluate(model, test_loader)\n            print(\n                f\"Train iter {iter+1}, train loss={train_loss:4f}, \"\n                f\"avg test accuracy={avg_test_acc:4f}\"\n            )\n            if (iter+1) &gt;= n_train_iters:\n                break\n</code></pre>"},{"location":"examples/discriminative_pc/#run","title":"Run","text":"<pre><code>train(\n    model=network,\n    lr=LEARNING_RATE,\n    batch_size=BATCH_SIZE,\n    test_every=TEST_EVERY,\n    n_train_iters=N_TRAIN_ITERS\n)\n</code></pre> <pre><code>Train iter 100, train loss=0.007197, avg test accuracy=93.309296\nTrain iter 200, train loss=0.005052, avg test accuracy=95.462738\nTrain iter 300, train loss=0.006984, avg test accuracy=95.903442\n</code></pre>"},{"location":"examples/epc/","title":"Error-reparameterised Predictive Coding (ePC)","text":"<p>This notebook demonstrates how to train PC networks with ePC (Goemaere et al., 2025), a reparameterisation of PC that can allow for faster convergence of the inference dynamics and training of deeper networks than standard PC.</p> <pre><code>%%capture\n!pip install torch==2.3.1\n!pip install torchvision==0.18.1\n</code></pre> <pre><code>import jpc\n\nimport numpy as np\nimport jax.random as jr\nimport equinox as eqx\nimport optax\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\n\nimport warnings\nwarnings.simplefilter('ignore')  # ignore warnings\n</code></pre>"},{"location":"examples/epc/#hyperparameters","title":"Hyperparameters","text":"<p>We define some global parameters, including the network architecture, learning rate, batch size, etc.</p> <pre><code>WIDTH = 128\nDEPTH = 10\nACT_FN = \"relu\"\n\nSTATE_LR = 5e-1  # for either activities (PC) or errors (ePC)\nPARAM_LR = 1e-3\nBATCH_SIZE = 64\nTEST_EVERY = 100\nN_TRAIN_ITERS = 300\n</code></pre>"},{"location":"examples/epc/#dataset","title":"Dataset","text":"<p>Some utils to fetch MNIST.</p> <pre><code>def get_mnist_loaders(batch_size):\n    train_data = MNIST(train=True, normalise=True)\n    test_data = MNIST(train=False, normalise=True)\n    train_loader = DataLoader(\n        dataset=train_data,\n        batch_size=batch_size,\n        shuffle=True,\n        drop_last=True\n    )\n    test_loader = DataLoader(\n        dataset=test_data,\n        batch_size=batch_size,\n        shuffle=True,\n        drop_last=True\n    )\n    return train_loader, test_loader\n\n\nclass MNIST(datasets.MNIST):\n    def __init__(self, train, normalise=True, save_dir=\"data\"):\n        if normalise:\n            transform = transforms.Compose(\n                [\n                    transforms.ToTensor(),\n                    transforms.Normalize(\n                        mean=(0.1307), std=(0.3081)\n                    )\n                ]\n            )\n        else:\n            transform = transforms.Compose([transforms.ToTensor()])\n        super().__init__(save_dir, download=True, train=train, transform=transform)\n\n    def __getitem__(self, index):\n        img, label = super().__getitem__(index)\n        img = torch.flatten(img)\n        label = one_hot(label)\n        return img, label\n\n\ndef one_hot(labels, n_classes=10):\n    arr = torch.eye(n_classes)\n    return arr[labels]\n</code></pre>"},{"location":"examples/epc/#train-and-test","title":"Train and test","text":"<pre><code>def evaluate(model, test_loader):\n    avg_test_acc = 0\n    for _, (img_batch, label_batch) in enumerate(test_loader):\n        img_batch, label_batch = img_batch.numpy(), label_batch.numpy()\n\n        _, test_acc = jpc.test_discriminative_pc(\n            model=model,\n            input=img_batch,\n            output=label_batch\n        )\n        avg_test_acc += test_acc\n\n    return avg_test_acc / len(test_loader)\n\n\ndef train(\n      algo,\n      width,\n      depth,\n      act_fn,\n      state_lr,  \n      param_lr,\n      batch_size,\n      test_every,\n      n_train_iters\n):  \n    key = jr.PRNGKey(5482)\n    model = jpc.make_mlp(\n        key,\n        input_dim=784,\n        width=width,\n        depth=depth,\n        output_dim=10,\n        act_fn=act_fn,\n        use_bias=True\n    )\n    layer_sizes = [784] + [width] * (depth-1) + [10]\n\n    if algo == \"pc\":\n        activity_optim = optax.sgd(state_lr)\n    elif algo == \"epc\":\n        error_optim = optax.sgd(state_lr)\n\n    param_optim = optax.adam(param_lr)\n    param_opt_state = param_optim.init(\n        (eqx.filter(model, eqx.is_array), None)\n    )\n    train_loader, test_loader = get_mnist_loaders(batch_size)\n\n    for iter, (img_batch, label_batch) in enumerate(train_loader):\n        img_batch, label_batch = img_batch.numpy(), label_batch.numpy()\n\n        # initialise activities or errors\n        activities = jpc.init_activities_with_ffwd(\n            model=model,\n            input=img_batch\n        )\n        train_loss = jpc.mse_loss(activities[-1], label_batch)\n\n        if algo == \"pc\":\n            activity_opt_state = activity_optim.init(activities)\n\n        elif algo == \"epc\":\n            errors = jpc.init_epc_errors(\n                layer_sizes=layer_sizes,\n                batch_size=batch_size\n            )\n            error_opt_state = error_optim.init(errors)\n\n        # inference\n        for _ in range(len(model)):\n            if algo == \"pc\":\n                activity_update_result = jpc.update_pc_activities(\n                    params=(model, None),\n                    activities=activities,\n                    optim=activity_optim,\n                    opt_state=activity_opt_state,\n                    output=label_batch,\n                    input=img_batch\n                )\n                activities = activity_update_result[\"activities\"]\n                activity_opt_state = activity_update_result[\"opt_state\"]\n\n            elif algo == \"epc\":\n                error_update_result = jpc.update_epc_errors(\n                    params=(model, None),\n                    errors=errors,\n                    optim=error_optim,\n                    opt_state=error_opt_state,\n                    output=label_batch,\n                    input=img_batch\n                )\n                errors = error_update_result[\"errors\"]\n                error_opt_state = error_update_result[\"opt_state\"]\n\n        # learning\n        if algo == \"pc\":\n            param_update_result = jpc.update_pc_params(\n                params=(model, None),\n                activities=activities,\n                optim=param_optim,\n                opt_state=param_opt_state,\n                output=label_batch,\n                input=img_batch\n            )\n        elif algo == \"epc\":\n            param_update_result = jpc.update_epc_params(\n                params=(model, None),\n                errors=errors,\n                optim=param_optim,\n                opt_state=param_opt_state,\n                output=label_batch,\n                input=img_batch\n            )\n\n        model = param_update_result[\"model\"]\n        param_opt_state = param_update_result[\"opt_state\"]\n\n        if np.isinf(train_loss) or np.isnan(train_loss):\n            print(\n                f\"Stopping training because of divergence, train loss={train_loss}\"\n            )\n            break\n\n        if ((iter+1) % test_every) == 0:\n            avg_test_acc = evaluate(model=model, test_loader=test_loader)\n            print(\n                f\"Train iter {iter+1}, train loss={train_loss:4f}, \"\n                f\"avg test accuracy={avg_test_acc:4f}\"\n            )\n            if (iter+1) &gt;= n_train_iters:\n                break\n</code></pre>"},{"location":"examples/epc/#run","title":"Run","text":"<p>The script below should take ~30s to run on a CPU.</p> <pre><code>train(\n    algo=\"epc\",\n    width=WIDTH,\n    depth=DEPTH,\n    act_fn=ACT_FN,\n    state_lr=STATE_LR,\n    param_lr=PARAM_LR,\n    batch_size=BATCH_SIZE,\n    test_every=TEST_EVERY,\n    n_train_iters=N_TRAIN_ITERS\n)\n</code></pre> <pre><code>Train iter 100, train loss=0.014619, avg test accuracy=82.291664\nTrain iter 200, train loss=0.006187, avg test accuracy=91.185898\nTrain iter 300, train loss=0.007181, avg test accuracy=91.606567\n</code></pre> <p>For comparison, try to change to standard pc with <code>algo = \"pc\"</code>.</p>"},{"location":"examples/hybrid_pc/","title":"Hybrid Predictive Coding","text":"<p>This notebook demonstrates how to train a hybrid predictive coding network (Tschantz et al., 2023) that can both generate and classify MNIST digits.</p> <pre><code>%%capture\n!pip install torch==2.3.1\n!pip install torchvision==0.18.1\n!pip install matplotlib==3.0.0\n</code></pre> <pre><code>import jpc\n\nimport jax\nimport jax.numpy as jnp\nimport equinox as eqx\nimport equinox.nn as nn\nimport optax\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\n\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.simplefilter('ignore')  # ignore warnings\n</code></pre>"},{"location":"examples/hybrid_pc/#hyperparameters","title":"Hyperparameters","text":"<p>We define some global parameters, including the network architecture, learning rate, batch size, etc.</p> <pre><code>SEED = 0\n\nINPUT_DIM = 10\nWIDTH = 300\nDEPTH = 3\nOUTPUT_DIM = 784\nACT_FN = \"relu\"\n\nLEARNING_RATE = 1e-3\nBATCH_SIZE = 64\nMAX_T1 = 50\nTEST_EVERY = 100\nN_TRAIN_ITERS = 300\n</code></pre>"},{"location":"examples/hybrid_pc/#dataset","title":"Dataset","text":"<p>Some utils to fetch MNIST.</p> <pre><code>def get_mnist_loaders(batch_size):\n    train_data = MNIST(train=True, normalise=True)\n    test_data = MNIST(train=False, normalise=True)\n    train_loader = DataLoader(\n        dataset=train_data,\n        batch_size=batch_size,\n        shuffle=True,\n        drop_last=True\n    )\n    test_loader = DataLoader(\n        dataset=test_data,\n        batch_size=batch_size,\n        shuffle=True,\n        drop_last=True\n    )\n    return train_loader, test_loader\n\n\nclass MNIST(datasets.MNIST):\n    def __init__(self, train, normalise=True, save_dir=\"data\"):\n        if normalise:\n            transform = transforms.Compose(\n                [\n                    transforms.ToTensor(),\n                    transforms.Normalize(\n                        mean=(0.1307), std=(0.3081)\n                    )\n                ]\n            )\n        else:\n            transform = transforms.Compose([transforms.ToTensor()])\n        super().__init__(save_dir, download=True, train=train, transform=transform)\n\n    def __getitem__(self, index):\n        img, label = super().__getitem__(index)\n        img = torch.flatten(img)\n        label = one_hot(label)\n        return img, label\n\n\ndef one_hot(labels, n_classes=10):\n    arr = torch.eye(n_classes)\n    return arr[labels]\n\n\ndef plot_mnist_imgs(imgs, labels, n_imgs=10):\n    plt.figure(figsize=(20, 2))\n    for i in range(n_imgs):\n        plt.subplot(1, n_imgs, i + 1)\n        plt.xticks([])\n        plt.yticks([])\n        plt.grid(False)\n        plt.imshow(imgs[i].reshape(28, 28), cmap=plt.cm.binary_r)\n        plt.xlabel(jnp.argmax(labels, axis=1)[i])\n    plt.show()\n</code></pre>"},{"location":"examples/hybrid_pc/#train-and-test","title":"Train and test","text":"<p>Similar to a standard PC network, a hybrid model can be trained in a single line of code with <code>jpc.make_hpc_step()</code>. Similarly, we can use <code>jpc.test_hpc()</code> to compute different test metrics. Note that these functions are already \"jitted\" for optimised performance. Below we simply wrap each of these functions in training and test loops, respectively.</p> <pre><code>def evaluate(\n      key,\n      layer_sizes,\n      batch_size,\n      generator,\n      amortiser,\n      test_loader\n):\n    amort_accs, hpc_accs, gen_accs = 0, 0, 0\n    for _, (img_batch, label_batch) in enumerate(test_loader):\n        img_batch, label_batch = img_batch.numpy(), label_batch.numpy()\n\n        amort_acc, hpc_acc, gen_acc, img_preds = jpc.test_hpc(\n            key=key,\n            layer_sizes=layer_sizes,\n            batch_size=batch_size,\n            generator=generator,\n            amortiser=amortiser,\n            input=label_batch,\n            output=img_batch\n        )\n        amort_accs += amort_acc\n        hpc_accs += hpc_acc\n        gen_accs += gen_acc\n\n    return (\n        amort_accs / len(test_loader),\n        hpc_accs / len(test_loader),\n        gen_accs / len(test_loader),\n        label_batch,\n        img_preds\n    )\n\n\ndef train(\n      seed,\n      input_dim,\n      width,\n      depth,\n      output_dim,\n      act_fn,\n      batch_size,\n      lr,\n      max_t1,\n      test_every,\n      n_train_iters\n):\n    key = jax.random.PRNGKey(seed)\n    key, *subkey = jax.random.split(key, 3)\n\n    layer_sizes = [input_dim] + [width]*(depth-1) + [output_dim]\n    generator = jpc.make_mlp(\n        subkey[0], \n        input_dim=input_dim,\n        width=width,\n        depth=depth,\n        output_dim=output_dim,\n        act_fn=act_fn\n    )\n    # NOTE: input and output are inverted for the amortiser\n    amortiser = jpc.make_mlp(\n        subkey[1],\n        input_dim=output_dim,\n        width=width,\n        depth=depth,\n        output_dim=input_dim,\n        act_fn=act_fn\n    )\n\n    gen_optim = optax.adam(lr)\n    amort_optim = optax.adam(lr)\n    optims = [gen_optim, amort_optim]\n\n    gen_opt_state = gen_optim.init(\n        (eqx.filter(generator, eqx.is_array), None)\n    )\n    amort_opt_state = amort_optim.init(eqx.filter(amortiser, eqx.is_array))\n    opt_states = [gen_opt_state, amort_opt_state]\n\n    train_loader, test_loader = get_mnist_loaders(batch_size)\n    for iter, (img_batch, label_batch) in enumerate(train_loader):\n        img_batch, label_batch = img_batch.numpy(), label_batch.numpy()\n\n        result = jpc.make_hpc_step(\n            generator=generator,\n            amortiser=amortiser,\n            optims=optims,\n            opt_states=opt_states,\n            input=label_batch,\n            output=img_batch,\n            max_t1=max_t1\n        )\n        generator, amortiser = result[\"generator\"], result[\"amortiser\"]\n        gen_loss, amort_loss = result[\"losses\"]\n        if ((iter+1) % test_every) == 0:\n            amort_acc, hpc_acc, gen_acc, label_batch, img_preds = evaluate(\n                key,\n                layer_sizes,\n                batch_size,\n                generator,\n                amortiser,\n                test_loader\n            )\n            print(\n                f\"Iter {iter+1}, gen loss={gen_loss:4f}, \"\n                f\"amort loss={amort_loss:4f}, \"\n                f\"avg amort test accuracy={amort_acc:4f}, \"\n                f\"avg hpc test accuracy={hpc_acc:4f}, \"\n                f\"avg gen test accuracy={gen_acc:4f}, \"\n            )\n            if (iter+1) &gt;= n_train_iters:\n                break\n\n    plot_mnist_imgs(img_preds, label_batch)\n    return amortiser, generator\n</code></pre>"},{"location":"examples/hybrid_pc/#run","title":"Run","text":"<pre><code>network = train(\n    seed=SEED,\n    input_dim=INPUT_DIM,\n    width=WIDTH,\n    depth=DEPTH,\n    output_dim=OUTPUT_DIM,\n    act_fn=ACT_FN,\n    batch_size=BATCH_SIZE,\n    lr=LEARNING_RATE,\n    max_t1=MAX_T1,\n    test_every=TEST_EVERY,\n    n_train_iters=N_TRAIN_ITERS\n)\n</code></pre> <pre><code>Iter 100, gen loss=0.617566, amort loss=0.052470, avg amort test accuracy=74.719551, avg hpc test accuracy=81.500404, avg gen test accuracy=81.390221, \nIter 200, gen loss=0.573021, amort loss=0.052784, avg amort test accuracy=80.669067, avg hpc test accuracy=82.341743, avg gen test accuracy=82.331734, \nIter 300, gen loss=0.531935, amort loss=0.041603, avg amort test accuracy=82.121391, avg hpc test accuracy=83.022835, avg gen test accuracy=83.203125,\n</code></pre>"},{"location":"examples/jpc_from_scratch/","title":"\u2699\ufe0f JPC from Scratch","text":"<p>This notebook is a walk-through of how Predictive Coding (PC) is implemented in JPC. It was developed as a lab session of an MSc course at the University of Sussex. We are going to implement all the core functionalities of JPC from scratch, building towards the training of a simple feedforward network to classify MNIST.</p> <p>If you're not familiar with JAX, have a look at their docs, but we will explain all the necessary concepts below. JAX is basically numpy for GPUs and other hardware accelerators. We will also use: (i) Equinox, which allows you to define neural nets with PyTorch-like syntax; and (ii) Optax, which provides a range of common machine learning optimisers such as gradient descent and Adam.</p>"},{"location":"examples/jpc_from_scratch/#installations-imports","title":"Installations &amp; imports","text":"<pre><code>%%capture\n!pip install torch==2.3.1\n!pip install torchvision==0.18.1\n</code></pre> <pre><code>import jax.random as jr\nimport jax.numpy as jnp\nfrom jax import vmap, grad\nfrom jax.tree_util import tree_map\n\nimport equinox as eqx\nimport equinox.nn as nn\nfrom equinox import filter_grad\nimport optax\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\n\nimport warnings\nwarnings.simplefilter(\"ignore\")\n</code></pre>"},{"location":"examples/jpc_from_scratch/#hyperparameters","title":"Hyperparameters","text":"<p>We define some global parameters related to the data, network, optimisers, etc.</p> <pre><code>SEED = 827\n\nINPUT_DIM, OUTPUT_DIM = 28*28, 10\nNETWORK_WIDTH = 300\n\nACTIVITY_LR = 1e-1\nINFERENCE_STEPS = 20\n\nPARAM_LR = 1e-3\nBATCH_SIZE = 64\n\nTEST_EVERY = 50\nN_TRAIN_ITERS = 500\n</code></pre>"},{"location":"examples/jpc_from_scratch/#dataset","title":"Dataset","text":"<p>Some utils to fetch MNIST.</p> <pre><code>def get_mnist_loaders(batch_size):\n    train_data = MNIST(train=True, normalise=True)\n    test_data = MNIST(train=False, normalise=True)\n    train_loader = DataLoader(\n        dataset=train_data,\n        batch_size=batch_size,\n        shuffle=True,\n        drop_last=True\n    )\n    test_loader = DataLoader(\n        dataset=test_data,\n        batch_size=batch_size,\n        shuffle=True,\n        drop_last=True\n    )\n    return train_loader, test_loader\n\n\nclass MNIST(datasets.MNIST):\n    def __init__(self, train, normalise=True, save_dir=\"data\"):\n        if normalise:\n            transform = transforms.Compose(\n                [\n                    transforms.ToTensor(),\n                    transforms.Normalize(\n                        mean=(0.1307), std=(0.3081)\n                    )\n                ]\n            )\n        else:\n            transform = transforms.Compose([transforms.ToTensor()])\n        super().__init__(save_dir, download=True, train=train, transform=transform)\n\n    def __getitem__(self, index):\n        img, label = super().__getitem__(index)\n        img = torch.flatten(img)\n        label = one_hot(label)\n        return img, label\n\n\ndef one_hot(labels, n_classes=10):\n    arr = torch.eye(n_classes)\n    return arr[labels]\n</code></pre>"},{"location":"examples/jpc_from_scratch/#pc-energy","title":"PC energy","text":"<p>First, recall that PC can be derived as a variational inference algorithm under certain assumptions. In particular, if we assume  * a dirac delta (point mass) posterior and * a hierarchical Gaussian generative model,</p> <p>we get the standard PC energy</p> <p>\\begin{equation}     \\mathcal{F} = \\frac{1}{2N}\\sum_{i=1}^{N} \\sum_{\\ell=1}^L ||\\mathbf{z}{\\ell, i} - f\\ell(W_\\ell \\mathbf{z}{\\ell-1, i} + \\mathbf{b}\\ell)||^2_2 \\end{equation} which is just a sum of squared prediction errors at each network layer. Here we are being a little bit more precise than in the lecture, including multiple (\\(N\\)) data points and biases \\(\\mathbf{b}_\\ell\\).</p> <p>\ud83e\udd14 Food for thought: Think about how the form of this energy could change depending on other assumptions we make about the generative model. See, for example, Learning on Arbitrary Graph Topologies via Predictive Coding  by Salvatori et al. (2022).</p> <p>Let's start by implementing this energy below. The function simply takes the model (with all the parameters), some initialised activities, and some input and output. Given these, it simply sums the prediction error at each layer.</p> <p>NOTE: below we use <code>vmap</code>, one of the core JAX transforms that allows you to vectorise operations, in this case for multiple data points or over a batch. See their docs for more details.</p> <pre><code>def pc_energy_fn(model, activities, input, output):\n    batch_size = output.shape[0]\n    n_activity_layers = len(activities) - 1\n    n_layers = len(model) - 1\n\n    eL = output - vmap(model[-1])(activities[-2])\n    energies = [jnp.sum(eL ** 2)]\n    for act_l, net_l in zip(\n            range(1, n_activity_layers),\n            range(1, n_layers)\n    ):\n        err = activities[act_l] - vmap(model[net_l])(activities[act_l - 1])\n        energies.append(jnp.sum(err ** 2))\n\n    e1 = activities[0] - vmap(model[0])(input)\n    energies.append(jnp.sum(e1 ** 2))\n\n    return jnp.sum(jnp.array(energies)) / batch_size\n</code></pre> <p>Now let's test it. To do so, we first need a model. Below we use Equinox to create a simple feedforward network with 2 hidden layers and Tanh activations. Note that we split the model into different parts with <code>nn.Sequential</code> to define the activities which PC will optimise over (during inference, more on this below).</p> <p>\u2753 Question: Think about other ways in which we could split the layers, for example by separating the non-linearities. Can you think of potential issues with this?</p> <pre><code># jax uses explicit random number generators (see https://jax.readthedocs.io/en/latest/random-numbers.html)\nkey = jr.PRNGKey(SEED)\nsubkeys = jr.split(key, 3)\n\nmodel = [\n    nn.Sequential(\n        [\n            nn.Linear(INPUT_DIM, NETWORK_WIDTH, key=subkeys[0]),\n            nn.Lambda(jnp.tanh)\n        ],\n    ),\n    nn.Sequential(\n        [\n            nn.Linear(NETWORK_WIDTH, NETWORK_WIDTH, key=subkeys[1]),\n            nn.Lambda(jnp.tanh)\n        ],\n    ),\n    nn.Linear(NETWORK_WIDTH, OUTPUT_DIM, key=subkeys[2]),\n]\nmodel\n</code></pre> <pre><code>[Sequential(\n   layers=(\n     Linear(\n       weight=f32[300,784],\n       bias=f32[300],\n       in_features=784,\n       out_features=300,\n       use_bias=True\n     ),\n     Lambda(fn=&lt;wrapped function tanh&gt;)\n   )\n ),\n Sequential(\n   layers=(\n     Linear(\n       weight=f32[300,300],\n       bias=f32[300],\n       in_features=300,\n       out_features=300,\n       use_bias=True\n     ),\n     Lambda(fn=&lt;wrapped function tanh&gt;)\n   )\n ),\n Linear(\n   weight=f32[10,300],\n   bias=f32[10],\n   in_features=300,\n   out_features=10,\n   use_bias=True\n )]\n</code></pre> <p>The last thing we need is to initialise the activities. For this, we will use a feedforward pass as often done in practice.</p> <p>\u2753 Question: Can you think of other ways of initialising the activities?</p> <pre><code>def init_activities_with_ffwd(model, input):\n    activities = [vmap(model[0])(input)]\n    for l in range(1, len(model)):\n        layer_output = vmap(model[l])(activities[l - 1])\n        activities.append(layer_output)\n\n    return activities\n</code></pre> <p>Let's test it on an MNIST sample.</p> <pre><code># get a data sample\ntrain_loader, test_loader = get_mnist_loaders(BATCH_SIZE)\nimg_batch, label_batch = next(iter(train_loader))\n\n# we need to turn the torch.Tensor data into numpy arrays for jax\nimg_batch, label_batch = img_batch.numpy(), label_batch.numpy()\n\n# let's check our initialised activities\nactivities = init_activities_with_ffwd(model, img_batch)\nfor i, a in enumerate(activities):\n    print(f\"activity z at layer {i+1}: {a.shape}\")\n</code></pre> <pre><code>activity z at layer 1: (64, 300)\nactivity z at layer 2: (64, 300)\nactivity z at layer 3: (64, 10)\n</code></pre> <p>Ok so now we have everything to test our PC energy function: model, activities, and some data.</p> <pre><code>pc_energy_fn(\n    model=model,\n    activities=activities,\n    input=img_batch,\n    output=label_batch\n)\n</code></pre> <pre><code>Array(1.2335204, dtype=float32)\n</code></pre> <p>And it works!</p>"},{"location":"examples/jpc_from_scratch/#energy-gradients","title":"Energy gradients","text":"<p>How do we minimise the PC energy we defined above (Eq. 1)? Recall from the lecture that we do this in two phases: first with respect to the activities (inference) and then with respect to the weights (learning).</p> \\[\\begin{equation}     \\textit{Inference:} - \\frac{\\partial \\mathcal{F}}{\\partial \\mathbf{z}_\\ell} \\end{equation}\\] \\[\\begin{equation}     \\textit{Learning:} - \\frac{\\partial \\mathcal{F}}{\\partial W_\\ell} \\end{equation}\\] <p>So we just need to take these gradients of the energy. We are going to use autodiff, which JAX embeds by design (see the docs). If you're familiar with PyTorch, you are probably used to <code>loss.backward()</code> for this, which might feel obstruse at times. JAX, on the other hand, is a fully functional (as opposed to object-oriented) language whose syntax is very close to the maths as you can see below.</p> <pre><code># note how close this code is to the maths\n# this can be read as \"take the gradient of the energy...\n# ...with the respect to the 2nd argument (the activities)\n\ndef compute_activity_grad(model, activities, input, output):\n    return grad(pc_energy_fn, argnums=1)(\n        model,\n        activities,\n        input,\n        output\n    )\n</code></pre> <p>Let's test this out.</p> <pre><code>dFdzs = compute_activity_grad(\n    model=model, \n    activities=activities, \n    input=img_batch, \n    output=label_batch\n)\nfor i, dFdz in enumerate(dFdzs):\n    print(f\"activity gradient dFdz shape at layer {i+1}: {dFdz.shape}\")\n</code></pre> <pre><code>activity gradient dFdz shape at layer 1: (64, 300)\nactivity gradient dFdz shape at layer 2: (64, 300)\nactivity gradient dFdz shape at layer 3: (64, 10)\n</code></pre> <p>Now we do the same and take the gradient of the energy with respect to the parameters.</p> <p>Technical note: below we use Equinox's convenience function <code>filter_grad</code> rather than JAX's native <code>grad</code>. This is because things like activation functions do not have parameters and so we do not want to differentiate them. <code>filter_grad</code> automatically filters these non-differentiable objects for us, while <code>grad</code> alone would throw an error.</p> <pre><code># note that, compared to the previous function,...\n# ...we just change the argument with respect to which...\n# ...we are differentiating (the first, or in this case the model)\n\ndef compute_param_grad(model, activities, input, output):\n    return filter_grad(pc_energy_fn)(\n        model,\n        activities,\n        input,\n        output\n    )\n</code></pre> <p>And let's test it.</p> <pre><code>param_grads = compute_param_grad(\n    model=model, \n    activities=activities, \n    input=img_batch, \n    output=label_batch\n)\n</code></pre>"},{"location":"examples/jpc_from_scratch/#updates","title":"Updates","text":"<p>Before putting everything together, let's wrap our gradients into update functions. This will also allow us to use JAX's <code>jit</code> primitive, which essentially compiles your code the first time it's executed so that it can be run more efficiently the next time (see the docs for more details).</p> <p>These functions take an (Optax) optimiser such as gradient descent in addition to the previous arguments (model, activities and data).</p> <pre><code>@eqx.filter_jit\ndef update_activities(model, activities, optim, opt_state, input, output):\n    activity_grads = compute_activity_grad(\n        model=model,\n        activities=activities,\n        input=input,\n        output=output\n    )\n    activity_updates, activity_opt_state = optim.update(\n        updates=activity_grads,\n        state=opt_state,\n        params=activities\n    )\n    activities = eqx.apply_updates(\n        model=activities,\n        updates=activity_updates\n    )\n    return activities, optim, opt_state\n\n\n# note that the only difference with the above function is...\n# ...the variable we are updating (parameters vs activities)\n@eqx.filter_jit\ndef update_params(model, activities, optim, opt_state, input, output):\n    param_grads = compute_param_grad(\n        model=model,\n        activities=activities,\n        input=input,\n        output=output\n    )\n    param_updates, param_opt_state = optim.update(\n        updates=param_grads,\n        state=opt_state,\n        params=model\n    )\n    model = eqx.apply_updates(\n        model=model,\n        updates=param_updates\n    )\n    return model, optim, opt_state\n</code></pre>"},{"location":"examples/jpc_from_scratch/#putting-everything-together-training-and-testing","title":"Putting everything together: Training and testing","text":"<p>Now that we have our activity and parameter updates, we just need to wrap them in a training and test loop.</p> <pre><code># note: the test accuracy computation below could be sped up...\n# ...with jit in a separate function\n\ndef evaluate(model, test_loader):\n    avg_test_acc = 0\n    for test_iter, (img_batch, label_batch) in enumerate(test_loader):\n        img_batch, label_batch = img_batch.numpy(), label_batch.numpy()\n\n        preds = init_activities_with_ffwd(model, img_batch)[-1]\n        test_acc = jnp.mean(\n            jnp.argmax(label_batch, axis=1) == jnp.argmax(preds, axis=1)\n        ) * 100\n        avg_test_acc += test_acc\n\n    return avg_test_acc / len(test_loader)\n\n\ndef train(\n      model,\n      activity_lr,\n      inference_steps,\n      param_lr,\n      batch_size,\n      test_every,\n      n_train_iters\n):\n    # define optimisers for activities and parameters\n    activity_optim = optax.sgd(activity_lr)\n    param_optim = optax.adam(param_lr)\n    param_opt_state = param_optim.init(eqx.filter(model, eqx.is_array))\n\n    train_loader, test_loader = get_mnist_loaders(batch_size)\n    for train_iter, (img_batch, label_batch) in enumerate(train_loader):\n        img_batch, label_batch = img_batch.numpy(), label_batch.numpy()\n\n        # initialise activities\n        activities = init_activities_with_ffwd(model, img_batch)\n        activity_opt_state = activity_optim.init(activities)\n\n        # calculate loss\n        train_loss = jnp.mean((label_batch - activities[-1])**2)\n\n        # inference\n        for t in range(inference_steps):\n            activities, activity_optim, activity_opt_state = update_activities(\n                model=model, \n                activities=activities, \n                optim=activity_optim, \n                opt_state=activity_opt_state, \n                input=img_batch, \n                output=label_batch\n            )\n\n        # learning\n        model, param_optim, param_opt_state = update_params(\n            model=model,\n            activities=activities,  # note how we use the optimised activities\n            optim=param_optim,\n            opt_state=param_opt_state,\n            input=img_batch,\n            output=label_batch\n        )\n        if ((train_iter+1) % test_every) == 0:\n            avg_test_acc = evaluate(model, test_loader)\n            print(\n                f\"Train iter {train_iter+1}, train loss={train_loss:4f}, \"\n                f\"avg test accuracy={avg_test_acc:4f}\"\n            )\n            if (train_iter+1) &gt;= n_train_iters:\n                break\n</code></pre>"},{"location":"examples/jpc_from_scratch/#run","title":"Run","text":"<p>Let's test our implementation.</p> <pre><code>train(\n    model=model,\n    activity_lr=ACTIVITY_LR,\n    inference_steps=INFERENCE_STEPS,\n    param_lr=PARAM_LR,\n    batch_size=BATCH_SIZE,\n    test_every=TEST_EVERY,\n    n_train_iters=N_TRAIN_ITERS\n)\n</code></pre> <pre><code>Train iter 50, train loss=0.065566, avg test accuracy=72.726364\nTrain iter 100, train loss=0.046521, avg test accuracy=76.292068\nTrain iter 150, train loss=0.042710, avg test accuracy=86.568512\nTrain iter 200, train loss=0.029598, avg test accuracy=89.082535\nTrain iter 250, train loss=0.031486, avg test accuracy=89.222755\nTrain iter 300, train loss=0.016624, avg test accuracy=91.296074\nTrain iter 350, train loss=0.025201, avg test accuracy=92.648239\nTrain iter 400, train loss=0.018597, avg test accuracy=92.968750\nTrain iter 450, train loss=0.019027, avg test accuracy=94.130608\nTrain iter 500, train loss=0.014850, avg test accuracy=93.760017\n</code></pre> <p>\ud83e\udd73 Great, we see that our model is learning! This model was not tuned, and you can probably improve the performance by tweaking some of the hyperparameters (e.g. try a higher number of inference steps).</p> <p>Even if you didn't follow all the implementation details, you should now have at least an idea of how PC works in practice. Indeed, this is basically the core code behind a new PC library our lab will soon release: JPC. Play around with the notebook examples there where you can learn how to train a variety of PC networks.</p>"},{"location":"examples/linear_net_theoretical_energy/","title":"Theoretical PC energy of deep linear networks","text":"<p>This notebook demonstrates how to compute the theoretical PC energy at the inference equilibrium \\(\\mathcal{F}^*\\) when \\(\\nabla_{\\mathbf{z}} \\mathcal{F} = \\mathbf{0}\\) for deep linear networks (Innocenti et al., 2024). For a set of inputs and outputs \\(\\{(\\mathbf{x}_i, \\mathbf{y}_i)\\}_{i=1}^N\\), this is given by </p> \\[\\begin{equation}     \\mathcal{F}^* = \\frac{1}{2N} \\sum_{i=1}^N (\\mathbf{y}_i - \\mathbf{W}_{L:1}\\mathbf{x}_i)^T \\mathbf{S}^{-1}(\\mathbf{y}_i - \\mathbf{W}_{L:1}\\mathbf{x}_i) \\end{equation}\\] <p>where \\(\\mathbf{S} = \\mathbf{I}_{d_y} + \\sum_{\\ell=2}^L (\\mathbf{W}_{L:\\ell})(\\mathbf{W}_{L:\\ell})^T\\) and \\(\\mathbf{W}_{k:\\ell} = \\mathbf{W}_k \\dots \\mathbf{W}_\\ell\\) for \\(\\ell, k \\in 1,\\dots, L\\). This result can be generalised to any linear layer transformation \\(\\mathbf{B}_\\ell\\), e.g. for a ResNet \\(\\mathbf{B}_\\ell = \\mathbf{I} + \\mathbf{W}_\\ell\\) (see Innocenti et al., 2025).</p> <pre><code>%%capture\n!pip install torch==2.3.1\n!pip install torchvision==0.18.1\n!pip install plotly==5.11.0\n!pip install -U kaleido\n</code></pre> <pre><code>import jpc\n\nimport jax\nimport jax.numpy as jnp\nimport equinox as eqx\nimport optax\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\n\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.simplefilter('ignore')  # ignore warnings\n</code></pre>"},{"location":"examples/linear_net_theoretical_energy/#hyperparameters","title":"Hyperparameters","text":"<p>We define some global parameters, including the network architecture, learning rate, batch size, etc.</p> <pre><code>SEED = 0\n\nINPUT_DIM = 784\nWIDTH = 300\nDEPTH = 5\nOUTPUT_DIM = 10\nACT_FN = \"linear\"\n\nLEARNING_RATE = 1e-3\nBATCH_SIZE = 64\nMAX_T1 = 300\nTEST_EVERY = 10\nN_TRAIN_ITERS = 100\n</code></pre>"},{"location":"examples/linear_net_theoretical_energy/#dataset","title":"Dataset","text":"<p>Some utils to fetch MNIST.</p> <pre><code>def get_mnist_loaders(batch_size):\n    train_data = MNIST(train=True, normalise=True)\n    test_data = MNIST(train=False, normalise=True)\n    train_loader = DataLoader(\n        dataset=train_data,\n        batch_size=batch_size,\n        shuffle=True,\n        drop_last=True\n    )\n    test_loader = DataLoader(\n        dataset=test_data,\n        batch_size=batch_size,\n        shuffle=True,\n        drop_last=True\n    )\n    return train_loader, test_loader\n\n\nclass MNIST(datasets.MNIST):\n    def __init__(self, train, normalise=True, save_dir=\"data\"):\n        if normalise:\n            transform = transforms.Compose(\n                [\n                    transforms.ToTensor(),\n                    transforms.Normalize(\n                        mean=(0.1307), std=(0.3081)\n                    )\n                ]\n            )\n        else:\n            transform = transforms.Compose([transforms.ToTensor()])\n        super().__init__(save_dir, download=True, train=train, transform=transform)\n\n    def __getitem__(self, index):\n        img, label = super().__getitem__(index)\n        img = torch.flatten(img)\n        label = one_hot(label)\n        return img, label\n\n\ndef one_hot(labels, n_classes=10):\n    arr = torch.eye(n_classes)\n    return arr[labels]\n</code></pre>"},{"location":"examples/linear_net_theoretical_energy/#plotting","title":"Plotting","text":"<pre><code>def plot_total_energies(energies):\n    n_train_iters = len(energies[\"theory\"])\n    train_iters = [b+1 for b in range(n_train_iters)]\n\n    _, ax = plt.subplots(figsize=(6, 3))\n\n    for energy_type, energy in energies.items():\n        is_theory = energy_type == \"theory\"\n        line_style = \"--\" if is_theory else \"-\"\n        color = \"black\" if is_theory else \"#00CC96\"  #\"rgb(27, 158, 119)\"\n\n        if color.startswith(\"rgb\"):\n            rgb = tuple(int(x)/255 for x in color[4:-1].split(\",\"))\n        else:\n            rgb = color\n\n        ax.plot(\n            train_iters, \n            energy, \n            label=energy_type, \n            linewidth=3 if is_theory else 2,\n            linestyle=line_style,\n            color=rgb\n        )\n\n    ax.legend(fontsize=16)\n    ax.set_xlabel(\"Training Iteration\", fontsize=18, labelpad=10)\n    ax.set_ylabel(\"Energy\", fontsize=18, labelpad=10)\n    ax.tick_params(axis='both', labelsize=14)\n    plt.grid(True)\n    plt.show()\n</code></pre>"},{"location":"examples/linear_net_theoretical_energy/#train-and-test","title":"Train and test","text":"<p>To compute the theoretical energy, we can use <code>jpc.linear_equilib_energy()</code> which as clear from the equation above just takes a linear network and some data.</p> <pre><code>def evaluate(model, test_loader):\n    avg_test_loss, avg_test_acc = 0, 0\n    for _, (img_batch, label_batch) in enumerate(test_loader):\n        img_batch, label_batch = img_batch.numpy(), label_batch.numpy()\n\n        test_loss, test_acc = jpc.test_discriminative_pc(\n            model=model,\n            output=label_batch,\n            input=img_batch\n        )\n        avg_test_loss += test_loss\n        avg_test_acc += test_acc\n\n    return avg_test_loss / len(test_loader), avg_test_acc / len(test_loader)\n\n\ndef train( \n      input_dim,\n      width,\n      depth,\n      output_dim,\n      act_fn,\n      lr,\n      batch_size,\n      max_t1,\n      test_every,\n      n_train_iters\n):\n    key = jax.random.PRNGKey(0)\n\n    # NOTE: act_fn is linear and we use no biases \n    model = jpc.make_mlp(\n        key, \n        input_dim=input_dim,\n        width=width,\n        depth=depth,\n        output_dim=output_dim,\n        act_fn=act_fn,\n        use_bias=False\n    )\n    optim = optax.adam(lr)\n    opt_state = optim.init(\n        (eqx.filter(model, eqx.is_array), None)\n    )\n    train_loader, test_loader = get_mnist_loaders(batch_size)\n\n    num_total_energies, theory_total_energies = [], []\n    for iter, (img_batch, label_batch) in enumerate(train_loader):\n        img_batch, label_batch = img_batch.numpy(), label_batch.numpy()\n\n        theory_total_energies.append(\n            jpc.linear_equilib_energy(\n                model=model, \n                x=img_batch, \n                y=label_batch\n            )\n        )\n        result = jpc.make_pc_step(\n            model,\n            optim,\n            opt_state,\n            output=label_batch,\n            input=img_batch,\n            max_t1=max_t1,\n            record_energies=True\n        )\n        model, opt_state = result[\"model\"], result[\"opt_state\"]\n        train_loss, t_max = result[\"loss\"], result[\"t_max\"]\n        num_total_energies.append(result[\"energies\"][:, t_max-1].sum())\n\n        if ((iter+1) % test_every) == 0:\n            _, avg_test_acc = evaluate(model, test_loader)\n            print(\n                f\"Train iter {iter+1}, train loss={train_loss:4f}, \"\n                f\"avg test accuracy={avg_test_acc:4f}\"\n            )\n            if (iter+1) &gt;= n_train_iters:\n                break\n\n    return {\n        \"theory\": jnp.array(theory_total_energies),\n        \"experiment\": jnp.array(num_total_energies)\n    }\n</code></pre>"},{"location":"examples/linear_net_theoretical_energy/#run","title":"Run","text":"<p>Below we plot the theoretical energy against the numerical one.</p> <pre><code>energies = train(\n    input_dim=INPUT_DIM,\n    width=WIDTH,\n    depth=DEPTH,\n    output_dim=OUTPUT_DIM,\n    act_fn=ACT_FN,\n    lr=LEARNING_RATE,\n    batch_size=BATCH_SIZE,\n    test_every=TEST_EVERY,\n    max_t1=MAX_T1,\n    n_train_iters=N_TRAIN_ITERS\n)\nplot_total_energies(energies)\n</code></pre> <pre><code>Train iter 10, train loss=0.027985, avg test accuracy=72.105370\nTrain iter 20, train loss=0.027246, avg test accuracy=80.458733\nTrain iter 30, train loss=0.024185, avg test accuracy=78.325317\nTrain iter 40, train loss=0.025714, avg test accuracy=82.381813\nTrain iter 50, train loss=0.025583, avg test accuracy=80.558891\nTrain iter 60, train loss=0.026662, avg test accuracy=79.837738\nTrain iter 70, train loss=0.026263, avg test accuracy=80.568909\nTrain iter 80, train loss=0.021536, avg test accuracy=79.597359\nTrain iter 90, train loss=0.026155, avg test accuracy=82.391830\nTrain iter 100, train loss=0.024597, avg test accuracy=77.724358\n</code></pre> <p></p>"},{"location":"examples/mupc/","title":"\u03bcPC","text":"<p>This notebook demonstrates how to train residual networks with \u03bcPC (Innocenti et al., 2025), a reparameterisation of PC that allows stable training of very deep (100+ layer) networks while also enabling zero-shot hyperparameter transfer. For a theoretical justification and extension of this parameterisation, see Innocenti et al., 2026.</p> <pre><code>%%capture\n!pip install torch==2.3.1\n!pip install torchvision==0.18.1\n</code></pre> <pre><code>import jpc\n\nimport jax.random as jr\nimport equinox as eqx\nimport equinox.nn as nn\nimport optax\n\nimport math\nimport random\nimport numpy as np\nfrom typing import List, Callable\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\n\nimport warnings\nwarnings.simplefilter('ignore')  # ignore warnings\n</code></pre> <pre><code># for reproducibility\ndef set_global_seed(seed):\n    torch.manual_seed(seed)             \n    torch.cuda.manual_seed(seed)            \n    torch.cuda.manual_seed_all(seed)        \n    np.random.seed(seed)                  \n    random.seed(seed)                       \n    torch.backends.cudnn.deterministic = True \n    torch.backends.cudnn.benchmark = False\n</code></pre>"},{"location":"examples/mupc/#hyperparameters","title":"Hyperparameters","text":"<p>We define some global parameters, including the network architecture, learning rate, batch size, etc. We choose a network with \"only\" 30 layers and 128 hidden neurons so that it can run relatively fast on a CPU, but feel free to try deeper and wider networks.</p> <pre><code>SEED = 4329\n\nINPUT_DIM = 784\nWIDTH = 128\nDEPTH = 30\nOUTPUT_DIM = 10\nACT_FN = \"relu\"\n\nACTIVITY_LR = 5e-1\nPARAM_LR = 1e-1\nBATCH_SIZE = 64\nTEST_EVERY = 100\nN_TRAIN_ITERS = 900\n</code></pre>"},{"location":"examples/mupc/#dataset","title":"Dataset","text":"<p>Some utils to fetch MNIST.</p> <pre><code>def get_mnist_loaders(batch_size):\n    train_data = MNIST(train=True, normalise=True)\n    test_data = MNIST(train=False, normalise=True)\n    train_loader = DataLoader(\n        dataset=train_data,\n        batch_size=batch_size,\n        shuffle=True,\n        drop_last=True\n    )\n    test_loader = DataLoader(\n        dataset=test_data,\n        batch_size=batch_size,\n        shuffle=True,\n        drop_last=True\n    )\n    return train_loader, test_loader\n\n\nclass MNIST(datasets.MNIST):\n    def __init__(self, train, normalise=True, save_dir=\"data\"):\n        if normalise:\n            transform = transforms.Compose(\n                [\n                    transforms.ToTensor(),\n                    transforms.Normalize(\n                        mean=(0.1307), std=(0.3081)\n                    )\n                ]\n            )\n        else:\n            transform = transforms.Compose([transforms.ToTensor()])\n        super().__init__(save_dir, download=True, train=train, transform=transform)\n\n    def __getitem__(self, index):\n        img, label = super().__getitem__(index)\n        img = torch.flatten(img)\n        label = one_hot(label)\n        return img, label\n\n\ndef one_hot(labels, n_classes=10):\n    arr = torch.eye(n_classes)\n    return arr[labels]\n</code></pre>"},{"location":"examples/mupc/#creating-a-pc-model","title":"Creating a \u03bcPC model","text":"<p>To parameterise a model with \u03bcPC, one can use a few convenience functions of <code>jpc</code> to create an MLP or fully connected network with <code>jpc.make_mlp()</code> and an associated skip model with <code>jpc.make_skip model()</code>. Note that \u03bcPC works only for a specific type of ResNet, namely one with one-layer skip connections at every layer except from the input to the next layer and from the penultimate layer to the output (see Innocenti et al., 2025), as shown below.</p> <pre><code>key = jr.PRNGKey(SEED)\n\n# MLP\nmodel = jpc.make_mlp(\n    key,\n    input_dim=INPUT_DIM,\n    width=WIDTH,\n    depth=DEPTH,\n    output_dim=OUTPUT_DIM,\n    act_fn=ACT_FN,\n    param_type=\"mupc\"\n)\n\n# skip model\nskip_model = jpc.make_skip_model(DEPTH)\n</code></pre> <p>At training and test time we would need to pass both models to relevant <code>jpc</code> functions and change the argument <code>param_type = \"mupc\"</code> (default is <code>\"sp\"</code> for standard parameterisation). </p> <p>Alternatively, one could define a model class embedding the parameterisation itself and leave the above arguments to their default. This solution is more elegant but it can be harder to debug, at least for a fully connected architecture. However, if you would like to experiment with different parameterisations and more complex architectures (e.g. CNNs), we recommend this approach. </p> <pre><code>class ScaledLinear(eqx.Module):\n    \"\"\"Scaled linear transformation.\"\"\"\n    linear: nn.Linear\n    scaling: float = eqx.static_field()\n\n    def __init__(\n            self,\n            in_features,\n            out_features,\n            *,\n            key,\n            scaling=1.,\n            param_type=\"sp\",\n            use_bias=False\n    ):\n        keys = jr.split(key, 2)\n        linear = nn.Linear(\n            in_features, \n            out_features, \n            use_bias=use_bias,\n            key=keys[0]\n        )\n        if param_type == \"mupc\":\n            W = jr.normal(keys[1], linear.weight.shape)\n            linear = eqx.tree_at(lambda l: l.weight, linear, W)\n\n        self.linear = linear\n        self.scaling = scaling\n\n    def __call__(self, x):\n        return self.scaling * self.linear(x)\n\n\nclass ResNetBlock(eqx.Module):\n    \"\"\"Identity residual block applying activation and a scaled linear layer.\"\"\"\n    act_fn: Callable = eqx.static_field()\n    scaled_linear: ScaledLinear\n\n    def __init__(\n        self,\n        in_features,\n        out_features,\n        *,\n        key,\n        scaling=1.,\n        param_type=\"sp\",\n        use_bias=False,\n        act_fn=\"linear\"\n    ):\n        self.act_fn = act_fn\n        self.scaled_linear = ScaledLinear(\n            in_features=in_features,\n            out_features=out_features,\n            key=key,\n            scaling=scaling,\n            param_type=param_type,\n            use_bias=use_bias\n        )\n\n    def __call__(self, x):\n        res_path = x\n        x = self.act_fn(x)\n        return self.scaled_linear(x) + res_path\n\n\nclass Readout(eqx.Module):\n    \"\"\"Final network layer applying activation and a scaled linear layer.\"\"\"\n    act_fn: Callable = eqx.static_field()\n    scaled_linear: ScaledLinear\n\n    def __init__(\n        self,\n        in_features,\n        out_features,\n        *,\n        key,\n        scaling=1.,\n        param_type=\"sp\",\n        use_bias=False,\n        act_fn=\"linear\"\n    ):\n        self.act_fn = act_fn\n        self.scaled_linear = ScaledLinear(\n            in_features=in_features,\n            out_features=out_features,\n            key=key,\n            scaling=scaling,\n            param_type=param_type,\n            use_bias=use_bias\n        )\n\n    def __call__(self, x):\n        x = self.act_fn(x)\n        return self.scaled_linear(x)\n\n\nclass FCResNet(eqx.Module):\n    \"\"\"Fully-connected ResNet compatible with different parameterisations.\"\"\"\n    layers: List[eqx.Module]\n\n    def __init__(\n            self, \n            *,\n            key, \n            in_dim, \n            width, \n            depth, \n            out_dim, \n            act_fn=\"linear\", \n            use_bias=False,\n            param_type=\"sp\"\n        ):\n        act_fn = jpc.get_act_fn(act_fn)\n        if param_type == \"sp\":\n            in_scaling = 1.\n            hidden_scaling = 1.\n            out_scaling = 1.\n\n        elif param_type == \"mupc\":\n            in_scaling = 1 / math.sqrt(in_dim)\n            hidden_scaling = 1 / math.sqrt(width * depth)\n            out_scaling = 1 / width\n\n        keys = jr.split(key, depth)\n        self.layers = [\n            ScaledLinear(\n                key=keys[0],\n                in_features=in_dim,\n                out_features=width,\n                scaling=in_scaling,\n                param_type=param_type,\n                use_bias=use_bias\n            )\n        ]\n\n        for i in range(1, depth - 1):\n            self.layers.append(\n                ResNetBlock(\n                    key=keys[i],\n                    in_features=width,\n                    out_features=width,\n                    scaling=hidden_scaling,\n                    param_type=param_type,\n                    use_bias=use_bias,\n                    act_fn=act_fn\n                )\n            )\n\n        self.layers.append(\n            Readout(\n                key=keys[-1],\n                in_features=width,\n                out_features=out_dim,\n                scaling=out_scaling,\n                param_type=param_type,\n                use_bias=use_bias,\n                act_fn=act_fn\n            )\n        )\n\n    def __call__(self, x):\n        for f in self.layers:\n            x = f(x)      \n        return x\n\n    def __len__(self):\n        return len(self.layers)\n\n    def __getitem__(self, idx):\n        return self.layers[idx]\n</code></pre> <pre><code>mupc_model = FCResNet(\n    key=key, \n    in_dim=INPUT_DIM, \n    width=WIDTH, \n    depth=DEPTH, \n    out_dim=OUTPUT_DIM, \n    act_fn=ACT_FN, \n    use_bias=False, \n    param_type=\"mupc\"\n)\n</code></pre> <p>The following makes sure that the models have identical weights.</p> <pre><code>mupc_model = FCResNet(\n    key=key, \n    in_dim=INPUT_DIM, \n    width=WIDTH, \n    depth=DEPTH, \n    out_dim=OUTPUT_DIM, \n    act_fn=ACT_FN, \n    use_bias=False, \n    param_type=\"mupc\"\n)\nmupc_model = eqx.tree_at(\n    where=lambda tree: tree[0].linear.weight,\n    pytree=mupc_model,\n    replace=model[0][1].weight\n)\nfor l in range(1, len(model)):\n    mupc_model = eqx.tree_at(\n        where=lambda tree: tree[l].scaled_linear.linear.weight,\n        pytree=mupc_model,\n        replace=model[l][1].weight\n    )\n</code></pre>"},{"location":"examples/mupc/#train-and-test","title":"Train and test","text":"<p>For training, we use the advanced API including the functions <code>jpc.init_activities_with_ffwd()</code> to initialise the activities, <code>jpc.update_activities()</code> to perform PC inference, and <code>jpc.update_params()</code> to update the weights. All these functions accept <code>skip_model</code> and <code>param_type</code> as arguments. Note, however, that one can replace these functions with <code>jpc.make_pc_step()</code>. For testing, we use <code>jpc.test_discriminative_pc()</code>.</p> <pre><code>def evaluate(model, skip_model, test_loader, param_type):\n    avg_test_acc = 0\n    for _, (img_batch, label_batch) in enumerate(test_loader):\n        img_batch, label_batch = img_batch.numpy(), label_batch.numpy()\n\n        _, test_acc = jpc.test_discriminative_pc(\n            model=model,\n            input=img_batch,\n            output=label_batch,\n            skip_model=skip_model,\n            param_type=param_type\n        )\n        avg_test_acc += test_acc\n\n    return avg_test_acc / len(test_loader)\n\n\ndef train(\n      seed,  \n      model,\n      skip_model,\n      param_type,\n      activity_lr,  \n      param_lr,\n      batch_size,\n      test_every,\n      n_train_iters\n):  \n    set_global_seed(seed)\n    activity_optim = optax.sgd(activity_lr)\n    param_optim = optax.adam(param_lr)\n    param_opt_state = param_optim.init(\n        (eqx.filter(model, eqx.is_array), skip_model)\n    )\n    train_loader, test_loader = get_mnist_loaders(batch_size)\n\n    for iter, (img_batch, label_batch) in enumerate(train_loader):\n        img_batch, label_batch = img_batch.numpy(), label_batch.numpy()\n\n        # initialise activities\n        activities = jpc.init_activities_with_ffwd(\n            model=model,\n            input=img_batch,\n            skip_model=skip_model,\n            param_type=param_type\n        )\n        activity_opt_state = activity_optim.init(activities)\n        train_loss = jpc.mse_loss(activities[-1], label_batch)\n\n        # inference\n        for t in range(len(model)):\n            activity_update_result = jpc.update_pc_activities(\n                params=(model, skip_model),\n                activities=activities,\n                optim=activity_optim,\n                opt_state=activity_opt_state,\n                output=label_batch,\n                input=img_batch,\n                param_type=param_type\n            )\n            activities = activity_update_result[\"activities\"]\n            activity_opt_state = activity_update_result[\"opt_state\"]\n\n        # learning\n        param_update_result = jpc.update_pc_params(\n            params=(model, skip_model),\n            activities=activities,\n            optim=param_optim,\n            opt_state=param_opt_state,\n            output=label_batch,\n            input=img_batch,\n            param_type=param_type\n        )\n        model = param_update_result[\"model\"]\n        skip_model = param_update_result[\"skip_model\"]\n        param_opt_state = param_update_result[\"opt_state\"]\n\n        if np.isinf(train_loss) or np.isnan(train_loss):\n            print(\n                f\"Stopping training because of divergence, train loss={train_loss}\"\n            )\n            break\n\n        if ((iter+1) % test_every) == 0:\n            avg_test_acc = evaluate(\n                model=model,\n                skip_model=skip_model, \n                test_loader=test_loader, \n                param_type=param_type\n            )\n            print(\n                f\"Train iter {iter+1}, train loss={train_loss:4f}, \"\n                f\"avg test accuracy={avg_test_acc:4f}\"\n            )\n            if (iter+1) &gt;= n_train_iters:\n                break\n</code></pre>"},{"location":"examples/mupc/#run","title":"Run","text":"<p>Note that on a CPU the script below should take about a minute to complete.</p> <pre><code>train(\n    seed=SEED,\n    model=model,\n    skip_model=skip_model,\n    param_type=\"mupc\",\n    activity_lr=ACTIVITY_LR,\n    param_lr=PARAM_LR,\n    batch_size=BATCH_SIZE,\n    test_every=TEST_EVERY,\n    n_train_iters=N_TRAIN_ITERS\n)\n</code></pre> <pre><code>Train iter 100, train loss=0.016015, avg test accuracy=85.827324\nTrain iter 200, train loss=0.012215, avg test accuracy=88.541664\nTrain iter 300, train loss=0.009235, avg test accuracy=90.805290\nTrain iter 400, train loss=0.008675, avg test accuracy=91.286057\nTrain iter 500, train loss=0.011475, avg test accuracy=91.836937\nTrain iter 600, train loss=0.007697, avg test accuracy=92.177483\nTrain iter 700, train loss=0.007377, avg test accuracy=92.778442\nTrain iter 800, train loss=0.009710, avg test accuracy=92.477966\nTrain iter 900, train loss=0.009722, avg test accuracy=93.259216\n</code></pre> <p>For comparison, try to change to the standard parameterisation with <code>param_type = \"sp\"</code>. </p> <p>If you are using your own \u03bcPC-parameterised model class, then you can leave the default <code>skip_model = None</code> and <code>param_type = \"sp\"</code>, as shown below.</p> <pre><code>train(\n    seed=SEED,\n    model=mupc_model,\n    skip_model=None,\n    param_type=\"sp\",\n    activity_lr=ACTIVITY_LR,\n    param_lr=PARAM_LR,\n    batch_size=BATCH_SIZE,\n    test_every=TEST_EVERY,\n    n_train_iters=N_TRAIN_ITERS\n)\n</code></pre> <pre><code>Train iter 100, train loss=0.016063, avg test accuracy=85.787262\nTrain iter 200, train loss=0.012327, avg test accuracy=88.571716\nTrain iter 300, train loss=0.009621, avg test accuracy=90.875404\nTrain iter 400, train loss=0.009056, avg test accuracy=91.336136\nTrain iter 500, train loss=0.011603, avg test accuracy=92.007210\nTrain iter 600, train loss=0.007781, avg test accuracy=91.887016\nTrain iter 700, train loss=0.006997, avg test accuracy=92.938705\nTrain iter 800, train loss=0.010020, avg test accuracy=93.129005\nTrain iter 900, train loss=0.009978, avg test accuracy=93.279243\n</code></pre>"},{"location":"examples/supervised_generative_pc/","title":"Supervised generative PC","text":"<p>This notebook demonstrates how to train a simple feedforward network with predictive coding to generate MNIST digits.</p> <pre><code>%%capture\n!pip install torch==2.3.1\n!pip install torchvision==0.18.1\n!pip install matplotlib==3.0.0\n</code></pre> <pre><code>import jpc\n\nimport jax\nimport jax.numpy as jnp\nimport equinox as eqx\nimport equinox.nn as nn\nimport optax\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\n\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.simplefilter('ignore')  # ignore warnings\n</code></pre>"},{"location":"examples/supervised_generative_pc/#hyperparameters","title":"Hyperparameters","text":"<p>We define some global parameters, including the network architecture, learning rate, batch size, etc.</p> <pre><code>SEED = 0\n\nINPUT_DIM = 10\nWIDTH = 300\nDEPTH = 3\nOUTPUT_DIM = 784\nACT_FN = \"relu\"\n\nLEARNING_RATE = 1e-3\nBATCH_SIZE = 64\nMAX_T1 = 100\nTEST_EVERY = 50\nN_TRAIN_ITERS = 200\n</code></pre>"},{"location":"examples/supervised_generative_pc/#dataset","title":"Dataset","text":"<p>Some utils to fetch and plot MNIST.</p> <pre><code>def get_mnist_loaders(batch_size):\n    train_data = MNIST(train=True, normalise=True)\n    test_data = MNIST(train=False, normalise=True)\n    train_loader = DataLoader(\n        dataset=train_data,\n        batch_size=batch_size,\n        shuffle=True,\n        drop_last=True\n    )\n    test_loader = DataLoader(\n        dataset=test_data,\n        batch_size=batch_size,\n        shuffle=True,\n        drop_last=True\n    )\n    return train_loader, test_loader\n\n\nclass MNIST(datasets.MNIST):\n    def __init__(self, train, normalise=True, save_dir=\"data\"):\n        if normalise:\n            transform = transforms.Compose(\n                [\n                    transforms.ToTensor(),\n                    transforms.Normalize(\n                        mean=(0.1307), std=(0.3081)\n                    )\n                ]\n            )\n        else:\n            transform = transforms.Compose([transforms.ToTensor()])\n        super().__init__(save_dir, download=True, train=train, transform=transform)\n\n    def __getitem__(self, index):\n        img, label = super().__getitem__(index)\n        img = torch.flatten(img)\n        label = one_hot(label)\n        return img, label\n\n\ndef one_hot(labels, n_classes=10):\n    arr = torch.eye(n_classes)\n    return arr[labels]\n\n\ndef plot_mnist_img_preds(imgs, labels, n_imgs=10):\n    plt.figure(figsize=(20, 2))\n    for i in range(n_imgs):\n        plt.subplot(1, n_imgs, i + 1)\n        plt.xticks([])\n        plt.yticks([])\n        plt.grid(False)\n        plt.imshow(imgs[i].reshape(28, 28), cmap=plt.cm.binary_r)\n        plt.xlabel(jnp.argmax(labels, axis=1)[i], fontsize=16)\n    plt.show()\n</code></pre>"},{"location":"examples/supervised_generative_pc/#network","title":"Network","text":"<p>For <code>jpc</code> to work, we need to provide a network with callable layers. This is easy to do with the PyTorch-like <code>nn.Sequential()</code> in equinox. For example, we can define a ReLU MLP with two hidden layers as follows</p> <pre><code>key = jax.random.PRNGKey(SEED)\nkey, *subkeys = jax.random.split(key, 4)\nnetwork = [\n    nn.Sequential(\n        [\n            nn.Linear(10, 300, key=subkeys[0]),\n            nn.Lambda(jax.nn.relu)\n        ],\n    ),\n    nn.Sequential(\n        [\n            nn.Linear(300, 300, key=subkeys[1]),\n            nn.Lambda(jax.nn.relu)\n        ],\n    ),\n    nn.Linear(300, 784, key=subkeys[2]),\n]\n</code></pre> <p>You can also use <code>jpc.make_mlp()</code> to define a multi-layer perceptron (MLP) or fully connected network.</p> <pre><code>network = jpc.make_mlp(\n    key,\n    input_dim=INPUT_DIM,\n    width=WIDTH,\n    depth=DEPTH,\n    output_dim=OUTPUT_DIM,\n    act_fn=ACT_FN,\n    use_bias=True\n)\nprint(network)\n</code></pre> <pre><code>[Sequential(\n  layers=(\n    Lambda(fn=Identity()),\n    Linear(\n      weight=f32[300,10],\n      bias=f32[300],\n      in_features=10,\n      out_features=300,\n      use_bias=True\n    )\n  )\n), Sequential(\n  layers=(\n    Lambda(fn=&lt;PjitFunction of &lt;function relu at 0x117801bd0&gt;&gt;),\n    Linear(\n      weight=f32[300,300],\n      bias=f32[300],\n      in_features=300,\n      out_features=300,\n      use_bias=True\n    )\n  )\n), Sequential(\n  layers=(\n    Lambda(fn=&lt;PjitFunction of &lt;function relu at 0x117801bd0&gt;&gt;),\n    Linear(\n      weight=f32[784,300],\n      bias=f32[784],\n      in_features=300,\n      out_features=784,\n      use_bias=True\n    )\n  )\n)]\n</code></pre>"},{"location":"examples/supervised_generative_pc/#train-and-test","title":"Train and test","text":"<p>A PC network can be updated in a single line of code with <code>jpc.make_pc_step()</code>. Similarly, we can use <code>jpc.test_generative_pc()</code> to compute the network accuracy. Note that these functions are already \"jitted\" for optimised performance. Below we simply wrap each of these functions in training and test loops, respectively. </p> <p>Note that to train in an unsupervised way, you would simply need to remove the <code>input</code> from <code>jpc.make_pc_step()</code> and the <code>evaluate()</code> script. See this example notebook.</p> <pre><code>def evaluate(key, layer_sizes, batch_size, network, test_loader, max_t1):\n    test_acc = 0\n    for _, (img_batch, label_batch) in enumerate(test_loader):\n        img_batch, label_batch = img_batch.numpy(), label_batch.numpy()\n\n        acc, img_preds = jpc.test_generative_pc(\n            model=network,\n            input=label_batch,\n            output=img_batch,\n            key=key,\n            layer_sizes=layer_sizes,\n            batch_size=batch_size,\n            max_t1=max_t1\n        )\n        test_acc += acc\n\n    avg_test_acc = test_acc / len(test_loader)\n\n    return avg_test_acc, label_batch, img_preds\n\n\ndef train(\n      key,\n      input_dim,\n      width,\n      depth,\n      output_dim,\n      batch_size,\n      network,\n      lr,\n      max_t1,\n      test_every,\n      n_train_iters\n):\n    layer_sizes = [input_dim] + [width]*(depth-1) + [output_dim]\n    optim = optax.adam(lr)\n    opt_state = optim.init(\n        (eqx.filter(network, eqx.is_array), None)\n    )\n    train_loader, test_loader = get_mnist_loaders(batch_size)\n\n    for iter, (img_batch, label_batch) in enumerate(train_loader):\n        img_batch, label_batch = img_batch.numpy(), label_batch.numpy()\n\n        result = jpc.make_pc_step(\n            model=network,\n            optim=optim,\n            opt_state=opt_state,\n            input=label_batch,\n            output=img_batch,\n            max_t1=max_t1\n        )\n        network, opt_state = result[\"model\"], result[\"opt_state\"]\n        train_loss = result[\"loss\"]\n        if ((iter+1) % test_every) == 0:\n            avg_test_acc, test_label_batch, img_preds = evaluate(\n                key,\n                layer_sizes,\n                batch_size,\n                network,\n                test_loader,\n                max_t1=max_t1\n            )\n            print(\n                f\"Train iter {iter+1}, train loss={train_loss:4f}, \"\n                f\"avg test accuracy={avg_test_acc:4f}\"\n            )\n            if (iter+1) &gt;= n_train_iters:\n                break\n\n    plot_mnist_img_preds(img_preds, test_label_batch)\n    return network\n</code></pre>"},{"location":"examples/supervised_generative_pc/#run","title":"Run","text":"<pre><code>network = train(\n    key=key,\n    input_dim=INPUT_DIM,\n    width=WIDTH,\n    depth=DEPTH,\n    output_dim=OUTPUT_DIM,\n    batch_size=BATCH_SIZE,\n    network=network,\n    lr=LEARNING_RATE,\n    max_t1=MAX_T1,\n    test_every=TEST_EVERY,\n    n_train_iters=N_TRAIN_ITERS\n)\n</code></pre> <pre><code>Train iter 50, train loss=0.312354, avg test accuracy=79.717545\nTrain iter 100, train loss=0.275381, avg test accuracy=83.794067\nTrain iter 150, train loss=0.293271, avg test accuracy=84.755608\nTrain iter 200, train loss=0.297628, avg test accuracy=84.785660\n</code></pre>"},{"location":"examples/unsupervised_generative_pc/","title":"Unsupervised generative PC","text":"<p>This notebook demonstrates how to train a simple feedforward network with predictive coding to encode MNIST digits in an unsupervised manner.</p> <pre><code>%%capture\n!pip install torch==2.3.1\n!pip install torchvision==0.18.1\n!pip install matplotlib==3.0.0\n</code></pre> <pre><code>import jpc\n\nimport jax\nimport equinox as eqx\nimport equinox.nn as nn\nimport optax\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\n\nimport matplotlib.pyplot as plt\nimport matplotlib.colors as mcolors\n\nimport warnings\nwarnings.simplefilter('ignore')  # ignore warnings\n</code></pre>"},{"location":"examples/unsupervised_generative_pc/#hyperparameters","title":"Hyperparameters","text":"<p>We define some global parameters, including the network architecture, learning rate, batch size, etc.</p> <pre><code>SEED = 0\n\nINPUT_DIM = 50\nWIDTH = 300\nDEPTH = 3\nOUTPUT_DIM = 784\nACT_FN = \"relu\"\n\nLEARNING_RATE = 1e-3\nBATCH_SIZE = 64\nMAX_T1 = 100\nN_TRAIN_ITERS = 300\n</code></pre>"},{"location":"examples/unsupervised_generative_pc/#dataset","title":"Dataset","text":"<p>Some utils to fetch and plot MNIST.</p> <pre><code>def get_mnist_loaders(batch_size):\n    train_data = MNIST(train=True, normalise=True)\n    test_data = MNIST(train=False, normalise=True)\n    train_loader = DataLoader(\n        dataset=train_data,\n        batch_size=batch_size,\n        shuffle=True,\n        drop_last=True\n    )\n    test_loader = DataLoader(\n        dataset=test_data,\n        batch_size=batch_size,\n        shuffle=True,\n        drop_last=True\n    )\n    return train_loader, test_loader\n\n\nclass MNIST(datasets.MNIST):\n    def __init__(self, train, normalise=True, save_dir=\"data\"):\n        if normalise:\n            transform = transforms.Compose(\n                [\n                    transforms.ToTensor(),\n                    transforms.Normalize(\n                        mean=(0.1307), std=(0.3081)\n                    )\n                ]\n            )\n        else:\n            transform = transforms.Compose([transforms.ToTensor()])\n        super().__init__(save_dir, download=True, train=train, transform=transform)\n\n    def __getitem__(self, index):\n        img, _ = super().__getitem__(index)\n        img = torch.flatten(img)\n        return img\n</code></pre>"},{"location":"examples/unsupervised_generative_pc/#plotting","title":"Plotting","text":"<pre><code>def plot_train_energies(energies, ts):\n    t_max = int(ts[0])\n    norm = mcolors.Normalize(vmin=0, vmax=len(energies)-1)\n    fig, ax = plt.subplots(figsize=(8, 4))\n\n    cmap_blues = plt.get_cmap(\"Blues\")\n    cmap_reds = plt.get_cmap(\"Reds\")\n    cmap_greens = plt.get_cmap(\"Greens\")\n\n    legend_handles = []\n    legend_labels = []\n\n    for t, energies_iter in enumerate(energies):\n        line1, = ax.plot(energies_iter[0, :t_max], color=cmap_blues(norm(t)))\n        line2, = ax.plot(energies_iter[1, :t_max], color=cmap_reds(norm(t)))\n        line3, = ax.plot(energies_iter[2, :t_max], color=cmap_greens(norm(t)))\n\n        if t == 70:\n            legend_handles.append(line1)\n            legend_labels.append(\"$\\ell_1$\")\n            legend_handles.append(line2)\n            legend_labels.append(\"$\\ell_2$\")\n            legend_handles.append(line3)\n            legend_labels.append(\"$\\ell_3$\")\n\n    ax.legend(legend_handles, legend_labels, loc=\"best\", fontsize=16)\n    sm = plt.cm.ScalarMappable(cmap=plt.get_cmap(\"Greys\"), norm=norm)\n    sm._A = []\n    cbar = fig.colorbar(sm, ax=ax)\n    cbar.set_label(\"Training iteration\", fontsize=16, labelpad=14)\n    cbar.ax.tick_params(labelsize=14) \n    plt.gca().tick_params(axis=\"both\", which=\"major\", labelsize=16)\n\n    ax.set_xlabel(\"Inference iterations\", fontsize=18, labelpad=14)\n    ax.set_ylabel(\"Energy\", fontsize=18, labelpad=14)\n    ax.set_yscale(\"log\")\n    plt.show()\n</code></pre>"},{"location":"examples/unsupervised_generative_pc/#network","title":"Network","text":"<p>For <code>jpc</code> to work, we need to provide a network with callable layers. This is easy to do with the PyTorch-like <code>nn.Sequential()</code> in equinox. For example, we can define a ReLU MLP with two hidden layers as follows</p> <pre><code>key = jax.random.PRNGKey(SEED)\nkey, *subkeys = jax.random.split(key, 4)\nnetwork = [\n    nn.Sequential(\n        [\n            nn.Linear(10, 300, key=subkeys[0]),\n            nn.Lambda(jax.nn.relu)\n        ],\n    ),\n    nn.Sequential(\n        [\n            nn.Linear(300, 300, key=subkeys[1]),\n            nn.Lambda(jax.nn.relu)\n        ],\n    ),\n    nn.Linear(300, 784, key=subkeys[2]),\n]\n</code></pre> <p>You can also use <code>jpc.make_mlp()</code> to define a multi-layer perceptron (MLP) or fully connected network.</p> <pre><code>network = jpc.make_mlp(\n    key,\n    input_dim=INPUT_DIM,\n    width=WIDTH,\n    depth=DEPTH,\n    output_dim=OUTPUT_DIM,\n    act_fn=ACT_FN,\n    use_bias=True\n)\nprint(network)\n</code></pre> <pre><code>[Sequential(\n  layers=(\n    Lambda(fn=Identity()),\n    Linear(\n      weight=f32[300,50],\n      bias=f32[300],\n      in_features=50,\n      out_features=300,\n      use_bias=True\n    )\n  )\n), Sequential(\n  layers=(\n    Lambda(fn=&lt;PjitFunction of &lt;function relu at 0x12db01e10&gt;&gt;),\n    Linear(\n      weight=f32[300,300],\n      bias=f32[300],\n      in_features=300,\n      out_features=300,\n      use_bias=True\n    )\n  )\n), Sequential(\n  layers=(\n    Lambda(fn=&lt;PjitFunction of &lt;function relu at 0x12db01e10&gt;&gt;),\n    Linear(\n      weight=f32[784,300],\n      bias=f32[784],\n      in_features=300,\n      out_features=784,\n      use_bias=True\n    )\n  )\n)]\n</code></pre>"},{"location":"examples/unsupervised_generative_pc/#train","title":"Train","text":"<p>A PC network can be updated in a single line of code with <code>jpc.make_pc_step()</code>, which is already \"jitted\" for optimised performance. To train in an unsupervised way, we simply avoid providing an <code>input</code> to <code>jpc.make_pc_step()</code>. To test the learned encoding or representation for downstream accuracy, you could simply add a classifier.</p> <pre><code>def train(\n      key,\n      input_dim,\n      width,\n      depth,\n      output_dim,\n      batch_size,\n      network,\n      lr,\n      max_t1,\n      n_train_iters\n):\n    layer_sizes = [input_dim] + [width]*(depth-1) + [output_dim]\n    optim = optax.adam(lr)\n    opt_state = optim.init(\n        (eqx.filter(network, eqx.is_array), None)\n    )\n    train_loader, _ = get_mnist_loaders(batch_size)\n\n    train_energies, ts = [], []\n    for iter, img_batch in enumerate(train_loader):\n        img_batch = img_batch.numpy()\n\n        result = jpc.make_pc_step(\n            key=key,\n            layer_sizes=layer_sizes,\n            batch_size=batch_size,\n            model=network,\n            optim=optim,\n            opt_state=opt_state,\n            output=img_batch,\n            max_t1=max_t1,\n            record_activities=True,\n            record_energies=True\n        )\n        network, opt_state = result[\"model\"], result[\"opt_state\"]\n        train_energies.append(result[\"energies\"])\n        ts.append(result[\"t_max\"])\n        if (iter+1) &gt;= n_train_iters:\n            break\n\n    return result[\"model\"], train_energies, ts\n</code></pre>"},{"location":"examples/unsupervised_generative_pc/#run","title":"Run","text":"<p>Below we simply plot the energy dynamics of each layer during both inference and learning.</p> <pre><code>network, energies, ts = train(\n    key=key,\n    input_dim=INPUT_DIM,\n    width=WIDTH,\n    depth=DEPTH,\n    output_dim=OUTPUT_DIM,\n    batch_size=BATCH_SIZE,\n    network=network,\n    lr=LEARNING_RATE,\n    max_t1=MAX_T1,\n    n_train_iters=N_TRAIN_ITERS\n)\nplot_train_energies(energies, ts)\n</code></pre> <p></p>"}]}