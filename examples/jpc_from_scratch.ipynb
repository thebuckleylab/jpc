{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚öôÔ∏è JPC from Scratch\n",
    "\n",
    "This notebook is a walk-through of how Predictive Coding (PC) is implemented in JPC. It was developed as a lab session of an MSc course at the University of Sussex. We are going to implement all the core functionalities of JPC from scratch, building towards the training of a simple feedforward network to classify MNIST.\n",
    "\n",
    "If you're not familiar with JAX, have a look at their [docs](https://jax.readthedocs.io/en/latest/quickstart.html), but we will explain all the necessary concepts below. JAX is basically numpy for GPUs and other hardware accelerators. We will also use: (i) [Equinox](https://docs.kidger.site/equinox/), which allows you to define neural nets with PyTorch-like syntax; and (ii) [Optax](https://optax.readthedocs.io/en/latest/index.html), which provides a range of common machine learning optimisers such as gradient descent and Adam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Installations & imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install torch==2.3.1\n",
    "!pip install torchvision==0.18.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.random as jr\n",
    "import jax.numpy as jnp\n",
    "from jax import vmap, grad\n",
    "from jax.tree_util import tree_map\n",
    "\n",
    "import equinox as eqx\n",
    "import equinox.nn as nn\n",
    "from equinox import filter_grad\n",
    "import optax\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Hyperparameters\n",
    "\n",
    "We define some global parameters related to the data, network, optimisers, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 827\n",
    "\n",
    "INPUT_DIM, OUTPUT_DIM = 28*28, 10\n",
    "NETWORK_WIDTH = 300\n",
    "\n",
    "ACTIVITY_LR = 1e-1\n",
    "INFERENCE_STEPS = 20\n",
    "\n",
    "PARAM_LR = 1e-3\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "TEST_EVERY = 50\n",
    "N_TRAIN_ITERS = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Dataset\n",
    "\n",
    "Some utils to fetch MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mnist_loaders(batch_size):\n",
    "    train_data = MNIST(train=True, normalise=True)\n",
    "    test_data = MNIST(train=False, normalise=True)\n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        dataset=test_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "class MNIST(datasets.MNIST):\n",
    "    def __init__(self, train, normalise=True, save_dir=\"data\"):\n",
    "        if normalise:\n",
    "            transform = transforms.Compose(\n",
    "                [\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(\n",
    "                        mean=(0.1307), std=(0.3081)\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            transform = transforms.Compose([transforms.ToTensor()])\n",
    "        super().__init__(save_dir, download=True, train=train, transform=transform)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, label = super().__getitem__(index)\n",
    "        img = torch.flatten(img)\n",
    "        label = one_hot(label)\n",
    "        return img, label\n",
    "\n",
    "\n",
    "def one_hot(labels, n_classes=10):\n",
    "    arr = torch.eye(n_classes)\n",
    "    return arr[labels]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PC energy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, recall that PC can be derived as a variational inference algorithm under certain assumptions. In particular, if we assume \n",
    "* a dirac delta (point mass) posterior and\n",
    "* a hierarchical Gaussian generative model,\n",
    "\n",
    "we get the standard PC energy\n",
    "\n",
    "$$\n",
    "    \\mathcal{F} = \\frac{1}{2N}\\sum_{i=1}^{N} \\sum_{\\ell=1}^L ||\\mathbf{z}_{\\ell, i} - f_\\ell(W_\\ell \\mathbf{z}_{\\ell-1, i} + \\mathbf{b}_\\ell)||^2_2\n",
    "$$\n",
    "which is just a sum of squared prediction errors at each network layer. Here we are being a little bit more precise than in the lecture, including multiple ($N$) data points and biases $\\mathbf{b}_\\ell$.\n",
    "\n",
    "ü§î **Food for thought**: Think about how the form of this energy could change depending on other assumptions we make about the generative model. See, for example, [Learning on Arbitrary Graph Topologies via Predictive Coding\n",
    "](https://proceedings.neurips.cc/paper_files/paper/2022/hash/f9f54762cbb4fe4dbffdd4f792c31221-Abstract-Conference.html) by Salvatori et al. (2022).\n",
    "\n",
    "Let's start by implementing this energy below. The function simply takes the model (with all the parameters), some initialised activities, and some input and output. Given these, it simply sums the prediction error at each layer.\n",
    "\n",
    "**NOTE**: below we use `vmap`, one of the core JAX transforms that allows you to vectorise operations, in this case for multiple data points or over a batch. See their [docs](https://jax.readthedocs.io/en/latest/automatic-vectorization.html) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pc_energy_fn(model, activities, input, output):\n",
    "    batch_size = output.shape[0]\n",
    "    n_activity_layers = len(activities) - 1\n",
    "    n_layers = len(model) - 1\n",
    "    \n",
    "    eL = output - vmap(model[-1])(activities[-2])\n",
    "    energies = [jnp.sum(eL ** 2)]\n",
    "    for act_l, net_l in zip(\n",
    "            range(1, n_activity_layers),\n",
    "            range(1, n_layers)\n",
    "    ):\n",
    "        err = activities[act_l] - vmap(model[net_l])(activities[act_l - 1])\n",
    "        energies.append(jnp.sum(err ** 2))\n",
    "\n",
    "    e1 = activities[0] - vmap(model[0])(input)\n",
    "    energies.append(jnp.sum(e1 ** 2))\n",
    "\n",
    "    return jnp.sum(jnp.array(energies)) / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's test it. To do so, we first need a model. Below we use Equinox to create a simple feedforward network with 2 hidden layers and Tanh activations. Note that we split the model into different parts with `nn.Sequential` to define the activities which PC will optimise over (during inference, more on this below).\n",
    "\n",
    "‚ùì **Question**: Think about other ways in which we could split the layers, for example by separating the non-linearities. Can you think of potential issues with this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sequential(\n",
       "   layers=(\n",
       "     Linear(\n",
       "       weight=f32[300,784],\n",
       "       bias=f32[300],\n",
       "       in_features=784,\n",
       "       out_features=300,\n",
       "       use_bias=True\n",
       "     ),\n",
       "     Lambda(fn=<wrapped function tanh>)\n",
       "   )\n",
       " ),\n",
       " Sequential(\n",
       "   layers=(\n",
       "     Linear(\n",
       "       weight=f32[300,300],\n",
       "       bias=f32[300],\n",
       "       in_features=300,\n",
       "       out_features=300,\n",
       "       use_bias=True\n",
       "     ),\n",
       "     Lambda(fn=<wrapped function tanh>)\n",
       "   )\n",
       " ),\n",
       " Linear(\n",
       "   weight=f32[10,300],\n",
       "   bias=f32[10],\n",
       "   in_features=300,\n",
       "   out_features=10,\n",
       "   use_bias=True\n",
       " )]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# jax uses explicit random number generators (see https://jax.readthedocs.io/en/latest/random-numbers.html)\n",
    "key = jr.PRNGKey(SEED)\n",
    "subkeys = jr.split(key, 3)\n",
    "\n",
    "model = [\n",
    "    nn.Sequential(\n",
    "        [\n",
    "            nn.Linear(INPUT_DIM, NETWORK_WIDTH, key=subkeys[0]),\n",
    "            nn.Lambda(jnp.tanh)\n",
    "        ],\n",
    "    ),\n",
    "    nn.Sequential(\n",
    "        [\n",
    "            nn.Linear(NETWORK_WIDTH, NETWORK_WIDTH, key=subkeys[1]),\n",
    "            nn.Lambda(jnp.tanh)\n",
    "        ],\n",
    "    ),\n",
    "    nn.Linear(NETWORK_WIDTH, OUTPUT_DIM, key=subkeys[2]),\n",
    "]\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last thing we need is to initialise the activities. For this, we will use a feedforward pass as often done in practice.\n",
    "\n",
    "‚ùì **Question**: Can you think of other ways of initialising the activities?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_activities_with_ffwd(model, input):\n",
    "    activities = [vmap(model[0])(input)]\n",
    "    for l in range(1, len(model)):\n",
    "        layer_output = vmap(model[l])(activities[l - 1])\n",
    "        activities.append(layer_output)\n",
    "\n",
    "    return activities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it on an MNIST sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activity z at layer 1: (64, 300)\n",
      "activity z at layer 2: (64, 300)\n",
      "activity z at layer 3: (64, 10)\n"
     ]
    }
   ],
   "source": [
    "# get a data sample\n",
    "train_loader, test_loader = get_mnist_loaders(BATCH_SIZE)\n",
    "img_batch, label_batch = next(iter(train_loader))\n",
    "\n",
    "# we need to turn the torch.Tensor data into numpy arrays for jax\n",
    "img_batch, label_batch = img_batch.numpy(), label_batch.numpy()\n",
    "\n",
    "# let's check our initialised activities\n",
    "activities = init_activities_with_ffwd(model, img_batch)\n",
    "for i, a in enumerate(activities):\n",
    "    print(f\"activity z at layer {i+1}: {a.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok so now we have everything to test our PC energy function: model, activities, and some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(1.2335204, dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc_energy_fn(\n",
    "    model=model,\n",
    "    activities=activities,\n",
    "    input=img_batch,\n",
    "    output=label_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And it works!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Energy gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we minimise the PC energy we defined above (Eq. 1)? Recall from the lecture that we do this in two phases: first with respect to the activities (inference) and then with respect to the weights (learning).\n",
    "\n",
    "\\begin{equation}\n",
    "    \\textit{Inference:} - \\frac{\\partial \\mathcal{F}}{\\partial \\mathbf{z}_\\ell}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    \\textit{Learning:} - \\frac{\\partial \\mathcal{F}}{\\partial W_\\ell}\n",
    "\\end{equation}\n",
    "\n",
    "So we just need to take these gradients of the energy. We are going to use autodiff, which JAX embeds by design (see the [docs](https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html)). If you're familiar with PyTorch, you are probably used to `loss.backward()` for this, which might feel obstruse at times. JAX, on the other hand, is a fully functional (as opposed to object-oriented) language whose syntax is very close to the maths as you can see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note how close this code is to the maths\n",
    "# this can be read as \"take the gradient of the energy...\n",
    "# ...with the respect to the 2nd argument (the activities)\n",
    "\n",
    "def compute_activity_grad(model, activities, input, output):\n",
    "    return grad(pc_energy_fn, argnums=1)(\n",
    "        model,\n",
    "        activities,\n",
    "        input,\n",
    "        output\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test this out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activity gradient dFdz shape at layer 1: (64, 300)\n",
      "activity gradient dFdz shape at layer 2: (64, 300)\n",
      "activity gradient dFdz shape at layer 3: (64, 10)\n"
     ]
    }
   ],
   "source": [
    "dFdzs = compute_activity_grad(\n",
    "    model=model, \n",
    "    activities=activities, \n",
    "    input=img_batch, \n",
    "    output=label_batch\n",
    ")\n",
    "for i, dFdz in enumerate(dFdzs):\n",
    "    print(f\"activity gradient dFdz shape at layer {i+1}: {dFdz.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we do the same and take the gradient of the energy with respect to the parameters.\n",
    "\n",
    "**Technical note**: below we use Equinox's convenience function `filter_grad` rather than JAX's native `grad`. This is because things like activation functions do not have parameters and so we do not want to differentiate them. `filter_grad` automatically filters these non-differentiable objects for us, while `grad` alone would throw an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that, compared to the previous function,...\n",
    "# ...we just change the argument with respect to which...\n",
    "# ...we are differentiating (the first, or in this case the model)\n",
    "\n",
    "def compute_param_grad(model, activities, input, output):\n",
    "    return filter_grad(pc_energy_fn)(\n",
    "        model,\n",
    "        activities,\n",
    "        input,\n",
    "        output\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grads = compute_param_grad(\n",
    "    model=model, \n",
    "    activities=activities, \n",
    "    input=img_batch, \n",
    "    output=label_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before putting everything together, let's wrap our gradients into update functions. This will also allow us to use JAX's `jit` primitive, which essentially compiles your code the first time it's executed so that it can be run more efficiently the next time (see the [docs](https://jax.readthedocs.io/en/latest/jit-compilation.html) for more details).\n",
    "\n",
    "These functions take an (Optax) optimiser such as gradient descent in addition to the previous arguments (model, activities and data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "@eqx.filter_jit\n",
    "def update_activities(model, activities, optim, opt_state, input, output):\n",
    "    activity_grads = compute_activity_grad(\n",
    "        model=model,\n",
    "        activities=activities,\n",
    "        input=input,\n",
    "        output=output\n",
    "    )\n",
    "    activity_updates, activity_opt_state = optim.update(\n",
    "        updates=activity_grads,\n",
    "        state=opt_state,\n",
    "        params=activities\n",
    "    )\n",
    "    activities = eqx.apply_updates(\n",
    "        model=activities,\n",
    "        updates=activity_updates\n",
    "    )\n",
    "    return activities, optim, opt_state\n",
    "\n",
    "\n",
    "# note that the only difference with the above function is...\n",
    "# ...the variable we are updating (parameters vs activities)\n",
    "@eqx.filter_jit\n",
    "def update_params(model, activities, optim, opt_state, input, output):\n",
    "    param_grads = compute_param_grad(\n",
    "        model=model,\n",
    "        activities=activities,\n",
    "        input=input,\n",
    "        output=output\n",
    "    )\n",
    "    param_updates, param_opt_state = optim.update(\n",
    "        updates=param_grads,\n",
    "        state=opt_state,\n",
    "        params=model\n",
    "    )\n",
    "    model = eqx.apply_updates(\n",
    "        model=model,\n",
    "        updates=param_updates\n",
    "    )\n",
    "    return model, optim, opt_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Putting everything together: Training and testing\n",
    "\n",
    "Now that we have our activity and parameter updates, we just need to wrap them in a training and test loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: the test accuracy computation below could be sped up...\n",
    "# ...with jit in a separate function\n",
    "\n",
    "def evaluate(model, test_loader):\n",
    "    avg_test_acc = 0\n",
    "    for test_iter, (img_batch, label_batch) in enumerate(test_loader):\n",
    "        img_batch, label_batch = img_batch.numpy(), label_batch.numpy()\n",
    "\n",
    "        preds = init_activities_with_ffwd(model, img_batch)[-1]\n",
    "        test_acc = jnp.mean(\n",
    "            jnp.argmax(label_batch, axis=1) == jnp.argmax(preds, axis=1)\n",
    "        ) * 100\n",
    "        avg_test_acc += test_acc\n",
    "\n",
    "    return avg_test_acc / len(test_loader)\n",
    "    \n",
    "\n",
    "def train(\n",
    "      model,\n",
    "      activity_lr,\n",
    "      inference_steps,\n",
    "      param_lr,\n",
    "      batch_size,\n",
    "      test_every,\n",
    "      n_train_iters\n",
    "):\n",
    "    # define optimisers for activities and parameters\n",
    "    activity_optim = optax.sgd(activity_lr)\n",
    "    param_optim = optax.adam(param_lr)\n",
    "    param_opt_state = param_optim.init(eqx.filter(model, eqx.is_array))\n",
    "    \n",
    "    train_loader, test_loader = get_mnist_loaders(batch_size)\n",
    "    for train_iter, (img_batch, label_batch) in enumerate(train_loader):\n",
    "        img_batch, label_batch = img_batch.numpy(), label_batch.numpy()\n",
    "\n",
    "        # initialise activities\n",
    "        activities = init_activities_with_ffwd(model, img_batch)\n",
    "        activity_opt_state = activity_optim.init(activities)\n",
    "\n",
    "        # calculate loss\n",
    "        train_loss = jnp.mean((label_batch - activities[-1])**2)\n",
    "\n",
    "        # inference\n",
    "        for t in range(inference_steps):\n",
    "            activities, activity_optim, activity_opt_state = update_activities(\n",
    "                model=model, \n",
    "                activities=activities, \n",
    "                optim=activity_optim, \n",
    "                opt_state=activity_opt_state, \n",
    "                input=img_batch, \n",
    "                output=label_batch\n",
    "            )\n",
    "\n",
    "        # learning\n",
    "        model, param_optim, param_opt_state = update_params(\n",
    "            model=model,\n",
    "            activities=activities,  # note how we use the optimised activities\n",
    "            optim=param_optim,\n",
    "            opt_state=param_opt_state,\n",
    "            input=img_batch,\n",
    "            output=label_batch\n",
    "        )\n",
    "        if ((train_iter+1) % test_every) == 0:\n",
    "            avg_test_acc = evaluate(model, test_loader)\n",
    "            print(\n",
    "                f\"Train iter {train_iter+1}, train loss={train_loss:4f}, \"\n",
    "                f\"avg test accuracy={avg_test_acc:4f}\"\n",
    "            )\n",
    "            if (train_iter+1) >= n_train_iters:\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train iter 50, train loss=0.065566, avg test accuracy=72.726364\n",
      "Train iter 100, train loss=0.046521, avg test accuracy=76.292068\n",
      "Train iter 150, train loss=0.042710, avg test accuracy=86.568512\n",
      "Train iter 200, train loss=0.029598, avg test accuracy=89.082535\n",
      "Train iter 250, train loss=0.031486, avg test accuracy=89.222755\n",
      "Train iter 300, train loss=0.016624, avg test accuracy=91.296074\n",
      "Train iter 350, train loss=0.025201, avg test accuracy=92.648239\n",
      "Train iter 400, train loss=0.018597, avg test accuracy=92.968750\n",
      "Train iter 450, train loss=0.019027, avg test accuracy=94.130608\n",
      "Train iter 500, train loss=0.014850, avg test accuracy=93.760017\n"
     ]
    }
   ],
   "source": [
    "train(\n",
    "    model=model,\n",
    "    activity_lr=ACTIVITY_LR,\n",
    "    inference_steps=INFERENCE_STEPS,\n",
    "    param_lr=PARAM_LR,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    test_every=TEST_EVERY,\n",
    "    n_train_iters=N_TRAIN_ITERS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü•≥ Great, we see that our model is learning! This model was not tuned, and you can probably improve the performance by tweaking some of the hyperparameters (e.g. try a higher number of inference steps).\n",
    "\n",
    "Even if you didn't follow all the implementation details, you should now have at least an idea of how PC works in practice. Indeed, this is basically the core code behind a new PC library our lab will soon release: [JPC](https://github.com/thebuckleylab/jpc). Play around with the notebook examples there where you can learn how to train a variety of PC networks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
